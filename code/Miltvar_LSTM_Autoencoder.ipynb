{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# from generate_data import ARData, fixed_ar_coefficients\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import os\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#read features from trackmate extracted csv for lstm autencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#ZXY = Altered point\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "dtype = torch.long\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv (r'Z:\\DataResult\\20200618\\Urine\\sc_2\\r1\\ImageJ\\Spots_in_tracks_statistics.csv', low_memory=False,index_col=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv (r'C:\\Users\\ZXY\\OneDrive\\Desktop\\Spots in tracks statistics.csv', low_memory=False,index_col=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv (r'Z:\\RawData\\20200918\\beads_1\\BgSub\\ImageJ\\Spots in tracks statistics.csv', low_memory=False,index_col=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Label</td>\n",
       "      <td>ID</td>\n",
       "      <td>TRACK_ID</td>\n",
       "      <td>QUALITY</td>\n",
       "      <td>POSITION_X</td>\n",
       "      <td>POSITION_Y</td>\n",
       "      <td>POSITION_Z</td>\n",
       "      <td>POSITION_T</td>\n",
       "      <td>FRAME</td>\n",
       "      <td>RADIUS</td>\n",
       "      <td>...</td>\n",
       "      <td>MANUAL_COLOR</td>\n",
       "      <td>MEAN_INTENSITY</td>\n",
       "      <td>MEDIAN_INTENSITY</td>\n",
       "      <td>MIN_INTENSITY</td>\n",
       "      <td>MAX_INTENSITY</td>\n",
       "      <td>TOTAL_INTENSITY</td>\n",
       "      <td>STANDARD_DEVIATION</td>\n",
       "      <td>ESTIMATED_DIAMETER</td>\n",
       "      <td>CONTRAST</td>\n",
       "      <td>SNR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID624489</td>\n",
       "      <td>624489</td>\n",
       "      <td>0</td>\n",
       "      <td>243.200</td>\n",
       "      <td>70.265</td>\n",
       "      <td>61.595</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-10921639</td>\n",
       "      <td>1356.206</td>\n",
       "      <td>976</td>\n",
       "      <td>272</td>\n",
       "      <td>4096</td>\n",
       "      <td>131552</td>\n",
       "      <td>1014.718</td>\n",
       "      <td>5.080</td>\n",
       "      <td>0.685</td>\n",
       "      <td>1.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID625852</td>\n",
       "      <td>625852</td>\n",
       "      <td>0</td>\n",
       "      <td>241.099</td>\n",
       "      <td>70.358</td>\n",
       "      <td>59.526</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-10921639</td>\n",
       "      <td>1326.680</td>\n",
       "      <td>928</td>\n",
       "      <td>256</td>\n",
       "      <td>3968</td>\n",
       "      <td>128688</td>\n",
       "      <td>1020.710</td>\n",
       "      <td>5.106</td>\n",
       "      <td>0.697</td>\n",
       "      <td>1.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID625696</td>\n",
       "      <td>625696</td>\n",
       "      <td>0</td>\n",
       "      <td>221.939</td>\n",
       "      <td>70.072</td>\n",
       "      <td>57.495</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-10921639</td>\n",
       "      <td>1264.990</td>\n",
       "      <td>848</td>\n",
       "      <td>224</td>\n",
       "      <td>3680</td>\n",
       "      <td>122704</td>\n",
       "      <td>919.347</td>\n",
       "      <td>5.105</td>\n",
       "      <td>0.668</td>\n",
       "      <td>1.103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID624333</td>\n",
       "      <td>624333</td>\n",
       "      <td>0</td>\n",
       "      <td>216.233</td>\n",
       "      <td>70.039</td>\n",
       "      <td>55.384</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-10921639</td>\n",
       "      <td>1222.598</td>\n",
       "      <td>848</td>\n",
       "      <td>208</td>\n",
       "      <td>3408</td>\n",
       "      <td>118592</td>\n",
       "      <td>884.482</td>\n",
       "      <td>5.085</td>\n",
       "      <td>0.679</td>\n",
       "      <td>1.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180909</td>\n",
       "      <td>ID805374</td>\n",
       "      <td>805374</td>\n",
       "      <td>1030</td>\n",
       "      <td>44.587</td>\n",
       "      <td>237.680</td>\n",
       "      <td>173.385</td>\n",
       "      <td>0</td>\n",
       "      <td>998</td>\n",
       "      <td>998</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-10921639</td>\n",
       "      <td>395.216</td>\n",
       "      <td>368</td>\n",
       "      <td>112</td>\n",
       "      <td>864</td>\n",
       "      <td>38336</td>\n",
       "      <td>199.869</td>\n",
       "      <td>5.088</td>\n",
       "      <td>0.378</td>\n",
       "      <td>1.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180910</td>\n",
       "      <td>ID805783</td>\n",
       "      <td>805783</td>\n",
       "      <td>1030</td>\n",
       "      <td>80.573</td>\n",
       "      <td>237.410</td>\n",
       "      <td>170.942</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-10921639</td>\n",
       "      <td>607.670</td>\n",
       "      <td>560</td>\n",
       "      <td>96</td>\n",
       "      <td>1456</td>\n",
       "      <td>58944</td>\n",
       "      <td>365.557</td>\n",
       "      <td>5.131</td>\n",
       "      <td>0.467</td>\n",
       "      <td>1.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180911</td>\n",
       "      <td>ID804569</td>\n",
       "      <td>804569</td>\n",
       "      <td>1031</td>\n",
       "      <td>41.442</td>\n",
       "      <td>916.603</td>\n",
       "      <td>228.651</td>\n",
       "      <td>0</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-10921639</td>\n",
       "      <td>358.268</td>\n",
       "      <td>320</td>\n",
       "      <td>112</td>\n",
       "      <td>832</td>\n",
       "      <td>34752</td>\n",
       "      <td>185.782</td>\n",
       "      <td>5.105</td>\n",
       "      <td>0.367</td>\n",
       "      <td>1.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180912</td>\n",
       "      <td>ID805587</td>\n",
       "      <td>805587</td>\n",
       "      <td>1031</td>\n",
       "      <td>40.321</td>\n",
       "      <td>915.851</td>\n",
       "      <td>227.360</td>\n",
       "      <td>0</td>\n",
       "      <td>996</td>\n",
       "      <td>996</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-10921639</td>\n",
       "      <td>360.907</td>\n",
       "      <td>320</td>\n",
       "      <td>112</td>\n",
       "      <td>816</td>\n",
       "      <td>35008</td>\n",
       "      <td>163.690</td>\n",
       "      <td>5.154</td>\n",
       "      <td>0.368</td>\n",
       "      <td>1.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180913</td>\n",
       "      <td>ID805181</td>\n",
       "      <td>805181</td>\n",
       "      <td>1031</td>\n",
       "      <td>40.228</td>\n",
       "      <td>915.152</td>\n",
       "      <td>226.580</td>\n",
       "      <td>0</td>\n",
       "      <td>997</td>\n",
       "      <td>997</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-10921639</td>\n",
       "      <td>357.113</td>\n",
       "      <td>304</td>\n",
       "      <td>80</td>\n",
       "      <td>864</td>\n",
       "      <td>34640</td>\n",
       "      <td>180.821</td>\n",
       "      <td>5.059</td>\n",
       "      <td>0.343</td>\n",
       "      <td>1.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180914 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0       1         2        3           4           5   \\\n",
       "0          Label      ID  TRACK_ID  QUALITY  POSITION_X  POSITION_Y   \n",
       "1       ID624489  624489         0  243.200      70.265      61.595   \n",
       "2       ID625852  625852         0  241.099      70.358      59.526   \n",
       "3       ID625696  625696         0  221.939      70.072      57.495   \n",
       "4       ID624333  624333         0  216.233      70.039      55.384   \n",
       "...          ...     ...       ...      ...         ...         ...   \n",
       "180909  ID805374  805374      1030   44.587     237.680     173.385   \n",
       "180910  ID805783  805783      1030   80.573     237.410     170.942   \n",
       "180911  ID804569  804569      1031   41.442     916.603     228.651   \n",
       "180912  ID805587  805587      1031   40.321     915.851     227.360   \n",
       "180913  ID805181  805181      1031   40.228     915.152     226.580   \n",
       "\n",
       "                6           7      8       9   ...            11  \\\n",
       "0       POSITION_Z  POSITION_T  FRAME  RADIUS  ...  MANUAL_COLOR   \n",
       "1                0           0      0       5  ...     -10921639   \n",
       "2                0           1      1       5  ...     -10921639   \n",
       "3                0           2      2       5  ...     -10921639   \n",
       "4                0           3      3       5  ...     -10921639   \n",
       "...            ...         ...    ...     ...  ...           ...   \n",
       "180909           0         998    998       5  ...     -10921639   \n",
       "180910           0         999    999       5  ...     -10921639   \n",
       "180911           0         995    995       5  ...     -10921639   \n",
       "180912           0         996    996       5  ...     -10921639   \n",
       "180913           0         997    997       5  ...     -10921639   \n",
       "\n",
       "                    12                13             14             15  \\\n",
       "0       MEAN_INTENSITY  MEDIAN_INTENSITY  MIN_INTENSITY  MAX_INTENSITY   \n",
       "1             1356.206               976            272           4096   \n",
       "2             1326.680               928            256           3968   \n",
       "3             1264.990               848            224           3680   \n",
       "4             1222.598               848            208           3408   \n",
       "...                ...               ...            ...            ...   \n",
       "180909         395.216               368            112            864   \n",
       "180910         607.670               560             96           1456   \n",
       "180911         358.268               320            112            832   \n",
       "180912         360.907               320            112            816   \n",
       "180913         357.113               304             80            864   \n",
       "\n",
       "                     16                  17                  18        19  \\\n",
       "0       TOTAL_INTENSITY  STANDARD_DEVIATION  ESTIMATED_DIAMETER  CONTRAST   \n",
       "1                131552            1014.718               5.080     0.685   \n",
       "2                128688            1020.710               5.106     0.697   \n",
       "3                122704             919.347               5.105     0.668   \n",
       "4                118592             884.482               5.085     0.679   \n",
       "...                 ...                 ...                 ...       ...   \n",
       "180909            38336             199.869               5.088     0.378   \n",
       "180910            58944             365.557               5.131     0.467   \n",
       "180911            34752             185.782               5.105     0.367   \n",
       "180912            35008             163.690               5.154     0.368   \n",
       "180913            34640             180.821               5.059     0.343   \n",
       "\n",
       "           20  \n",
       "0         SNR  \n",
       "1       1.087  \n",
       "2       1.068  \n",
       "3       1.103  \n",
       "4       1.118  \n",
       "...       ...  \n",
       "180909  1.085  \n",
       "180910  1.058  \n",
       "180911  1.035  \n",
       "180912  1.187  \n",
       "180913  1.009  \n",
       "\n",
       "[180914 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to_numpy()\n",
    "true_data = data[1:,2:].astype(np.float)#drop header and labelID"
   ]
  },
  {
   "attachments": {
    "header.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "true_data  \n",
    "# 0,&nbsp;&nbsp;1,&nbsp;&nbsp;&nbsp;&nbsp;2,&nbsp;&nbsp;&nbsp;&nbsp;3,&nbsp;&nbsp;&nbsp;&nbsp;4,&nbsp;&nbsp;&nbsp;&nbsp;5,&nbsp;&nbsp;6,&nbsp;&nbsp;7,&nbsp;&nbsp;8,&nbsp;&nbsp;9,&nbsp;&nbsp;&nbsp;&nbsp;10,&nbsp;&nbsp;&nbsp;&nbsp;11,&nbsp;&nbsp;&nbsp;&nbsp;12,&nbsp;&nbsp;&nbsp;&nbsp;13,&nbsp;&nbsp;&nbsp;&nbsp;14,&nbsp;&nbsp;&nbsp;&nbsp;15,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16,&nbsp;&nbsp;&nbsp;&nbsp;17,&nbsp;&nbsp;18\n",
    "![header.PNG](attachment:header.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select features\n",
    "selected_data = true_data[:,[0,1,2,3,10,11,12,13,14,15,17,18]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data[:,1] = selected_data[:,1]*10\n",
    "selected_data[:,7] = selected_data[:,7]/10\n",
    "selected_data[:,8] = selected_data[:,8]/100\n",
    "selected_data[:,10] = selected_data[:,10]*1000\n",
    "selected_data[:,11] = selected_data[:,11]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop tracks whose length fewer than n, divide long tracks to several n-length tracks \n",
    "def padding_truncate(data, t = 200):\n",
    "    #return data with shape(m,t,n), m is sample numbers, t is length of tracks, n is feature numbers\n",
    "    count = 1\n",
    "    n = data.shape[1]\n",
    "    dataset = np.zeros((1,t,n))\n",
    "    for i in range(1,data.shape[0],1):    \n",
    "        if data[i,0] == data[i-1,0]:\n",
    "            count+=1\n",
    "        else:\n",
    "                count = 1\n",
    "                \n",
    "        if count == t:\n",
    "            dataset = np.append(dataset, np.expand_dims(data[i+1-t:i+1],axis = 0), axis = 0)\n",
    "            count = 0\n",
    "        else:\n",
    "                pass\n",
    "    \n",
    "    dataset = dataset[1:]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = padding_truncate(selected_data,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 500, 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ids = dataset[:,1,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ids = set(track_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(track_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1356.206"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_data[0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3085.9930665220822"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = padding_truncate(true_data)\n",
    "np.mean(dataset_test[:,:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[:,:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "#lstm autoencoder dataset, y is the same as x\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, datax, labels, transforms=None):\n",
    "        self.X = datax\n",
    "        self.y = labels\n",
    "        self.transforms = transforms\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (self.X.shape[0])\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        #print(i)\n",
    "        i= i%self.X.shape[0]\n",
    "\n",
    "        data = self.X[i]\n",
    "        #data = self.X\n",
    "        \n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "            \n",
    "        if self.y is not None:\n",
    "            return (data, self.y[i])\n",
    "        else:\n",
    "            return (data,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(dataset,None)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 200, 11])\n",
      "tensor([[[2810.5300, 1180.0900,   95.2800,  ..., 1323.9190,  729.0000,\n",
      "           851.0000],\n",
      "         [3037.6500, 1180.1550,   95.7590,  ..., 1427.5110,  724.0000,\n",
      "           855.0000],\n",
      "         [2940.7100, 1180.1190,   96.0670,  ..., 1377.9730,  730.0000,\n",
      "           853.0000],\n",
      "         ...,\n",
      "         [1643.4400, 1215.5720,  175.0550,  ...,  771.3690,  621.0000,\n",
      "           869.0000],\n",
      "         [1655.7500, 1215.9780,  175.3120,  ...,  781.9890,  616.0000,\n",
      "           849.0000],\n",
      "         [1618.6300, 1215.9300,  175.7700,  ...,  750.8910,  617.0000,\n",
      "           878.0000]],\n",
      "\n",
      "        [[ 175.0400,   78.5080,  234.6980,  ...,  128.3490,  139.0000,\n",
      "           749.0000],\n",
      "         [ 153.7000,   77.4790,  233.1890,  ...,  128.7060,   61.0000,\n",
      "           314.0000],\n",
      "         [ 173.4600,   77.7490,  232.9190,  ...,  140.3770,   90.0000,\n",
      "           383.0000],\n",
      "         ...,\n",
      "         [ 196.0100,   78.8880,  231.2440,  ...,  136.5460,  137.0000,\n",
      "           536.0000],\n",
      "         [ 231.0200,   79.2230,  231.2650,  ...,  140.4270,  193.0000,\n",
      "           759.0000],\n",
      "         [ 192.3300,   78.5170,  231.2100,  ...,  141.0040,  130.0000,\n",
      "           497.0000]],\n",
      "\n",
      "        [[ 606.3800,  593.9360,  356.8190,  ...,  279.5280,  401.0000,\n",
      "           910.0000],\n",
      "         [ 484.2600,  594.3070,  357.0640,  ...,  235.1240,  328.0000,\n",
      "           844.0000],\n",
      "         [ 515.3000,  594.0280,  357.7450,  ...,  253.8920,  320.0000,\n",
      "           806.0000],\n",
      "         ...,\n",
      "         [ 480.4700,  537.8810,  494.4350,  ...,  231.2020,  341.0000,\n",
      "           841.0000],\n",
      "         [ 545.3200,  537.3640,  494.9300,  ...,  271.0640,  359.0000,\n",
      "           782.0000],\n",
      "         [ 871.2300,  537.1580,  495.7430,  ...,  403.0490,  491.0000,\n",
      "           895.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 378.8900,  240.4090,  545.4500,  ...,  255.6470,  176.0000,\n",
      "           627.0000],\n",
      "         [ 394.8200,  240.0010,  546.0190,  ...,  262.4900,  131.0000,\n",
      "           452.0000],\n",
      "         [ 257.2300,  240.2940,  546.1750,  ...,  220.4310,   71.0000,\n",
      "           277.0000],\n",
      "         ...,\n",
      "         [ 359.8700,  240.8170,  546.5900,  ...,  214.1670,  249.0000,\n",
      "           730.0000],\n",
      "         [ 430.4300,  240.7900,  546.6890,  ...,  258.8150,  271.0000,\n",
      "           677.0000],\n",
      "         [ 481.8800,  240.2570,  546.7530,  ...,  260.4510,  299.0000,\n",
      "           770.0000]],\n",
      "\n",
      "        [[ 365.9400,  610.3330,  142.8720,  ...,  217.9440,  246.0000,\n",
      "           774.0000],\n",
      "         [ 384.2500,  609.9010,  142.3950,  ...,  239.9190,  157.0000,\n",
      "           447.0000],\n",
      "         [ 392.4400,  609.8100,  142.8340,  ...,  226.8720,  215.0000,\n",
      "           605.0000],\n",
      "         ...,\n",
      "         [ 459.0900,  582.4600,  235.2040,  ...,  244.9870,  315.0000,\n",
      "           727.0000],\n",
      "         [ 369.7500,  582.2370,  235.4990,  ...,  203.2680,  270.0000,\n",
      "           686.0000],\n",
      "         [ 383.4700,  582.2700,  236.2640,  ...,  196.9970,  260.0000,\n",
      "           739.0000]],\n",
      "\n",
      "        [[ 834.8700, 1076.9670,   18.1710,  ...,  316.0030,  305.0000,\n",
      "          1507.0000],\n",
      "         [ 814.5500, 1078.0560,   18.6230,  ...,  286.3250,  297.0000,\n",
      "          1643.0000],\n",
      "         [ 792.0600, 1077.3430,   18.4920,  ...,  314.0290,  284.0000,\n",
      "          1388.0000],\n",
      "         ...,\n",
      "         [ 720.2100, 1078.2640,   17.5000,  ...,  357.4950,  256.0000,\n",
      "           964.0000],\n",
      "         [ 674.6900, 1078.6450,   17.9060,  ...,  312.1920,  258.0000,\n",
      "          1092.0000],\n",
      "         [ 711.2900, 1078.6220,   17.9500,  ...,  335.2450,  269.0000,\n",
      "          1102.0000]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i, (X_batch, labels_batch )in enumerate(train_loader):\n",
    "    print(X_batch.shape)\n",
    "    print(labels_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "def predict(model, loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_predicted = []\n",
    "        labels_total = []       \n",
    "        \n",
    "        for images, labels in loader:\n",
    "            images = images.to(device,dtype=torch.float)\n",
    "            labels = labels.squeeze().to(device)\n",
    "\n",
    "            outputs, _ = model.forward(images)\n",
    "                \n",
    "            _, predicted = torch.max(outputs.data, 1)# _ are max values, predicts are their indexs\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predicted += predicted.cpu().numpy().tolist()\n",
    "            labels_total += labels.cpu().numpy().tolist()\n",
    "#             print(outputs)\n",
    "#             print('predicted',predicted)\n",
    "#             print('labels', labels)\n",
    "#             print(correct)\n",
    "#             print(total)\n",
    "#             print(predicted == labels)\n",
    "\n",
    "        print(\"Accuracy: {}%\" .format(100* correct/total),'correct', correct, 'total',total)\n",
    "    model.train()\n",
    "    return all_predicted, labels_total, format(100* correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "code_folding": [
     5,
     56,
     108,
     165,
     216
    ]
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Build model\n",
    "#####################\n",
    "\n",
    "# Here we define our model as a class\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_ch = 1, N_classes = 2):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.input_ch = input_ch\n",
    "        self.ch1, self.ch2 = 16, 32 \n",
    "        self.cnn_embed_dim = 8 #cnn-embedded dim \n",
    "        self.k1, self.k2 = 5, 5\n",
    "        self.h_d = 8\n",
    "        self.num_layers = 2\n",
    "        self.N_classes = N_classes\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(self.input_ch, self.ch1, self.k1)\n",
    "        self.cnn2 = nn.Conv2d(self.ch1, self.ch2, self.k2)\n",
    "\n",
    "        self.fc1 = nn.Linear(((10-self.k1+1)-self.k2+1)**2 * self.ch2, self.cnn_embed_dim) # 10 is image size\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.cnn_embed_dim, self.h_d, self.num_layers)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.h_d, self.N_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, X_stacked):\n",
    "        \n",
    "        #print(X_stacked.size())\n",
    "        batch_size, timesteps, C, H, W = X_stacked.size()\n",
    "        cnn_in  =  X_stacked.view(batch_size*timesteps, C, H, W)#need to be checked\n",
    "        \n",
    "        x = self.cnn1(cnn_in)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.cnn2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, ((10-self.k1+1)-self.k2+1)**2 * self.ch2)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = x.view(batch_size, timesteps, self.cnn_embed_dim)\n",
    "        x = x.permute(1,0,2)\n",
    "        embed = x\n",
    "\n",
    "        lstm_out, (h_n, h_c) = self.lstm(x.view(-1, batch_size, self.cnn_embed_dim))# -1 is timesteps here\n",
    "        \n",
    "        y_pred = self.fc2(lstm_out[-1].view(batch_size, -1))# -1 is self.h_d here\n",
    "        #y_pred = self.softmax(y_pred)\n",
    "\n",
    "        return y_pred, embed\n",
    "    \n",
    "class Linear_LSTM(nn.Module):\n",
    "    def __init__(self, input_ch = 1, N_classes = 2):\n",
    "        super(Linear_LSTM, self).__init__()\n",
    "        self.input_ch = input_ch \n",
    "        self.cnn_embed_dim = 8 #cnn-embedded dim \n",
    "        self.h_d = 8\n",
    "        self.num_layers = 2\n",
    "        self.N_classes = N_classes\n",
    "\n",
    "        self.fc1 = nn.Linear(10*10, self.cnn_embed_dim, bias = True) # 10 is image size\n",
    "#         self.lstm = nn.LSTM(self.cnn_embed_dim, self.h_d, self.num_layers)\n",
    "        self.lstm = nn.LSTM(self.cnn_embed_dim, self.h_d, self.num_layers)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.h_d, self.N_classes)\n",
    "#         self.fc1.weight.data.fill_(0.01)\n",
    "        self.softmax = nn.Softmax()\n",
    "      \n",
    "    #separate operations on each channel\n",
    "    def divide_inputs(self,X):\n",
    "        batch_size, timesteps, C, H, W = X.size()\n",
    "        x  =  X.view(batch_size*timesteps, C, H*W)\n",
    "        embed = self.fc1(x[:,0,:])\n",
    "\n",
    "        for i in range(C-1):\n",
    "            embed = torch.cat((embed,self.fc1(x[:,i+1,:])), dim = 1)\n",
    "\n",
    "        return embed             \n",
    "\n",
    "    def forward(self, X_stacked):\n",
    "        \n",
    "#         print(X_stacked.size())\n",
    "        batch_size, timesteps, C, H, W = X_stacked.size()\n",
    "        \n",
    "#         x  =  X_stacked.view(batch_size*timesteps, C*H*W)#need to be checked\n",
    "        \n",
    "#         x = self.fc1(x)\n",
    "#         x = nn.functional.relu(x)\n",
    "        x = self.divide_inputs(X_stacked)  \n",
    "        \n",
    "        x = x.view(batch_size, timesteps, self.cnn_embed_dim)\n",
    "        x = x.permute(1,0,2)\n",
    "        embed = x#here view may distort the X\n",
    "\n",
    "        lstm_out, (h_n, h_c) = self.lstm(x)# -1 is timesteps here\n",
    "                                           # here view distort the X\n",
    "     \n",
    "        y_pred = self.fc2(lstm_out[-1].view(batch_size, -1))# -1 is self.h_d here\n",
    "        #y_pred = self.softmax(y_pred)\n",
    "\n",
    "        return y_pred, embed\n",
    "\n",
    "#this model is constructed to mimic extracting 1d time information, checked\n",
    "class Linear_CNN1D(nn.Module):\n",
    "    def __init__(self, input_ch = 1, N_classes = 2):\n",
    "        super(Linear_CNN1D, self).__init__()\n",
    "        self.input_ch = input_ch         \n",
    "        self.ch1, self.ch2, self.ch3 = 6, 16, 26 \n",
    "        self.k1, self.k2, self.k3 = 5, 5, 3\n",
    "\n",
    "\n",
    "        self.embed_dim = 1 #cnn-embedded dim \n",
    "\n",
    "        self.N_classes = N_classes\n",
    "\n",
    "        self.cnn1 = nn.Conv1d(self.input_ch, self.ch1, self.k1, stride=2)\n",
    "        self.cnn2 = nn.Conv1d(self.ch1, self.ch2, self.k2, stride=2)\n",
    "        self.cnn3 = nn.Conv1d(self.ch2, self.ch3, self.k3, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1*10*10, self.embed_dim) # 10 is image size\n",
    "        self.fc2 = nn.Linear(26, 8)\n",
    "        self.fc3 = nn.Linear(8, self.N_classes)\n",
    "        \n",
    "        self.fc1.weight.data.fill_(0.01)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, X_stacked):\n",
    "        \n",
    "\n",
    "        batch_size, timesteps, C, H, W = X_stacked.size()\n",
    "        x  =  X_stacked.view(batch_size*timesteps, C*H*W)#need to be checked\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        #x = nn.functional.leaky_relu(x)\n",
    "\n",
    "        x = x.view(batch_size, 1, -1)\n",
    "        emd = x\n",
    "\n",
    "        x = nn.functional.max_pool1d(self.cnn1(x),2)\n",
    "        x = nn.functional.max_pool1d(self.cnn2(x),2)\n",
    "        x = nn.functional.max_pool1d(self.cnn3(x),2)\n",
    "\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "\n",
    "        y_pred = self.fc3(x)\n",
    "\n",
    "        y_pred = self.softmax(y_pred)\n",
    "\n",
    "        return y_pred, emd\n",
    "    \n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "class CNN_ENCODER_LSTM(nn.Module):\n",
    "    def __init__(self, input_ch = 1, N_classes = 2):\n",
    "        super(CNN_ENCODER_LSTM, self).__init__()\n",
    "        self.input_ch = input_ch\n",
    "        self.ch1, self.ch2 = 4, 8 \n",
    "        self.cnn_embed_dim = 8 #cnn-embedded dim \n",
    "        self.k1, self.k2 = 5, 5\n",
    "        self.h_d = 8\n",
    "        self.num_layers = 2\n",
    "        self.N_classes = N_classes\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(self.input_ch, self.ch1, self.k1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.ch1, self.ch2, self.k2),\n",
    "            nn.ReLU(True)\n",
    "            )\n",
    "\n",
    "        self.fc1 = nn.Linear(((10-self.k1+1)-self.k2+1)**2 * self.ch2, self.cnn_embed_dim) # 10 is image size\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.cnn_embed_dim, self.h_d, self.num_layers)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.h_d, self.N_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, X_stacked):\n",
    "        \n",
    "        #print(X_stacked.size())\n",
    "        batch_size, timesteps, C, H, W = X_stacked.size()\n",
    "        cnn_in  =  X_stacked.view(batch_size*timesteps, C, H, W)#need to be checked\n",
    "        \n",
    "        x = self.encoder(cnn_in)\n",
    "        \n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, ((10-self.k1+1)-self.k2+1)**2 * self.ch2)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = x.view(batch_size, timesteps, self.cnn_embed_dim)\n",
    "        x = x.permute(1,0,2)\n",
    "        embed = x\n",
    "\n",
    "        lstm_out, (h_n, h_c) = self.lstm(x.view(-1, batch_size, self.cnn_embed_dim))# -1 is timesteps here\n",
    "        \n",
    "        y_pred = self.fc2(lstm_out[-1].view(batch_size, -1))# -1 is self.h_d here\n",
    "        #y_pred = self.softmax(y_pred)\n",
    "\n",
    "        return y_pred, embed\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(4,8,kernel_size=5),\n",
    "            nn.ReLU(True))\n",
    "            \n",
    "        self.decoder = nn.Sequential(             \n",
    "            nn.ConvTranspose2d(8,4,kernel_size=5),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(4,1,kernel_size=5),\n",
    "            nn.ReLU(True))\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        embed = x\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x, embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": [
     0,
     3,
     23
    ]
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Build model\n",
    "#####################\n",
    "class newresnet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(newresnet, self).__init__()\n",
    "\n",
    "        self.newresnet18  = torchvision.models.resnet18(pretrained = True)\n",
    "        # for param in self.newresnet18.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # here in_feature is 512*block.expansion\n",
    "        num_ftrs = self.newresnet18.fc.in_features \n",
    "\n",
    "        # remove the last fc layer\n",
    "        # myresnet = torch.nn.Sequential(*list(myresnet.children())[:-1])\n",
    "        self.newresnet18.fc = torch.nn.Identity()\n",
    "        self.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
    "    def forward(self,x):\n",
    "        x = self.newresnet18(x)\n",
    "        # x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Resnet_LSTM(nn.Module):\n",
    "    def __init__(self, input_ch = 1, N_classes = 2):\n",
    "        super(Resnet_LSTM, self).__init__()\n",
    "        self.input_ch = input_ch \n",
    "        self.h_d = 16\n",
    "        self.num_layers = 2\n",
    "        self.N_classes = N_classes\n",
    "        \n",
    "        self.newresnet18  = torchvision.models.resnet18(pretrained = True)\n",
    "\n",
    "        self.cnn_embed_dim = self.newresnet18.fc.in_features \n",
    "\n",
    "        self.newresnet18.fc = torch.nn.Identity()\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.cnn_embed_dim, self.h_d, self.num_layers)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.h_d, self.N_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, X_stacked):\n",
    "        \n",
    "        #print(X_stacked.size())\n",
    "        X_stacked = torch.cat((X_stacked,X_stacked,X_stacked), dim = 2)\n",
    "        batch_size, timesteps, C, H, W = X_stacked.size()\n",
    "        \n",
    "        cnn_in  =  X_stacked.view(batch_size*timesteps, C, H, W)#need to be checked\n",
    "        \n",
    "        x = self.newresnet18(cnn_in)\n",
    "\n",
    "        lstm_out, (h_n, h_c) = self.lstm(x.view(-1, batch_size, self.cnn_embed_dim))# -1 is timesteps here\n",
    "        \n",
    "        y_pred = self.fc2(lstm_out[-1].view(batch_size, -1))# -1 is self.h_d here\n",
    "        #y_pred = self.softmax(y_pred)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Build model\n",
    "#####################\n",
    "class LSTM_Autoencoder(nn.Module):\n",
    "    def __init__(self, features_in, input_ch = 1):\n",
    "        super(LSTM_Autoencoder, self).__init__()\n",
    "        self.input_ch = input_ch\n",
    "        self.features_dim = features_in #cnn-embedded dim \n",
    "        self.h_d = self.features_dim # be aware of here ZXY \n",
    "        self.num_layers = 2\n",
    "        \n",
    "        self.lstm_en = nn.LSTM(self.features_dim, self.h_d, self.num_layers)\n",
    "        self.lstm_de = nn.LSTM(self.h_d, self.features_dim, self.num_layers)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def lstm_encoder(self, X_stacked):\n",
    "        \n",
    "        #print(X_stacked.size())\n",
    "        batch_size, timesteps, features_in = X_stacked.size()\n",
    "        extra = []\n",
    "        \n",
    "        x = X_stacked.permute(1,0,2)\n",
    "\n",
    "        rep, (h_n, h_c) = self.lstm_en(x)# -1 is timesteps here\n",
    "\n",
    "        return rep[-1, :, :].unsqueeze(0), extra\n",
    "\n",
    "    def forward(self, X_stacked):\n",
    "        \n",
    "        batch_size, timesteps, features_in = X_stacked.size()\n",
    "        rep, extra = self.lstm_encoder(X_stacked)\n",
    "        decoder_generated = []\n",
    "        \n",
    "        for i in range(timesteps):\n",
    "            rep, (h_n, h_c) = self.lstm_en(rep)\n",
    "            decoder_generated.append(rep)\n",
    "        \n",
    "        decoder_generated.reverse()\n",
    "        decoder_generated = torch.stack(decoder_generated, dim = 0)\n",
    "        decoder_generated = decoder_generated.squeeze()\n",
    "        decoder_generated = decoder_generated.permute(1,0,2)\n",
    "#         decoder_generated = decoder_generated.view(batch_size*timesteps,-1)\n",
    "        return decoder_generated, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Root Mean Squared Logarithmic Error Loss\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tuantle/regression-losses-pytorch\n",
    "#https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n",
    "class LogCoshLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "\n",
    "\n",
    "class XTanhLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(ey_t * torch.tanh(ey_t))\n",
    "\n",
    "\n",
    "class XSigmoidLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(2 * ey_t / (1 + torch.exp(-ey_t)) - ey_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # for loading\n",
    "# autoenoder = Autoencoder()# (use the class to initialize the model)\n",
    "# autoenoder.to(device)\n",
    "# autoenoder.eval()\n",
    "# fname = r'C:\\Users\\ZXY\\Desktop\\ASU\\Lab\\CellClassfier\\Autoencoder\\autoencoder.pt'\n",
    "# autoenoder.load_state_dict(torch.load(fname), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_Autoencoder(\n",
       "  (lstm_en): LSTM(11, 11, num_layers=2)\n",
       "  (lstm_de): LSTM(11, 11, num_layers=2)\n",
       "  (softmax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = CNN_LSTM()\n",
    "# model = Resnet_LSTM()\n",
    "# model = Linear_LSTM()\n",
    "# model = Linear_CNN1D()\n",
    "# model = CNN_ENCODER_LSTM()\n",
    "model = LSTM_Autoencoder(features_in = dataset.shape[2])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = r'C:\\Users\\ZXY\\Desktop\\ASU\\Lab\\CellClassfier\\Autoencoder\\autoencoder.pt'\n",
    "# model.load_state_dict(torch.load(fname), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "loss_fun = LogCoshLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# optimizer = torch.optim.Adam([\n",
    "#                               {'params':model.fc1.parameters(),'lr': learning_rate/1000},\n",
    "#                               {'params':model.lstm_en.parameters()},\n",
    "#                               {'params':model.lstm_de.parameters()},\n",
    "#                               {'params':model.fc2.parameters()},\n",
    "#                               {'params':model.encoder.parameters(), 'lr': 0},\n",
    "#                              ], \n",
    "#                              lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#For updating learning rate\n",
    "def update_lr(opt, divide = 2):\n",
    "    for param_group in opt.param_groups:\n",
    "        param_group['lr'] = param_group['lr']/divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Autoencoder(\n",
      "  (lstm_en): LSTM(11, 11, num_layers=2)\n",
      "  (lstm_de): LSTM(11, 11, num_layers=2)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "Model's state_dict:\n",
      "lstm_en.weight_ih_l0 \t torch.Size([44, 11])\n",
      "lstm_en.weight_hh_l0 \t torch.Size([44, 11])\n",
      "lstm_en.bias_ih_l0 \t torch.Size([44])\n",
      "lstm_en.bias_hh_l0 \t torch.Size([44])\n",
      "lstm_en.weight_ih_l1 \t torch.Size([44, 11])\n",
      "lstm_en.weight_hh_l1 \t torch.Size([44, 11])\n",
      "lstm_en.bias_ih_l1 \t torch.Size([44])\n",
      "lstm_en.bias_hh_l1 \t torch.Size([44])\n",
      "lstm_de.weight_ih_l0 \t torch.Size([44, 11])\n",
      "lstm_de.weight_hh_l0 \t torch.Size([44, 11])\n",
      "lstm_de.bias_ih_l0 \t torch.Size([44])\n",
      "lstm_de.bias_hh_l0 \t torch.Size([44])\n",
      "lstm_de.weight_ih_l1 \t torch.Size([44, 11])\n",
      "lstm_de.weight_hh_l1 \t torch.Size([44, 11])\n",
      "lstm_de.bias_ih_l1 \t torch.Size([44])\n",
      "lstm_de.bias_hh_l1 \t torch.Size([44])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [2596926745480, 2596926745240, 2596924709288, 2596915383496, 2596915382056, 2596915380616, 2596915380776, 2596915382216, 2596926744280, 2596915382296, 2596915381496, 2596915381576, 2596915382776, 2596915381096, 2596915382536, 2596935957464]}]\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(model)\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    }
   ],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    a = param_group['lr']\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step[1/17] Loss:1403664.2500\n",
      "Epoch [1/200], Step[2/17] Loss:3184913.2500\n",
      "Epoch [1/200], Step[3/17] Loss:1643791.3750\n",
      "Epoch [1/200], Step[4/17] Loss:1185794.3750\n",
      "Epoch [1/200], Step[5/17] Loss:1119393.5000\n",
      "Epoch [1/200], Step[6/17] Loss:1763011.7500\n",
      "Epoch [1/200], Step[7/17] Loss:773714.7500\n",
      "Epoch [1/200], Step[8/17] Loss:1657404.7500\n",
      "Epoch [1/200], Step[9/17] Loss:1908040.8750\n",
      "Epoch [1/200], Step[10/17] Loss:2244463.7500\n",
      "Epoch [1/200], Step[11/17] Loss:1561880.7500\n",
      "Epoch [1/200], Step[12/17] Loss:1765271.1250\n",
      "Epoch [1/200], Step[13/17] Loss:5818099.0000\n",
      "Epoch [1/200], Step[14/17] Loss:3944721.7500\n",
      "Epoch [1/200], Step[15/17] Loss:1457217.0000\n",
      "Epoch [1/200], Step[16/17] Loss:1081403.8750\n",
      "Epoch [1/200], Step[17/17] Loss:8842523.0000\n",
      "Epoch [2/200], Step[1/17] Loss:1626858.1250\n",
      "Epoch [2/200], Step[2/17] Loss:1375288.3750\n",
      "Epoch [2/200], Step[3/17] Loss:924208.6875\n",
      "Epoch [2/200], Step[4/17] Loss:1314991.3750\n",
      "Epoch [2/200], Step[5/17] Loss:1459619.8750\n",
      "Epoch [2/200], Step[6/17] Loss:3634486.5000\n",
      "Epoch [2/200], Step[7/17] Loss:4079889.2500\n",
      "Epoch [2/200], Step[8/17] Loss:4196838.0000\n",
      "Epoch [2/200], Step[9/17] Loss:1338644.7500\n",
      "Epoch [2/200], Step[10/17] Loss:1376022.3750\n",
      "Epoch [2/200], Step[11/17] Loss:880918.1875\n",
      "Epoch [2/200], Step[12/17] Loss:7494881.5000\n",
      "Epoch [2/200], Step[13/17] Loss:3529692.7500\n",
      "Epoch [2/200], Step[14/17] Loss:2092679.0000\n",
      "Epoch [2/200], Step[15/17] Loss:2974806.0000\n",
      "Epoch [2/200], Step[16/17] Loss:613864.2500\n",
      "Epoch [2/200], Step[17/17] Loss:956429.8750\n",
      "Epoch [3/200], Step[1/17] Loss:1666144.6250\n",
      "Epoch [3/200], Step[2/17] Loss:2161296.7500\n",
      "Epoch [3/200], Step[3/17] Loss:1245142.7500\n",
      "Epoch [3/200], Step[4/17] Loss:1515273.3750\n",
      "Epoch [3/200], Step[5/17] Loss:840351.7500\n",
      "Epoch [3/200], Step[6/17] Loss:3973894.5000\n",
      "Epoch [3/200], Step[7/17] Loss:1196908.0000\n",
      "Epoch [3/200], Step[8/17] Loss:1812118.3750\n",
      "Epoch [3/200], Step[9/17] Loss:4050252.7500\n",
      "Epoch [3/200], Step[10/17] Loss:1498197.7500\n",
      "Epoch [3/200], Step[11/17] Loss:777392.1875\n",
      "Epoch [3/200], Step[12/17] Loss:10021648.0000\n",
      "Epoch [3/200], Step[13/17] Loss:1342278.0000\n",
      "Epoch [3/200], Step[14/17] Loss:3796787.2500\n",
      "Epoch [3/200], Step[15/17] Loss:735748.6875\n",
      "Epoch [3/200], Step[16/17] Loss:2032530.6250\n",
      "Epoch [3/200], Step[17/17] Loss:1253924.3750\n",
      "Epoch [4/200], Step[1/17] Loss:2016025.6250\n",
      "Epoch [4/200], Step[2/17] Loss:4246651.0000\n",
      "Epoch [4/200], Step[3/17] Loss:4300276.0000\n",
      "Epoch [4/200], Step[4/17] Loss:2530956.0000\n",
      "Epoch [4/200], Step[5/17] Loss:691357.1250\n",
      "Epoch [4/200], Step[6/17] Loss:7346466.5000\n",
      "Epoch [4/200], Step[7/17] Loss:1374705.1250\n",
      "Epoch [4/200], Step[8/17] Loss:3402852.0000\n",
      "Epoch [4/200], Step[9/17] Loss:1434179.5000\n",
      "Epoch [4/200], Step[10/17] Loss:1019434.0000\n",
      "Epoch [4/200], Step[11/17] Loss:942583.0625\n",
      "Epoch [4/200], Step[12/17] Loss:1812184.1250\n",
      "Epoch [4/200], Step[13/17] Loss:1294351.6250\n",
      "Epoch [4/200], Step[14/17] Loss:1780669.2500\n",
      "Epoch [4/200], Step[15/17] Loss:1407877.6250\n",
      "Epoch [4/200], Step[16/17] Loss:3252361.5000\n",
      "Epoch [4/200], Step[17/17] Loss:1021379.8125\n",
      "Epoch [5/200], Step[1/17] Loss:1401207.7500\n",
      "Epoch [5/200], Step[2/17] Loss:1509048.8750\n",
      "Epoch [5/200], Step[3/17] Loss:1852815.3750\n",
      "Epoch [5/200], Step[4/17] Loss:3894980.7500\n",
      "Epoch [5/200], Step[5/17] Loss:877838.5000\n",
      "Epoch [5/200], Step[6/17] Loss:1024629.7500\n",
      "Epoch [5/200], Step[7/17] Loss:2048361.6250\n",
      "Epoch [5/200], Step[8/17] Loss:1791301.6250\n",
      "Epoch [5/200], Step[9/17] Loss:10199920.0000\n",
      "Epoch [5/200], Step[10/17] Loss:1018102.8125\n",
      "Epoch [5/200], Step[11/17] Loss:2575989.5000\n",
      "Epoch [5/200], Step[12/17] Loss:1172541.6250\n",
      "Epoch [5/200], Step[13/17] Loss:759894.3750\n",
      "Epoch [5/200], Step[14/17] Loss:4812682.5000\n",
      "Epoch [5/200], Step[15/17] Loss:1481786.7500\n",
      "Epoch [5/200], Step[16/17] Loss:1367174.1250\n",
      "Epoch [5/200], Step[17/17] Loss:2331328.5000\n",
      "Epoch [6/200], Step[1/17] Loss:788599.4375\n",
      "Epoch [6/200], Step[2/17] Loss:4266863.0000\n",
      "Epoch [6/200], Step[3/17] Loss:1464679.7500\n",
      "Epoch [6/200], Step[4/17] Loss:2827490.2500\n",
      "Epoch [6/200], Step[5/17] Loss:1037758.6250\n",
      "Epoch [6/200], Step[6/17] Loss:2751070.5000\n",
      "Epoch [6/200], Step[7/17] Loss:1495421.6250\n",
      "Epoch [6/200], Step[8/17] Loss:1297183.3750\n",
      "Epoch [6/200], Step[9/17] Loss:842007.5000\n",
      "Epoch [6/200], Step[10/17] Loss:2275277.0000\n",
      "Epoch [6/200], Step[11/17] Loss:10241085.0000\n",
      "Epoch [6/200], Step[12/17] Loss:1773353.8750\n",
      "Epoch [6/200], Step[13/17] Loss:2303465.2500\n",
      "Epoch [6/200], Step[14/17] Loss:1997646.5000\n",
      "Epoch [6/200], Step[15/17] Loss:2067450.8750\n",
      "Epoch [6/200], Step[16/17] Loss:1041306.3125\n",
      "Epoch [6/200], Step[17/17] Loss:1491394.7500\n",
      "Epoch [7/200], Step[1/17] Loss:5229275.5000\n",
      "Epoch [7/200], Step[2/17] Loss:7930387.5000\n",
      "Epoch [7/200], Step[3/17] Loss:2924712.2500\n",
      "Epoch [7/200], Step[4/17] Loss:801601.3750\n",
      "Epoch [7/200], Step[5/17] Loss:1914157.7500\n",
      "Epoch [7/200], Step[6/17] Loss:2592908.0000\n",
      "Epoch [7/200], Step[7/17] Loss:2120011.5000\n",
      "Epoch [7/200], Step[8/17] Loss:3762524.5000\n",
      "Epoch [7/200], Step[9/17] Loss:702056.9375\n",
      "Epoch [7/200], Step[10/17] Loss:2000674.7500\n",
      "Epoch [7/200], Step[11/17] Loss:975612.3125\n",
      "Epoch [7/200], Step[12/17] Loss:2512964.7500\n",
      "Epoch [7/200], Step[13/17] Loss:1060537.5000\n",
      "Epoch [7/200], Step[14/17] Loss:1159478.8750\n",
      "Epoch [7/200], Step[15/17] Loss:2392459.2500\n",
      "Epoch [7/200], Step[16/17] Loss:827422.0000\n",
      "Epoch [7/200], Step[17/17] Loss:954589.5625\n",
      "Epoch [8/200], Step[1/17] Loss:1628045.3750\n",
      "Epoch [8/200], Step[2/17] Loss:2542741.0000\n",
      "Epoch [8/200], Step[3/17] Loss:1660226.3750\n",
      "Epoch [8/200], Step[4/17] Loss:760185.1250\n",
      "Epoch [8/200], Step[5/17] Loss:1028147.1875\n",
      "Epoch [8/200], Step[6/17] Loss:1123471.1250\n",
      "Epoch [8/200], Step[7/17] Loss:890926.4375\n",
      "Epoch [8/200], Step[8/17] Loss:2975354.5000\n",
      "Epoch [8/200], Step[9/17] Loss:4146393.5000\n",
      "Epoch [8/200], Step[10/17] Loss:1612296.1250\n",
      "Epoch [8/200], Step[11/17] Loss:1394339.7500\n",
      "Epoch [8/200], Step[12/17] Loss:1246218.3750\n",
      "Epoch [8/200], Step[13/17] Loss:2672763.2500\n",
      "Epoch [8/200], Step[14/17] Loss:2010206.7500\n",
      "Epoch [8/200], Step[15/17] Loss:9221362.0000\n",
      "Epoch [8/200], Step[16/17] Loss:3463356.5000\n",
      "Epoch [8/200], Step[17/17] Loss:1607798.6250\n",
      "Epoch [9/200], Step[1/17] Loss:4484526.5000\n",
      "Epoch [9/200], Step[2/17] Loss:786175.0000\n",
      "Epoch [9/200], Step[3/17] Loss:1334889.3750\n",
      "Epoch [9/200], Step[4/17] Loss:3053972.0000\n",
      "Epoch [9/200], Step[5/17] Loss:1602411.1250\n",
      "Epoch [9/200], Step[6/17] Loss:1465458.5000\n",
      "Epoch [9/200], Step[7/17] Loss:1141850.1250\n",
      "Epoch [9/200], Step[8/17] Loss:1563399.7500\n",
      "Epoch [9/200], Step[9/17] Loss:4190506.0000\n",
      "Epoch [9/200], Step[10/17] Loss:10497827.0000\n",
      "Epoch [9/200], Step[11/17] Loss:896187.8125\n",
      "Epoch [9/200], Step[12/17] Loss:1718709.8750\n",
      "Epoch [9/200], Step[13/17] Loss:1081472.1250\n",
      "Epoch [9/200], Step[14/17] Loss:740935.6875\n",
      "Epoch [9/200], Step[15/17] Loss:1718701.1250\n",
      "Epoch [9/200], Step[16/17] Loss:2750857.7500\n",
      "Epoch [9/200], Step[17/17] Loss:805510.5000\n",
      "Epoch [10/200], Step[1/17] Loss:1286476.7500\n",
      "Epoch [10/200], Step[2/17] Loss:1158471.8750\n",
      "Epoch [10/200], Step[3/17] Loss:1616377.0000\n",
      "Epoch [10/200], Step[4/17] Loss:7417846.5000\n",
      "Epoch [10/200], Step[5/17] Loss:988895.4375\n",
      "Epoch [10/200], Step[6/17] Loss:1664676.5000\n",
      "Epoch [10/200], Step[7/17] Loss:2133506.2500\n",
      "Epoch [10/200], Step[8/17] Loss:3325162.5000\n",
      "Epoch [10/200], Step[9/17] Loss:2013408.3750\n",
      "Epoch [10/200], Step[10/17] Loss:3686719.7500\n",
      "Epoch [10/200], Step[11/17] Loss:589232.8750\n",
      "Epoch [10/200], Step[12/17] Loss:2874299.7500\n",
      "Epoch [10/200], Step[13/17] Loss:1651738.1250\n",
      "Epoch [10/200], Step[14/17] Loss:2278205.7500\n",
      "Epoch [10/200], Step[15/17] Loss:2521891.2500\n",
      "Epoch [10/200], Step[16/17] Loss:3940249.7500\n",
      "Epoch [10/200], Step[17/17] Loss:658692.9375\n",
      "Epoch [11/200], Step[1/17] Loss:1093781.8750\n",
      "Epoch [11/200], Step[2/17] Loss:3157727.5000\n",
      "Epoch [11/200], Step[3/17] Loss:1970198.6250\n",
      "Epoch [11/200], Step[4/17] Loss:7267687.5000\n",
      "Epoch [11/200], Step[5/17] Loss:1415530.6250\n",
      "Epoch [11/200], Step[6/17] Loss:911874.6875\n",
      "Epoch [11/200], Step[7/17] Loss:3383596.7500\n",
      "Epoch [11/200], Step[8/17] Loss:906597.1875\n",
      "Epoch [11/200], Step[9/17] Loss:2796279.2500\n",
      "Epoch [11/200], Step[10/17] Loss:1145346.2500\n",
      "Epoch [11/200], Step[11/17] Loss:897954.6875\n",
      "Epoch [11/200], Step[12/17] Loss:2208307.2500\n",
      "Epoch [11/200], Step[13/17] Loss:2253312.7500\n",
      "Epoch [11/200], Step[14/17] Loss:1651022.8750\n",
      "Epoch [11/200], Step[15/17] Loss:1979740.3750\n",
      "Epoch [11/200], Step[16/17] Loss:4969641.5000\n",
      "Epoch [11/200], Step[17/17] Loss:2059986.6250\n",
      "Epoch [12/200], Step[1/17] Loss:818038.9375\n",
      "Epoch [12/200], Step[2/17] Loss:4143112.0000\n",
      "Epoch [12/200], Step[3/17] Loss:8217456.0000\n",
      "Epoch [12/200], Step[4/17] Loss:579659.0625\n",
      "Epoch [12/200], Step[5/17] Loss:1847451.2500\n",
      "Epoch [12/200], Step[6/17] Loss:4321502.5000\n",
      "Epoch [12/200], Step[7/17] Loss:1000105.8125\n",
      "Epoch [12/200], Step[8/17] Loss:1330444.2500\n",
      "Epoch [12/200], Step[9/17] Loss:1251679.0000\n",
      "Epoch [12/200], Step[10/17] Loss:3254583.2500\n",
      "Epoch [12/200], Step[11/17] Loss:901460.4375\n",
      "Epoch [12/200], Step[12/17] Loss:1120769.8750\n",
      "Epoch [12/200], Step[13/17] Loss:1015483.5625\n",
      "Epoch [12/200], Step[14/17] Loss:3043971.2500\n",
      "Epoch [12/200], Step[15/17] Loss:4363482.5000\n",
      "Epoch [12/200], Step[16/17] Loss:1091076.0000\n",
      "Epoch [12/200], Step[17/17] Loss:1700993.6250\n",
      "Epoch [13/200], Step[1/17] Loss:3921757.5000\n",
      "Epoch [13/200], Step[2/17] Loss:3142195.7500\n",
      "Epoch [13/200], Step[3/17] Loss:1099461.7500\n",
      "Epoch [13/200], Step[4/17] Loss:836399.0000\n",
      "Epoch [13/200], Step[5/17] Loss:1811829.1250\n",
      "Epoch [13/200], Step[6/17] Loss:7483883.5000\n",
      "Epoch [13/200], Step[7/17] Loss:2223382.2500\n",
      "Epoch [13/200], Step[8/17] Loss:1704413.6250\n",
      "Epoch [13/200], Step[9/17] Loss:1867890.1250\n",
      "Epoch [13/200], Step[10/17] Loss:903444.7500\n",
      "Epoch [13/200], Step[11/17] Loss:827376.0625\n",
      "Epoch [13/200], Step[12/17] Loss:3623941.5000\n",
      "Epoch [13/200], Step[13/17] Loss:1291299.6250\n",
      "Epoch [13/200], Step[14/17] Loss:1086092.3750\n",
      "Epoch [13/200], Step[15/17] Loss:5490902.0000\n",
      "Epoch [13/200], Step[16/17] Loss:1350315.3750\n",
      "Epoch [13/200], Step[17/17] Loss:1252606.3750\n",
      "Epoch [14/200], Step[1/17] Loss:8502605.0000\n",
      "Epoch [14/200], Step[2/17] Loss:1402351.0000\n",
      "Epoch [14/200], Step[3/17] Loss:3386972.2500\n",
      "Epoch [14/200], Step[4/17] Loss:948311.2500\n",
      "Epoch [14/200], Step[5/17] Loss:1276443.2500\n",
      "Epoch [14/200], Step[6/17] Loss:5018102.5000\n",
      "Epoch [14/200], Step[7/17] Loss:3228823.2500\n",
      "Epoch [14/200], Step[8/17] Loss:755149.0625\n",
      "Epoch [14/200], Step[9/17] Loss:1328722.2500\n",
      "Epoch [14/200], Step[10/17] Loss:2187885.2500\n",
      "Epoch [14/200], Step[11/17] Loss:1685272.5000\n",
      "Epoch [14/200], Step[12/17] Loss:1308834.2500\n",
      "Epoch [14/200], Step[13/17] Loss:1248140.5000\n",
      "Epoch [14/200], Step[14/17] Loss:3108606.7500\n",
      "Epoch [14/200], Step[15/17] Loss:1951147.1250\n",
      "Epoch [14/200], Step[16/17] Loss:1128392.8750\n",
      "Epoch [14/200], Step[17/17] Loss:1497307.8750\n",
      "Epoch [15/200], Step[1/17] Loss:3435583.2500\n",
      "Epoch [15/200], Step[2/17] Loss:1409927.6250\n",
      "Epoch [15/200], Step[3/17] Loss:957076.4375\n",
      "Epoch [15/200], Step[4/17] Loss:1090518.6250\n",
      "Epoch [15/200], Step[5/17] Loss:3647058.0000\n",
      "Epoch [15/200], Step[6/17] Loss:762588.6875\n",
      "Epoch [15/200], Step[7/17] Loss:2748989.0000\n",
      "Epoch [15/200], Step[8/17] Loss:2603141.0000\n",
      "Epoch [15/200], Step[9/17] Loss:2552566.5000\n",
      "Epoch [15/200], Step[10/17] Loss:1089036.6250\n",
      "Epoch [15/200], Step[11/17] Loss:3691632.7500\n",
      "Epoch [15/200], Step[12/17] Loss:1055782.2500\n",
      "Epoch [15/200], Step[13/17] Loss:8723078.0000\n",
      "Epoch [15/200], Step[14/17] Loss:1859117.3750\n",
      "Epoch [15/200], Step[15/17] Loss:822278.3125\n",
      "Epoch [15/200], Step[16/17] Loss:1881286.1250\n",
      "Epoch [15/200], Step[17/17] Loss:1664812.1250\n",
      "Epoch [16/200], Step[1/17] Loss:1329675.2500\n",
      "Epoch [16/200], Step[2/17] Loss:1224130.2500\n",
      "Epoch [16/200], Step[3/17] Loss:2394693.0000\n",
      "Epoch [16/200], Step[4/17] Loss:1038466.6875\n",
      "Epoch [16/200], Step[5/17] Loss:1185329.8750\n",
      "Epoch [16/200], Step[6/17] Loss:1375934.8750\n",
      "Epoch [16/200], Step[7/17] Loss:1181656.0000\n",
      "Epoch [16/200], Step[8/17] Loss:8499880.0000\n",
      "Epoch [16/200], Step[9/17] Loss:1765993.3750\n",
      "Epoch [16/200], Step[10/17] Loss:3569209.7500\n",
      "Epoch [16/200], Step[11/17] Loss:947572.1250\n",
      "Epoch [16/200], Step[12/17] Loss:3436940.5000\n",
      "Epoch [16/200], Step[13/17] Loss:1040422.7500\n",
      "Epoch [16/200], Step[14/17] Loss:1457870.6250\n",
      "Epoch [16/200], Step[15/17] Loss:2286386.0000\n",
      "Epoch [16/200], Step[16/17] Loss:4070905.5000\n",
      "Epoch [16/200], Step[17/17] Loss:3541228.2500\n",
      "Epoch [17/200], Step[1/17] Loss:2279502.7500\n",
      "Epoch [17/200], Step[2/17] Loss:3560120.0000\n",
      "Epoch [17/200], Step[3/17] Loss:2138079.7500\n",
      "Epoch [17/200], Step[4/17] Loss:3511140.5000\n",
      "Epoch [17/200], Step[5/17] Loss:833169.2500\n",
      "Epoch [17/200], Step[6/17] Loss:1625181.7500\n",
      "Epoch [17/200], Step[7/17] Loss:8128949.5000\n",
      "Epoch [17/200], Step[8/17] Loss:1338336.6250\n",
      "Epoch [17/200], Step[9/17] Loss:1468199.7500\n",
      "Epoch [17/200], Step[10/17] Loss:1071728.8750\n",
      "Epoch [17/200], Step[11/17] Loss:1002599.0000\n",
      "Epoch [17/200], Step[12/17] Loss:1066560.5000\n",
      "Epoch [17/200], Step[13/17] Loss:1737397.7500\n",
      "Epoch [17/200], Step[14/17] Loss:872740.3750\n",
      "Epoch [17/200], Step[15/17] Loss:2709808.0000\n",
      "Epoch [17/200], Step[16/17] Loss:2734788.0000\n",
      "Epoch [17/200], Step[17/17] Loss:4435704.5000\n",
      "Epoch [18/200], Step[1/17] Loss:2488460.0000\n",
      "Epoch [18/200], Step[2/17] Loss:1137673.3750\n",
      "Epoch [18/200], Step[3/17] Loss:1559605.2500\n",
      "Epoch [18/200], Step[4/17] Loss:2350188.7500\n",
      "Epoch [18/200], Step[5/17] Loss:7274674.0000\n",
      "Epoch [18/200], Step[6/17] Loss:4448938.5000\n",
      "Epoch [18/200], Step[7/17] Loss:1451997.5000\n",
      "Epoch [18/200], Step[8/17] Loss:1064545.6250\n",
      "Epoch [18/200], Step[9/17] Loss:3718796.5000\n",
      "Epoch [18/200], Step[10/17] Loss:720384.9375\n",
      "Epoch [18/200], Step[11/17] Loss:1119844.6250\n",
      "Epoch [18/200], Step[12/17] Loss:3539852.5000\n",
      "Epoch [18/200], Step[13/17] Loss:2633014.0000\n",
      "Epoch [18/200], Step[14/17] Loss:1868597.5000\n",
      "Epoch [18/200], Step[15/17] Loss:2456068.5000\n",
      "Epoch [18/200], Step[16/17] Loss:880771.7500\n",
      "Epoch [18/200], Step[17/17] Loss:1192487.7500\n",
      "Epoch [19/200], Step[1/17] Loss:1234880.1250\n",
      "Epoch [19/200], Step[2/17] Loss:6337829.5000\n",
      "Epoch [19/200], Step[3/17] Loss:893364.6875\n",
      "Epoch [19/200], Step[4/17] Loss:2493390.5000\n",
      "Epoch [19/200], Step[5/17] Loss:1043677.8125\n",
      "Epoch [19/200], Step[6/17] Loss:3130127.5000\n",
      "Epoch [19/200], Step[7/17] Loss:1623093.8750\n",
      "Epoch [19/200], Step[8/17] Loss:782167.7500\n",
      "Epoch [19/200], Step[9/17] Loss:1168095.8750\n",
      "Epoch [19/200], Step[10/17] Loss:1947485.3750\n",
      "Epoch [19/200], Step[11/17] Loss:8146921.5000\n",
      "Epoch [19/200], Step[12/17] Loss:2058559.3750\n",
      "Epoch [19/200], Step[13/17] Loss:1205383.2500\n",
      "Epoch [19/200], Step[14/17] Loss:2510119.5000\n",
      "Epoch [19/200], Step[15/17] Loss:2532396.7500\n",
      "Epoch [19/200], Step[16/17] Loss:1868558.2500\n",
      "Epoch [19/200], Step[17/17] Loss:869236.6250\n",
      "Epoch [20/200], Step[1/17] Loss:1557030.3750\n",
      "Epoch [20/200], Step[2/17] Loss:1366441.5000\n",
      "Epoch [20/200], Step[3/17] Loss:1751791.2500\n",
      "Epoch [20/200], Step[4/17] Loss:4755633.0000\n",
      "Epoch [20/200], Step[5/17] Loss:7360467.0000\n",
      "Epoch [20/200], Step[6/17] Loss:1885242.6250\n",
      "Epoch [20/200], Step[7/17] Loss:1539218.6250\n",
      "Epoch [20/200], Step[8/17] Loss:828633.5000\n",
      "Epoch [20/200], Step[9/17] Loss:993801.0625\n",
      "Epoch [20/200], Step[10/17] Loss:4233871.0000\n",
      "Epoch [20/200], Step[11/17] Loss:1450580.5000\n",
      "Epoch [20/200], Step[12/17] Loss:756880.9375\n",
      "Epoch [20/200], Step[13/17] Loss:1360009.5000\n",
      "Epoch [20/200], Step[14/17] Loss:1685355.1250\n",
      "Epoch [20/200], Step[15/17] Loss:6196214.5000\n",
      "Epoch [20/200], Step[16/17] Loss:1004564.7500\n",
      "Epoch [20/200], Step[17/17] Loss:1177317.8750\n",
      "Epoch [21/200], Step[1/17] Loss:3564615.0000\n",
      "Epoch [21/200], Step[2/17] Loss:1428641.0000\n",
      "Epoch [21/200], Step[3/17] Loss:2443242.2500\n",
      "Epoch [21/200], Step[4/17] Loss:3967641.7500\n",
      "Epoch [21/200], Step[5/17] Loss:1179906.6250\n",
      "Epoch [21/200], Step[6/17] Loss:2492626.7500\n",
      "Epoch [21/200], Step[7/17] Loss:4638271.5000\n",
      "Epoch [21/200], Step[8/17] Loss:1147181.3750\n",
      "Epoch [21/200], Step[9/17] Loss:2018742.5000\n",
      "Epoch [21/200], Step[10/17] Loss:1924980.8750\n",
      "Epoch [21/200], Step[11/17] Loss:7306747.0000\n",
      "Epoch [21/200], Step[12/17] Loss:1785029.8750\n",
      "Epoch [21/200], Step[13/17] Loss:827313.4375\n",
      "Epoch [21/200], Step[14/17] Loss:2541567.5000\n",
      "Epoch [21/200], Step[15/17] Loss:1089782.0000\n",
      "Epoch [21/200], Step[16/17] Loss:801792.0625\n",
      "Epoch [21/200], Step[17/17] Loss:645195.9375\n",
      "Epoch [22/200], Step[1/17] Loss:1992999.3750\n",
      "Epoch [22/200], Step[2/17] Loss:7298783.0000\n",
      "Epoch [22/200], Step[3/17] Loss:1648046.3750\n",
      "Epoch [22/200], Step[4/17] Loss:2624048.7500\n",
      "Epoch [22/200], Step[5/17] Loss:884125.1875\n",
      "Epoch [22/200], Step[6/17] Loss:2918356.7500\n",
      "Epoch [22/200], Step[7/17] Loss:1397624.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/200], Step[8/17] Loss:1026842.6250\n",
      "Epoch [22/200], Step[9/17] Loss:1805098.2500\n",
      "Epoch [22/200], Step[10/17] Loss:4277378.0000\n",
      "Epoch [22/200], Step[11/17] Loss:1219002.1250\n",
      "Epoch [22/200], Step[12/17] Loss:1315884.1250\n",
      "Epoch [22/200], Step[13/17] Loss:2924363.2500\n",
      "Epoch [22/200], Step[14/17] Loss:3748002.0000\n",
      "Epoch [22/200], Step[15/17] Loss:1536500.1250\n",
      "Epoch [22/200], Step[16/17] Loss:1727824.1250\n",
      "Epoch [22/200], Step[17/17] Loss:1646060.0000\n",
      "Epoch [23/200], Step[1/17] Loss:8640776.0000\n",
      "Epoch [23/200], Step[2/17] Loss:1847287.1250\n",
      "Epoch [23/200], Step[3/17] Loss:2076060.6250\n",
      "Epoch [23/200], Step[4/17] Loss:2114875.5000\n",
      "Epoch [23/200], Step[5/17] Loss:2955992.7500\n",
      "Epoch [23/200], Step[6/17] Loss:1258157.8750\n",
      "Epoch [23/200], Step[7/17] Loss:2923785.0000\n",
      "Epoch [23/200], Step[8/17] Loss:948054.3125\n",
      "Epoch [23/200], Step[9/17] Loss:3871209.5000\n",
      "Epoch [23/200], Step[10/17] Loss:3556028.2500\n",
      "Epoch [23/200], Step[11/17] Loss:1269393.7500\n",
      "Epoch [23/200], Step[12/17] Loss:1201642.3750\n",
      "Epoch [23/200], Step[13/17] Loss:1439909.8750\n",
      "Epoch [23/200], Step[14/17] Loss:1571354.7500\n",
      "Epoch [23/200], Step[15/17] Loss:1741282.2500\n",
      "Epoch [23/200], Step[16/17] Loss:1232054.8750\n",
      "Epoch [23/200], Step[17/17] Loss:1273151.6250\n",
      "Epoch [24/200], Step[1/17] Loss:7807746.5000\n",
      "Epoch [24/200], Step[2/17] Loss:2324968.7500\n",
      "Epoch [24/200], Step[3/17] Loss:4986985.5000\n",
      "Epoch [24/200], Step[4/17] Loss:1911732.1250\n",
      "Epoch [24/200], Step[5/17] Loss:1722501.0000\n",
      "Epoch [24/200], Step[6/17] Loss:1329205.7500\n",
      "Epoch [24/200], Step[7/17] Loss:2280461.0000\n",
      "Epoch [24/200], Step[8/17] Loss:1167785.0000\n",
      "Epoch [24/200], Step[9/17] Loss:663553.9375\n",
      "Epoch [24/200], Step[10/17] Loss:1916799.6250\n",
      "Epoch [24/200], Step[11/17] Loss:1344830.7500\n",
      "Epoch [24/200], Step[12/17] Loss:1094861.7500\n",
      "Epoch [24/200], Step[13/17] Loss:2704189.2500\n",
      "Epoch [24/200], Step[14/17] Loss:3107217.7500\n",
      "Epoch [24/200], Step[15/17] Loss:1508293.6250\n",
      "Epoch [24/200], Step[16/17] Loss:2230242.5000\n",
      "Epoch [24/200], Step[17/17] Loss:1945752.5000\n",
      "Epoch [25/200], Step[1/17] Loss:1089538.1250\n",
      "Epoch [25/200], Step[2/17] Loss:1430467.2500\n",
      "Epoch [25/200], Step[3/17] Loss:3768259.2500\n",
      "Epoch [25/200], Step[4/17] Loss:2293640.0000\n",
      "Epoch [25/200], Step[5/17] Loss:797868.1250\n",
      "Epoch [25/200], Step[6/17] Loss:2406319.0000\n",
      "Epoch [25/200], Step[7/17] Loss:1201511.2500\n",
      "Epoch [25/200], Step[8/17] Loss:1115224.3750\n",
      "Epoch [25/200], Step[9/17] Loss:865255.5625\n",
      "Epoch [25/200], Step[10/17] Loss:933640.4375\n",
      "Epoch [25/200], Step[11/17] Loss:1566916.6250\n",
      "Epoch [25/200], Step[12/17] Loss:945499.3125\n",
      "Epoch [25/200], Step[13/17] Loss:1637508.3750\n",
      "Epoch [25/200], Step[14/17] Loss:8415757.0000\n",
      "Epoch [25/200], Step[15/17] Loss:4955611.5000\n",
      "Epoch [25/200], Step[16/17] Loss:2409028.0000\n",
      "Epoch [25/200], Step[17/17] Loss:4738774.5000\n",
      "Epoch [26/200], Step[1/17] Loss:2127540.7500\n",
      "Epoch [26/200], Step[2/17] Loss:3012283.7500\n",
      "Epoch [26/200], Step[3/17] Loss:716515.1875\n",
      "Epoch [26/200], Step[4/17] Loss:913559.1875\n",
      "Epoch [26/200], Step[5/17] Loss:8500923.0000\n",
      "Epoch [26/200], Step[6/17] Loss:2026492.7500\n",
      "Epoch [26/200], Step[7/17] Loss:3382736.7500\n",
      "Epoch [26/200], Step[8/17] Loss:1180208.7500\n",
      "Epoch [26/200], Step[9/17] Loss:1234166.0000\n",
      "Epoch [26/200], Step[10/17] Loss:1046393.8125\n",
      "Epoch [26/200], Step[11/17] Loss:4708529.0000\n",
      "Epoch [26/200], Step[12/17] Loss:1545954.6250\n",
      "Epoch [26/200], Step[13/17] Loss:1621398.2500\n",
      "Epoch [26/200], Step[14/17] Loss:2029051.6250\n",
      "Epoch [26/200], Step[15/17] Loss:1072909.6250\n",
      "Epoch [26/200], Step[16/17] Loss:3573293.0000\n",
      "Epoch [26/200], Step[17/17] Loss:1218882.0000\n",
      "Epoch [27/200], Step[1/17] Loss:1056711.6250\n",
      "Epoch [27/200], Step[2/17] Loss:1669675.3750\n",
      "Epoch [27/200], Step[3/17] Loss:2701088.2500\n",
      "Epoch [27/200], Step[4/17] Loss:1188292.8750\n",
      "Epoch [27/200], Step[5/17] Loss:8442886.0000\n",
      "Epoch [27/200], Step[6/17] Loss:847280.7500\n",
      "Epoch [27/200], Step[7/17] Loss:414844.3438\n",
      "Epoch [27/200], Step[8/17] Loss:4260563.5000\n",
      "Epoch [27/200], Step[9/17] Loss:1730268.0000\n",
      "Epoch [27/200], Step[10/17] Loss:1027106.1875\n",
      "Epoch [27/200], Step[11/17] Loss:1700939.2500\n",
      "Epoch [27/200], Step[12/17] Loss:1093651.6250\n",
      "Epoch [27/200], Step[13/17] Loss:5322683.5000\n",
      "Epoch [27/200], Step[14/17] Loss:3852069.2500\n",
      "Epoch [27/200], Step[15/17] Loss:1613489.3750\n",
      "Epoch [27/200], Step[16/17] Loss:1501123.8750\n",
      "Epoch [27/200], Step[17/17] Loss:1550304.5000\n",
      "Epoch [28/200], Step[1/17] Loss:10058617.0000\n",
      "Epoch [28/200], Step[2/17] Loss:2517399.5000\n",
      "Epoch [28/200], Step[3/17] Loss:1740655.3750\n",
      "Epoch [28/200], Step[4/17] Loss:1375933.6250\n",
      "Epoch [28/200], Step[5/17] Loss:1683006.7500\n",
      "Epoch [28/200], Step[6/17] Loss:848180.3125\n",
      "Epoch [28/200], Step[7/17] Loss:1412084.1250\n",
      "Epoch [28/200], Step[8/17] Loss:582308.3125\n",
      "Epoch [28/200], Step[9/17] Loss:1844814.6250\n",
      "Epoch [28/200], Step[10/17] Loss:2851521.0000\n",
      "Epoch [28/200], Step[11/17] Loss:1424885.5000\n",
      "Epoch [28/200], Step[12/17] Loss:2692849.7500\n",
      "Epoch [28/200], Step[13/17] Loss:1208464.0000\n",
      "Epoch [28/200], Step[14/17] Loss:2001726.3750\n",
      "Epoch [28/200], Step[15/17] Loss:3807841.0000\n",
      "Epoch [28/200], Step[16/17] Loss:1285270.1250\n",
      "Epoch [28/200], Step[17/17] Loss:2888291.2500\n",
      "Epoch [29/200], Step[1/17] Loss:3317763.2500\n",
      "Epoch [29/200], Step[2/17] Loss:2144865.5000\n",
      "Epoch [29/200], Step[3/17] Loss:976409.0000\n",
      "Epoch [29/200], Step[4/17] Loss:7998870.0000\n",
      "Epoch [29/200], Step[5/17] Loss:785400.0625\n",
      "Epoch [29/200], Step[6/17] Loss:2737700.0000\n",
      "Epoch [29/200], Step[7/17] Loss:996723.0625\n",
      "Epoch [29/200], Step[8/17] Loss:1392154.2500\n",
      "Epoch [29/200], Step[9/17] Loss:2276674.0000\n",
      "Epoch [29/200], Step[10/17] Loss:2943607.2500\n",
      "Epoch [29/200], Step[11/17] Loss:633190.8125\n",
      "Epoch [29/200], Step[12/17] Loss:2126240.2500\n",
      "Epoch [29/200], Step[13/17] Loss:1501689.8750\n",
      "Epoch [29/200], Step[14/17] Loss:841082.7500\n",
      "Epoch [29/200], Step[15/17] Loss:2159512.0000\n",
      "Epoch [29/200], Step[16/17] Loss:1076391.0000\n",
      "Epoch [29/200], Step[17/17] Loss:7106488.0000\n",
      "Epoch [30/200], Step[1/17] Loss:1361738.8750\n",
      "Epoch [30/200], Step[2/17] Loss:2272950.0000\n",
      "Epoch [30/200], Step[3/17] Loss:1576592.5000\n",
      "Epoch [30/200], Step[4/17] Loss:2749356.2500\n",
      "Epoch [30/200], Step[5/17] Loss:1308542.1250\n",
      "Epoch [30/200], Step[6/17] Loss:1483199.0000\n",
      "Epoch [30/200], Step[7/17] Loss:3303989.5000\n",
      "Epoch [30/200], Step[8/17] Loss:7070736.5000\n",
      "Epoch [30/200], Step[9/17] Loss:2767823.5000\n",
      "Epoch [30/200], Step[10/17] Loss:1523742.3750\n",
      "Epoch [30/200], Step[11/17] Loss:2930110.5000\n",
      "Epoch [30/200], Step[12/17] Loss:4302708.5000\n",
      "Epoch [30/200], Step[13/17] Loss:731213.7500\n",
      "Epoch [30/200], Step[14/17] Loss:1888788.2500\n",
      "Epoch [30/200], Step[15/17] Loss:2016677.7500\n",
      "Epoch [30/200], Step[16/17] Loss:1566289.5000\n",
      "Epoch [30/200], Step[17/17] Loss:1018874.6250\n",
      "Epoch [31/200], Step[1/17] Loss:2497948.7500\n",
      "Epoch [31/200], Step[2/17] Loss:8839862.0000\n",
      "Epoch [31/200], Step[3/17] Loss:1081801.8750\n",
      "Epoch [31/200], Step[4/17] Loss:3218489.7500\n",
      "Epoch [31/200], Step[5/17] Loss:1127679.0000\n",
      "Epoch [31/200], Step[6/17] Loss:1724648.7500\n",
      "Epoch [31/200], Step[7/17] Loss:5566811.5000\n",
      "Epoch [31/200], Step[8/17] Loss:1151867.5000\n",
      "Epoch [31/200], Step[9/17] Loss:1864026.0000\n",
      "Epoch [31/200], Step[10/17] Loss:1181728.1250\n",
      "Epoch [31/200], Step[11/17] Loss:1196096.7500\n",
      "Epoch [31/200], Step[12/17] Loss:772179.0000\n",
      "Epoch [31/200], Step[13/17] Loss:3349586.2500\n",
      "Epoch [31/200], Step[14/17] Loss:1513000.7500\n",
      "Epoch [31/200], Step[15/17] Loss:2974296.0000\n",
      "Epoch [31/200], Step[16/17] Loss:675630.3750\n",
      "Epoch [31/200], Step[17/17] Loss:1165096.3750\n",
      "Epoch [32/200], Step[1/17] Loss:1050316.7500\n",
      "Epoch [32/200], Step[2/17] Loss:1385867.3750\n",
      "Epoch [32/200], Step[3/17] Loss:1738854.0000\n",
      "Epoch [32/200], Step[4/17] Loss:2333633.5000\n",
      "Epoch [32/200], Step[5/17] Loss:1019935.3125\n",
      "Epoch [32/200], Step[6/17] Loss:7420764.5000\n",
      "Epoch [32/200], Step[7/17] Loss:1209803.6250\n",
      "Epoch [32/200], Step[8/17] Loss:3659505.7500\n",
      "Epoch [32/200], Step[9/17] Loss:1620278.1250\n",
      "Epoch [32/200], Step[10/17] Loss:1768833.1250\n",
      "Epoch [32/200], Step[11/17] Loss:1426517.2500\n",
      "Epoch [32/200], Step[12/17] Loss:2165540.0000\n",
      "Epoch [32/200], Step[13/17] Loss:5091045.5000\n",
      "Epoch [32/200], Step[14/17] Loss:922025.1250\n",
      "Epoch [32/200], Step[15/17] Loss:3314165.5000\n",
      "Epoch [32/200], Step[16/17] Loss:1557578.1250\n",
      "Epoch [32/200], Step[17/17] Loss:2458619.2500\n",
      "Epoch [33/200], Step[1/17] Loss:853092.0000\n",
      "Epoch [33/200], Step[2/17] Loss:1877779.6250\n",
      "Epoch [33/200], Step[3/17] Loss:1065164.6250\n",
      "Epoch [33/200], Step[4/17] Loss:1656102.1250\n",
      "Epoch [33/200], Step[5/17] Loss:879589.0000\n",
      "Epoch [33/200], Step[6/17] Loss:2198917.2500\n",
      "Epoch [33/200], Step[7/17] Loss:485313.0938\n",
      "Epoch [33/200], Step[8/17] Loss:4054380.5000\n",
      "Epoch [33/200], Step[9/17] Loss:3012644.5000\n",
      "Epoch [33/200], Step[10/17] Loss:1334474.5000\n",
      "Epoch [33/200], Step[11/17] Loss:3134980.2500\n",
      "Epoch [33/200], Step[12/17] Loss:2485648.2500\n",
      "Epoch [33/200], Step[13/17] Loss:10443665.0000\n",
      "Epoch [33/200], Step[14/17] Loss:1746152.5000\n",
      "Epoch [33/200], Step[15/17] Loss:1359401.2500\n",
      "Epoch [33/200], Step[16/17] Loss:1658749.6250\n",
      "Epoch [33/200], Step[17/17] Loss:1767675.3750\n",
      "Epoch [34/200], Step[1/17] Loss:1515060.2500\n",
      "Epoch [34/200], Step[2/17] Loss:1325271.5000\n",
      "Epoch [34/200], Step[3/17] Loss:1743307.6250\n",
      "Epoch [34/200], Step[4/17] Loss:3111725.7500\n",
      "Epoch [34/200], Step[5/17] Loss:1141280.5000\n",
      "Epoch [34/200], Step[6/17] Loss:884068.9375\n",
      "Epoch [34/200], Step[7/17] Loss:3717326.2500\n",
      "Epoch [34/200], Step[8/17] Loss:1010546.5000\n",
      "Epoch [34/200], Step[9/17] Loss:1019059.4375\n",
      "Epoch [34/200], Step[10/17] Loss:3413291.2500\n",
      "Epoch [34/200], Step[11/17] Loss:4050785.2500\n",
      "Epoch [34/200], Step[12/17] Loss:7859195.5000\n",
      "Epoch [34/200], Step[13/17] Loss:1438080.5000\n",
      "Epoch [34/200], Step[14/17] Loss:1518581.6250\n",
      "Epoch [34/200], Step[15/17] Loss:2380598.5000\n",
      "Epoch [34/200], Step[16/17] Loss:2786634.5000\n",
      "Epoch [34/200], Step[17/17] Loss:944588.1875\n",
      "Epoch [35/200], Step[1/17] Loss:3313349.7500\n",
      "Epoch [35/200], Step[2/17] Loss:1448584.8750\n",
      "Epoch [35/200], Step[3/17] Loss:1040764.3750\n",
      "Epoch [35/200], Step[4/17] Loss:7398216.0000\n",
      "Epoch [35/200], Step[5/17] Loss:727748.4375\n",
      "Epoch [35/200], Step[6/17] Loss:2911664.7500\n",
      "Epoch [35/200], Step[7/17] Loss:2401831.0000\n",
      "Epoch [35/200], Step[8/17] Loss:4233806.0000\n",
      "Epoch [35/200], Step[9/17] Loss:2321752.7500\n",
      "Epoch [35/200], Step[10/17] Loss:731045.0625\n",
      "Epoch [35/200], Step[11/17] Loss:2799751.5000\n",
      "Epoch [35/200], Step[12/17] Loss:658914.0000\n",
      "Epoch [35/200], Step[13/17] Loss:1374515.8750\n",
      "Epoch [35/200], Step[14/17] Loss:3305592.5000\n",
      "Epoch [35/200], Step[15/17] Loss:2999899.2500\n",
      "Epoch [35/200], Step[16/17] Loss:1010535.9375\n",
      "Epoch [35/200], Step[17/17] Loss:1236084.3750\n",
      "Epoch [36/200], Step[1/17] Loss:1970433.8750\n",
      "Epoch [36/200], Step[2/17] Loss:596364.1875\n",
      "Epoch [36/200], Step[3/17] Loss:2614277.2500\n",
      "Epoch [36/200], Step[4/17] Loss:8665477.0000\n",
      "Epoch [36/200], Step[5/17] Loss:2258085.5000\n",
      "Epoch [36/200], Step[6/17] Loss:1424770.7500\n",
      "Epoch [36/200], Step[7/17] Loss:794421.5625\n",
      "Epoch [36/200], Step[8/17] Loss:1335388.5000\n",
      "Epoch [36/200], Step[9/17] Loss:1316141.8750\n",
      "Epoch [36/200], Step[10/17] Loss:3470907.2500\n",
      "Epoch [36/200], Step[11/17] Loss:3712499.5000\n",
      "Epoch [36/200], Step[12/17] Loss:892762.7500\n",
      "Epoch [36/200], Step[13/17] Loss:665376.0625\n",
      "Epoch [36/200], Step[14/17] Loss:3377500.0000\n",
      "Epoch [36/200], Step[15/17] Loss:1911894.3750\n",
      "Epoch [36/200], Step[16/17] Loss:2512404.0000\n",
      "Epoch [36/200], Step[17/17] Loss:2662873.7500\n",
      "Epoch [37/200], Step[1/17] Loss:896466.0000\n",
      "Epoch [37/200], Step[2/17] Loss:1984050.0000\n",
      "Epoch [37/200], Step[3/17] Loss:2167255.7500\n",
      "Epoch [37/200], Step[4/17] Loss:725544.1250\n",
      "Epoch [37/200], Step[5/17] Loss:1516295.7500\n",
      "Epoch [37/200], Step[6/17] Loss:7547682.5000\n",
      "Epoch [37/200], Step[7/17] Loss:1280266.7500\n",
      "Epoch [37/200], Step[8/17] Loss:1259867.2500\n",
      "Epoch [37/200], Step[9/17] Loss:1438291.1250\n",
      "Epoch [37/200], Step[10/17] Loss:2840013.0000\n",
      "Epoch [37/200], Step[11/17] Loss:3554635.2500\n",
      "Epoch [37/200], Step[12/17] Loss:7258972.0000\n",
      "Epoch [37/200], Step[13/17] Loss:1026501.3750\n",
      "Epoch [37/200], Step[14/17] Loss:664593.5625\n",
      "Epoch [37/200], Step[15/17] Loss:2083316.1250\n",
      "Epoch [37/200], Step[16/17] Loss:1757803.2500\n",
      "Epoch [37/200], Step[17/17] Loss:2068597.2500\n",
      "Epoch [38/200], Step[1/17] Loss:1424049.3750\n",
      "Epoch [38/200], Step[2/17] Loss:1202517.1250\n",
      "Epoch [38/200], Step[3/17] Loss:3024839.7500\n",
      "Epoch [38/200], Step[4/17] Loss:2402793.5000\n",
      "Epoch [38/200], Step[5/17] Loss:2501883.7500\n",
      "Epoch [38/200], Step[6/17] Loss:1712211.5000\n",
      "Epoch [38/200], Step[7/17] Loss:5634979.0000\n",
      "Epoch [38/200], Step[8/17] Loss:7500854.0000\n",
      "Epoch [38/200], Step[9/17] Loss:1114591.5000\n",
      "Epoch [38/200], Step[10/17] Loss:1048007.3125\n",
      "Epoch [38/200], Step[11/17] Loss:3215466.2500\n",
      "Epoch [38/200], Step[12/17] Loss:3024332.5000\n",
      "Epoch [38/200], Step[13/17] Loss:1050554.2500\n",
      "Epoch [38/200], Step[14/17] Loss:1078619.5000\n",
      "Epoch [38/200], Step[15/17] Loss:1484659.3750\n",
      "Epoch [38/200], Step[16/17] Loss:1557605.8750\n",
      "Epoch [38/200], Step[17/17] Loss:866860.8750\n",
      "Epoch [39/200], Step[1/17] Loss:1447935.2500\n",
      "Epoch [39/200], Step[2/17] Loss:1003803.3750\n",
      "Epoch [39/200], Step[3/17] Loss:1633621.0000\n",
      "Epoch [39/200], Step[4/17] Loss:2310645.2500\n",
      "Epoch [39/200], Step[5/17] Loss:4115581.2500\n",
      "Epoch [39/200], Step[6/17] Loss:2989793.7500\n",
      "Epoch [39/200], Step[7/17] Loss:2082422.2500\n",
      "Epoch [39/200], Step[8/17] Loss:1092669.0000\n",
      "Epoch [39/200], Step[9/17] Loss:1860388.6250\n",
      "Epoch [39/200], Step[10/17] Loss:10173361.0000\n",
      "Epoch [39/200], Step[11/17] Loss:1021250.3125\n",
      "Epoch [39/200], Step[12/17] Loss:1457782.8750\n",
      "Epoch [39/200], Step[13/17] Loss:2602585.0000\n",
      "Epoch [39/200], Step[14/17] Loss:1644052.3750\n",
      "Epoch [39/200], Step[15/17] Loss:2160448.0000\n",
      "Epoch [39/200], Step[16/17] Loss:1203859.5000\n",
      "Epoch [39/200], Step[17/17] Loss:1085649.3750\n",
      "Epoch [40/200], Step[1/17] Loss:2636868.5000\n",
      "Epoch [40/200], Step[2/17] Loss:2691520.7500\n",
      "Epoch [40/200], Step[3/17] Loss:1713315.1250\n",
      "Epoch [40/200], Step[4/17] Loss:1812817.3750\n",
      "Epoch [40/200], Step[5/17] Loss:1013036.4375\n",
      "Epoch [40/200], Step[6/17] Loss:1223886.8750\n",
      "Epoch [40/200], Step[7/17] Loss:944450.3125\n",
      "Epoch [40/200], Step[8/17] Loss:3257203.0000\n",
      "Epoch [40/200], Step[9/17] Loss:797813.2500\n",
      "Epoch [40/200], Step[10/17] Loss:1885697.8750\n",
      "Epoch [40/200], Step[11/17] Loss:1514414.7500\n",
      "Epoch [40/200], Step[12/17] Loss:2925264.7500\n",
      "Epoch [40/200], Step[13/17] Loss:7360132.0000\n",
      "Epoch [40/200], Step[14/17] Loss:5238027.5000\n",
      "Epoch [40/200], Step[15/17] Loss:968967.1875\n",
      "Epoch [40/200], Step[16/17] Loss:1727987.2500\n",
      "Epoch [40/200], Step[17/17] Loss:2425705.7500\n",
      "Epoch [41/200], Step[1/17] Loss:660325.0000\n",
      "Epoch [41/200], Step[2/17] Loss:2470588.0000\n",
      "Epoch [41/200], Step[3/17] Loss:2809235.2500\n",
      "Epoch [41/200], Step[4/17] Loss:1261159.7500\n",
      "Epoch [41/200], Step[5/17] Loss:1587437.7500\n",
      "Epoch [41/200], Step[6/17] Loss:3912127.2500\n",
      "Epoch [41/200], Step[7/17] Loss:4772772.0000\n",
      "Epoch [41/200], Step[8/17] Loss:1154269.7500\n",
      "Epoch [41/200], Step[9/17] Loss:1034143.4375\n",
      "Epoch [41/200], Step[10/17] Loss:1011660.3125\n",
      "Epoch [41/200], Step[11/17] Loss:2205897.5000\n",
      "Epoch [41/200], Step[12/17] Loss:4462787.0000\n",
      "Epoch [41/200], Step[13/17] Loss:1239228.5000\n",
      "Epoch [41/200], Step[14/17] Loss:1697499.5000\n",
      "Epoch [41/200], Step[15/17] Loss:1332351.6250\n",
      "Epoch [41/200], Step[16/17] Loss:7611933.5000\n",
      "Epoch [41/200], Step[17/17] Loss:564764.6875\n",
      "Epoch [42/200], Step[1/17] Loss:2127468.5000\n",
      "Epoch [42/200], Step[2/17] Loss:1704573.8750\n",
      "Epoch [42/200], Step[3/17] Loss:2365322.2500\n",
      "Epoch [42/200], Step[4/17] Loss:7509667.0000\n",
      "Epoch [42/200], Step[5/17] Loss:1001797.9375\n",
      "Epoch [42/200], Step[6/17] Loss:1357350.7500\n",
      "Epoch [42/200], Step[7/17] Loss:1181046.8750\n",
      "Epoch [42/200], Step[8/17] Loss:1204959.8750\n",
      "Epoch [42/200], Step[9/17] Loss:3257175.7500\n",
      "Epoch [42/200], Step[10/17] Loss:2056419.8750\n",
      "Epoch [42/200], Step[11/17] Loss:5669809.5000\n",
      "Epoch [42/200], Step[12/17] Loss:1162495.1250\n",
      "Epoch [42/200], Step[13/17] Loss:2210992.2500\n",
      "Epoch [42/200], Step[14/17] Loss:1441147.6250\n",
      "Epoch [42/200], Step[15/17] Loss:585998.4375\n",
      "Epoch [42/200], Step[16/17] Loss:1880470.0000\n",
      "Epoch [42/200], Step[17/17] Loss:3649960.7500\n",
      "Epoch [43/200], Step[1/17] Loss:1689814.6250\n",
      "Epoch [43/200], Step[2/17] Loss:2467852.5000\n",
      "Epoch [43/200], Step[3/17] Loss:847073.6875\n",
      "Epoch [43/200], Step[4/17] Loss:3768154.0000\n",
      "Epoch [43/200], Step[5/17] Loss:1768262.1250\n",
      "Epoch [43/200], Step[6/17] Loss:1898360.0000\n",
      "Epoch [43/200], Step[7/17] Loss:1783558.6250\n",
      "Epoch [43/200], Step[8/17] Loss:1418085.1250\n",
      "Epoch [43/200], Step[9/17] Loss:1828802.8750\n",
      "Epoch [43/200], Step[10/17] Loss:1194631.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/200], Step[11/17] Loss:1090992.6250\n",
      "Epoch [43/200], Step[12/17] Loss:1328598.0000\n",
      "Epoch [43/200], Step[13/17] Loss:1037233.5625\n",
      "Epoch [43/200], Step[14/17] Loss:3158075.2500\n",
      "Epoch [43/200], Step[15/17] Loss:5190308.0000\n",
      "Epoch [43/200], Step[16/17] Loss:1100420.1250\n",
      "Epoch [43/200], Step[17/17] Loss:9984082.0000\n",
      "Epoch [44/200], Step[1/17] Loss:2112844.0000\n",
      "Epoch [44/200], Step[2/17] Loss:1926499.6250\n",
      "Epoch [44/200], Step[3/17] Loss:1011238.5000\n",
      "Epoch [44/200], Step[4/17] Loss:1130582.6250\n",
      "Epoch [44/200], Step[5/17] Loss:2698073.5000\n",
      "Epoch [44/200], Step[6/17] Loss:1554535.6250\n",
      "Epoch [44/200], Step[7/17] Loss:762782.2500\n",
      "Epoch [44/200], Step[8/17] Loss:1445195.5000\n",
      "Epoch [44/200], Step[9/17] Loss:2155409.0000\n",
      "Epoch [44/200], Step[10/17] Loss:679385.6250\n",
      "Epoch [44/200], Step[11/17] Loss:1354783.3750\n",
      "Epoch [44/200], Step[12/17] Loss:1909725.7500\n",
      "Epoch [44/200], Step[13/17] Loss:10752376.0000\n",
      "Epoch [44/200], Step[14/17] Loss:1719702.0000\n",
      "Epoch [44/200], Step[15/17] Loss:3307924.5000\n",
      "Epoch [44/200], Step[16/17] Loss:4467943.0000\n",
      "Epoch [44/200], Step[17/17] Loss:853275.6875\n",
      "Epoch [45/200], Step[1/17] Loss:943884.8125\n",
      "Epoch [45/200], Step[2/17] Loss:1582925.5000\n",
      "Epoch [45/200], Step[3/17] Loss:2988735.5000\n",
      "Epoch [45/200], Step[4/17] Loss:1412456.6250\n",
      "Epoch [45/200], Step[5/17] Loss:1598191.8750\n",
      "Epoch [45/200], Step[6/17] Loss:2759049.2500\n",
      "Epoch [45/200], Step[7/17] Loss:545785.9375\n",
      "Epoch [45/200], Step[8/17] Loss:1185694.2500\n",
      "Epoch [45/200], Step[9/17] Loss:2243233.0000\n",
      "Epoch [45/200], Step[10/17] Loss:761475.0000\n",
      "Epoch [45/200], Step[11/17] Loss:4631289.5000\n",
      "Epoch [45/200], Step[12/17] Loss:1554451.1250\n",
      "Epoch [45/200], Step[13/17] Loss:1664321.3750\n",
      "Epoch [45/200], Step[14/17] Loss:815627.2500\n",
      "Epoch [45/200], Step[15/17] Loss:2905294.5000\n",
      "Epoch [45/200], Step[16/17] Loss:7293401.5000\n",
      "Epoch [45/200], Step[17/17] Loss:5903348.0000\n",
      "Epoch [46/200], Step[1/17] Loss:1422412.6250\n",
      "Epoch [46/200], Step[2/17] Loss:2588149.5000\n",
      "Epoch [46/200], Step[3/17] Loss:2111508.2500\n",
      "Epoch [46/200], Step[4/17] Loss:959762.8125\n",
      "Epoch [46/200], Step[5/17] Loss:3817468.2500\n",
      "Epoch [46/200], Step[6/17] Loss:3241541.2500\n",
      "Epoch [46/200], Step[7/17] Loss:759423.3125\n",
      "Epoch [46/200], Step[8/17] Loss:1691408.1250\n",
      "Epoch [46/200], Step[9/17] Loss:1577075.5000\n",
      "Epoch [46/200], Step[10/17] Loss:1288591.1250\n",
      "Epoch [46/200], Step[11/17] Loss:3300363.2500\n",
      "Epoch [46/200], Step[12/17] Loss:2079512.0000\n",
      "Epoch [46/200], Step[13/17] Loss:1334735.0000\n",
      "Epoch [46/200], Step[14/17] Loss:7262922.5000\n",
      "Epoch [46/200], Step[15/17] Loss:2561517.7500\n",
      "Epoch [46/200], Step[16/17] Loss:1720375.1250\n",
      "Epoch [46/200], Step[17/17] Loss:2419102.0000\n",
      "Epoch [47/200], Step[1/17] Loss:3800748.7500\n",
      "Epoch [47/200], Step[2/17] Loss:1920091.2500\n",
      "Epoch [47/200], Step[3/17] Loss:1200615.5000\n",
      "Epoch [47/200], Step[4/17] Loss:3182878.5000\n",
      "Epoch [47/200], Step[5/17] Loss:1092602.2500\n",
      "Epoch [47/200], Step[6/17] Loss:8148994.0000\n",
      "Epoch [47/200], Step[7/17] Loss:1631998.8750\n",
      "Epoch [47/200], Step[8/17] Loss:1038271.7500\n",
      "Epoch [47/200], Step[9/17] Loss:1576527.1250\n",
      "Epoch [47/200], Step[10/17] Loss:3039094.5000\n",
      "Epoch [47/200], Step[11/17] Loss:3942167.2500\n",
      "Epoch [47/200], Step[12/17] Loss:1439732.0000\n",
      "Epoch [47/200], Step[13/17] Loss:2420457.5000\n",
      "Epoch [47/200], Step[14/17] Loss:1440146.3750\n",
      "Epoch [47/200], Step[15/17] Loss:1469610.6250\n",
      "Epoch [47/200], Step[16/17] Loss:1654465.3750\n",
      "Epoch [47/200], Step[17/17] Loss:841704.5000\n",
      "Epoch [48/200], Step[1/17] Loss:1418842.2500\n",
      "Epoch [48/200], Step[2/17] Loss:3315501.7500\n",
      "Epoch [48/200], Step[3/17] Loss:1183097.0000\n",
      "Epoch [48/200], Step[4/17] Loss:1645360.7500\n",
      "Epoch [48/200], Step[5/17] Loss:1543472.3750\n",
      "Epoch [48/200], Step[6/17] Loss:8056054.0000\n",
      "Epoch [48/200], Step[7/17] Loss:902878.8125\n",
      "Epoch [48/200], Step[8/17] Loss:1506789.2500\n",
      "Epoch [48/200], Step[9/17] Loss:2363936.2500\n",
      "Epoch [48/200], Step[10/17] Loss:1164654.5000\n",
      "Epoch [48/200], Step[11/17] Loss:1968412.6250\n",
      "Epoch [48/200], Step[12/17] Loss:3515942.5000\n",
      "Epoch [48/200], Step[13/17] Loss:1846240.3750\n",
      "Epoch [48/200], Step[14/17] Loss:2617815.0000\n",
      "Epoch [48/200], Step[15/17] Loss:1033216.2500\n",
      "Epoch [48/200], Step[16/17] Loss:4987772.5000\n",
      "Epoch [48/200], Step[17/17] Loss:753599.5625\n",
      "Epoch [49/200], Step[1/17] Loss:5356288.5000\n",
      "Epoch [49/200], Step[2/17] Loss:920977.1250\n",
      "Epoch [49/200], Step[3/17] Loss:1678552.6250\n",
      "Epoch [49/200], Step[4/17] Loss:1688556.5000\n",
      "Epoch [49/200], Step[5/17] Loss:1685374.1250\n",
      "Epoch [49/200], Step[6/17] Loss:1242156.5000\n",
      "Epoch [49/200], Step[7/17] Loss:2939058.7500\n",
      "Epoch [49/200], Step[8/17] Loss:1261462.8750\n",
      "Epoch [49/200], Step[9/17] Loss:1358940.8750\n",
      "Epoch [49/200], Step[10/17] Loss:8404304.0000\n",
      "Epoch [49/200], Step[11/17] Loss:2131948.2500\n",
      "Epoch [49/200], Step[12/17] Loss:794962.6875\n",
      "Epoch [49/200], Step[13/17] Loss:1205978.5000\n",
      "Epoch [49/200], Step[14/17] Loss:3572840.7500\n",
      "Epoch [49/200], Step[15/17] Loss:1032151.6250\n",
      "Epoch [49/200], Step[16/17] Loss:1848954.2500\n",
      "Epoch [49/200], Step[17/17] Loss:3150495.5000\n",
      "Epoch [50/200], Step[1/17] Loss:1305210.0000\n",
      "Epoch [50/200], Step[2/17] Loss:2315649.2500\n",
      "Epoch [50/200], Step[3/17] Loss:2326037.2500\n",
      "Epoch [50/200], Step[4/17] Loss:1021754.3125\n",
      "Epoch [50/200], Step[5/17] Loss:1087361.7500\n",
      "Epoch [50/200], Step[6/17] Loss:8264546.0000\n",
      "Epoch [50/200], Step[7/17] Loss:1823192.3750\n",
      "Epoch [50/200], Step[8/17] Loss:4365993.5000\n",
      "Epoch [50/200], Step[9/17] Loss:5591160.0000\n",
      "Epoch [50/200], Step[10/17] Loss:1260199.7500\n",
      "Epoch [50/200], Step[11/17] Loss:955598.1875\n",
      "Epoch [50/200], Step[12/17] Loss:900835.7500\n",
      "Epoch [50/200], Step[13/17] Loss:1764072.3750\n",
      "Epoch [50/200], Step[14/17] Loss:3443334.2500\n",
      "Epoch [50/200], Step[15/17] Loss:1618145.7500\n",
      "Epoch [50/200], Step[16/17] Loss:892314.5625\n",
      "Epoch [50/200], Step[17/17] Loss:919238.6875\n",
      "Epoch [51/200], Step[1/17] Loss:1145937.0000\n",
      "Epoch [51/200], Step[2/17] Loss:2579541.2500\n",
      "Epoch [51/200], Step[3/17] Loss:829133.8750\n",
      "Epoch [51/200], Step[4/17] Loss:3677703.7500\n",
      "Epoch [51/200], Step[5/17] Loss:1595936.2500\n",
      "Epoch [51/200], Step[6/17] Loss:900053.0000\n",
      "Epoch [51/200], Step[7/17] Loss:842651.4375\n",
      "Epoch [51/200], Step[8/17] Loss:4822936.5000\n",
      "Epoch [51/200], Step[9/17] Loss:1816561.0000\n",
      "Epoch [51/200], Step[10/17] Loss:8264508.5000\n",
      "Epoch [51/200], Step[11/17] Loss:1105878.1250\n",
      "Epoch [51/200], Step[12/17] Loss:2753805.5000\n",
      "Epoch [51/200], Step[13/17] Loss:1698055.5000\n",
      "Epoch [51/200], Step[14/17] Loss:2292521.7500\n",
      "Epoch [51/200], Step[15/17] Loss:727651.2500\n",
      "Epoch [51/200], Step[16/17] Loss:1279206.2500\n",
      "Epoch [51/200], Step[17/17] Loss:4123330.0000\n",
      "Epoch [52/200], Step[1/17] Loss:2025368.8750\n",
      "Epoch [52/200], Step[2/17] Loss:1662101.2500\n",
      "Epoch [52/200], Step[3/17] Loss:2780083.0000\n",
      "Epoch [52/200], Step[4/17] Loss:2563073.5000\n",
      "Epoch [52/200], Step[5/17] Loss:1113668.2500\n",
      "Epoch [52/200], Step[6/17] Loss:1311734.8750\n",
      "Epoch [52/200], Step[7/17] Loss:736731.0000\n",
      "Epoch [52/200], Step[8/17] Loss:6443978.5000\n",
      "Epoch [52/200], Step[9/17] Loss:1256785.3750\n",
      "Epoch [52/200], Step[10/17] Loss:1063150.6250\n",
      "Epoch [52/200], Step[11/17] Loss:3304137.7500\n",
      "Epoch [52/200], Step[12/17] Loss:2230876.7500\n",
      "Epoch [52/200], Step[13/17] Loss:1338056.5000\n",
      "Epoch [52/200], Step[14/17] Loss:8273286.0000\n",
      "Epoch [52/200], Step[15/17] Loss:1656458.3750\n",
      "Epoch [52/200], Step[16/17] Loss:1148605.8750\n",
      "Epoch [52/200], Step[17/17] Loss:952846.9375\n",
      "Epoch [53/200], Step[1/17] Loss:1938148.5000\n",
      "Epoch [53/200], Step[2/17] Loss:2078108.8750\n",
      "Epoch [53/200], Step[3/17] Loss:1388032.5000\n",
      "Epoch [53/200], Step[4/17] Loss:7377734.0000\n",
      "Epoch [53/200], Step[5/17] Loss:906143.5000\n",
      "Epoch [53/200], Step[6/17] Loss:1207817.1250\n",
      "Epoch [53/200], Step[7/17] Loss:3182851.2500\n",
      "Epoch [53/200], Step[8/17] Loss:4878911.0000\n",
      "Epoch [53/200], Step[9/17] Loss:1361817.1250\n",
      "Epoch [53/200], Step[10/17] Loss:3778516.2500\n",
      "Epoch [53/200], Step[11/17] Loss:1357370.2500\n",
      "Epoch [53/200], Step[12/17] Loss:3182390.0000\n",
      "Epoch [53/200], Step[13/17] Loss:1396713.8750\n",
      "Epoch [53/200], Step[14/17] Loss:977198.1875\n",
      "Epoch [53/200], Step[15/17] Loss:1327624.3750\n",
      "Epoch [53/200], Step[16/17] Loss:1860434.7500\n",
      "Epoch [53/200], Step[17/17] Loss:1824584.0000\n",
      "Epoch [54/200], Step[1/17] Loss:1133131.5000\n",
      "Epoch [54/200], Step[2/17] Loss:5943253.0000\n",
      "Epoch [54/200], Step[3/17] Loss:3184113.5000\n",
      "Epoch [54/200], Step[4/17] Loss:1271858.1250\n",
      "Epoch [54/200], Step[5/17] Loss:2525710.2500\n",
      "Epoch [54/200], Step[6/17] Loss:1289434.5000\n",
      "Epoch [54/200], Step[7/17] Loss:1712981.6250\n",
      "Epoch [54/200], Step[8/17] Loss:7275938.5000\n",
      "Epoch [54/200], Step[9/17] Loss:847962.9375\n",
      "Epoch [54/200], Step[10/17] Loss:1393148.5000\n",
      "Epoch [54/200], Step[11/17] Loss:3259707.5000\n",
      "Epoch [54/200], Step[12/17] Loss:2225591.7500\n",
      "Epoch [54/200], Step[13/17] Loss:838965.5000\n",
      "Epoch [54/200], Step[14/17] Loss:2235864.7500\n",
      "Epoch [54/200], Step[15/17] Loss:1241543.1250\n",
      "Epoch [54/200], Step[16/17] Loss:2023082.6250\n",
      "Epoch [54/200], Step[17/17] Loss:1575382.3750\n",
      "Epoch [55/200], Step[1/17] Loss:793640.9375\n",
      "Epoch [55/200], Step[2/17] Loss:1653260.8750\n",
      "Epoch [55/200], Step[3/17] Loss:1448783.8750\n",
      "Epoch [55/200], Step[4/17] Loss:1524760.7500\n",
      "Epoch [55/200], Step[5/17] Loss:2386014.2500\n",
      "Epoch [55/200], Step[6/17] Loss:1308283.8750\n",
      "Epoch [55/200], Step[7/17] Loss:3495126.5000\n",
      "Epoch [55/200], Step[8/17] Loss:594702.7500\n",
      "Epoch [55/200], Step[9/17] Loss:1714328.2500\n",
      "Epoch [55/200], Step[10/17] Loss:1130146.6250\n",
      "Epoch [55/200], Step[11/17] Loss:4018439.5000\n",
      "Epoch [55/200], Step[12/17] Loss:1623441.8750\n",
      "Epoch [55/200], Step[13/17] Loss:1596170.1250\n",
      "Epoch [55/200], Step[14/17] Loss:2509984.2500\n",
      "Epoch [55/200], Step[15/17] Loss:3191984.0000\n",
      "Epoch [55/200], Step[16/17] Loss:10252782.0000\n",
      "Epoch [55/200], Step[17/17] Loss:542074.2500\n",
      "Epoch [56/200], Step[1/17] Loss:9457270.0000\n",
      "Epoch [56/200], Step[2/17] Loss:2283356.7500\n",
      "Epoch [56/200], Step[3/17] Loss:1636953.7500\n",
      "Epoch [56/200], Step[4/17] Loss:1785999.2500\n",
      "Epoch [56/200], Step[5/17] Loss:3932030.7500\n",
      "Epoch [56/200], Step[6/17] Loss:1397461.1250\n",
      "Epoch [56/200], Step[7/17] Loss:714620.6250\n",
      "Epoch [56/200], Step[8/17] Loss:2584913.2500\n",
      "Epoch [56/200], Step[9/17] Loss:2139596.0000\n",
      "Epoch [56/200], Step[10/17] Loss:599887.9375\n",
      "Epoch [56/200], Step[11/17] Loss:1359272.1250\n",
      "Epoch [56/200], Step[12/17] Loss:4174576.7500\n",
      "Epoch [56/200], Step[13/17] Loss:1304624.1250\n",
      "Epoch [56/200], Step[14/17] Loss:993797.0000\n",
      "Epoch [56/200], Step[15/17] Loss:2984183.7500\n",
      "Epoch [56/200], Step[16/17] Loss:1755363.8750\n",
      "Epoch [56/200], Step[17/17] Loss:711850.0000\n",
      "Epoch [57/200], Step[1/17] Loss:1917712.7500\n",
      "Epoch [57/200], Step[2/17] Loss:2929105.5000\n",
      "Epoch [57/200], Step[3/17] Loss:3270582.0000\n",
      "Epoch [57/200], Step[4/17] Loss:1354742.8750\n",
      "Epoch [57/200], Step[5/17] Loss:1263162.3750\n",
      "Epoch [57/200], Step[6/17] Loss:2572929.7500\n",
      "Epoch [57/200], Step[7/17] Loss:1280799.8750\n",
      "Epoch [57/200], Step[8/17] Loss:10412915.0000\n",
      "Epoch [57/200], Step[9/17] Loss:1849150.8750\n",
      "Epoch [57/200], Step[10/17] Loss:1241190.3750\n",
      "Epoch [57/200], Step[11/17] Loss:3469187.0000\n",
      "Epoch [57/200], Step[12/17] Loss:1072191.8750\n",
      "Epoch [57/200], Step[13/17] Loss:2340056.5000\n",
      "Epoch [57/200], Step[14/17] Loss:1543092.7500\n",
      "Epoch [57/200], Step[15/17] Loss:926560.0000\n",
      "Epoch [57/200], Step[16/17] Loss:1461685.2500\n",
      "Epoch [57/200], Step[17/17] Loss:956578.1875\n",
      "Epoch [58/200], Step[1/17] Loss:1963977.1250\n",
      "Epoch [58/200], Step[2/17] Loss:1523794.1250\n",
      "Epoch [58/200], Step[3/17] Loss:682902.5625\n",
      "Epoch [58/200], Step[4/17] Loss:1068849.5000\n",
      "Epoch [58/200], Step[5/17] Loss:2245214.5000\n",
      "Epoch [58/200], Step[6/17] Loss:2460644.7500\n",
      "Epoch [58/200], Step[7/17] Loss:3934197.7500\n",
      "Epoch [58/200], Step[8/17] Loss:775921.9375\n",
      "Epoch [58/200], Step[9/17] Loss:1694812.0000\n",
      "Epoch [58/200], Step[10/17] Loss:5274293.5000\n",
      "Epoch [58/200], Step[11/17] Loss:1428488.3750\n",
      "Epoch [58/200], Step[12/17] Loss:3036015.0000\n",
      "Epoch [58/200], Step[13/17] Loss:2328883.2500\n",
      "Epoch [58/200], Step[14/17] Loss:1264846.8750\n",
      "Epoch [58/200], Step[15/17] Loss:785375.5625\n",
      "Epoch [58/200], Step[16/17] Loss:8470417.0000\n",
      "Epoch [58/200], Step[17/17] Loss:915261.5625\n",
      "Epoch [59/200], Step[1/17] Loss:1386566.3750\n",
      "Epoch [59/200], Step[2/17] Loss:1037576.2500\n",
      "Epoch [59/200], Step[3/17] Loss:1786456.1250\n",
      "Epoch [59/200], Step[4/17] Loss:10572314.0000\n",
      "Epoch [59/200], Step[5/17] Loss:1104717.3750\n",
      "Epoch [59/200], Step[6/17] Loss:590042.4375\n",
      "Epoch [59/200], Step[7/17] Loss:1449648.6250\n",
      "Epoch [59/200], Step[8/17] Loss:2438133.5000\n",
      "Epoch [59/200], Step[9/17] Loss:992686.8125\n",
      "Epoch [59/200], Step[10/17] Loss:1536075.8750\n",
      "Epoch [59/200], Step[11/17] Loss:4149017.2500\n",
      "Epoch [59/200], Step[12/17] Loss:2598535.0000\n",
      "Epoch [59/200], Step[13/17] Loss:3159428.7500\n",
      "Epoch [59/200], Step[14/17] Loss:1746654.2500\n",
      "Epoch [59/200], Step[15/17] Loss:720546.1875\n",
      "Epoch [59/200], Step[16/17] Loss:1071612.6250\n",
      "Epoch [59/200], Step[17/17] Loss:4113567.5000\n",
      "Epoch [60/200], Step[1/17] Loss:1703461.1250\n",
      "Epoch [60/200], Step[2/17] Loss:729128.5625\n",
      "Epoch [60/200], Step[3/17] Loss:863394.3750\n",
      "Epoch [60/200], Step[4/17] Loss:2006205.2500\n",
      "Epoch [60/200], Step[5/17] Loss:1817121.5000\n",
      "Epoch [60/200], Step[6/17] Loss:3423472.0000\n",
      "Epoch [60/200], Step[7/17] Loss:3891944.7500\n",
      "Epoch [60/200], Step[8/17] Loss:3176076.0000\n",
      "Epoch [60/200], Step[9/17] Loss:1356775.3750\n",
      "Epoch [60/200], Step[10/17] Loss:759368.1875\n",
      "Epoch [60/200], Step[11/17] Loss:896094.0000\n",
      "Epoch [60/200], Step[12/17] Loss:8191036.5000\n",
      "Epoch [60/200], Step[13/17] Loss:2921925.5000\n",
      "Epoch [60/200], Step[14/17] Loss:1134354.7500\n",
      "Epoch [60/200], Step[15/17] Loss:3531554.0000\n",
      "Epoch [60/200], Step[16/17] Loss:1886100.7500\n",
      "Epoch [60/200], Step[17/17] Loss:1716025.2500\n",
      "Epoch [61/200], Step[1/17] Loss:666918.6875\n",
      "Epoch [61/200], Step[2/17] Loss:2590923.7500\n",
      "Epoch [61/200], Step[3/17] Loss:2301973.0000\n",
      "Epoch [61/200], Step[4/17] Loss:985471.4375\n",
      "Epoch [61/200], Step[5/17] Loss:3551270.2500\n",
      "Epoch [61/200], Step[6/17] Loss:7526155.5000\n",
      "Epoch [61/200], Step[7/17] Loss:1973071.1250\n",
      "Epoch [61/200], Step[8/17] Loss:1227108.0000\n",
      "Epoch [61/200], Step[9/17] Loss:1325119.2500\n",
      "Epoch [61/200], Step[10/17] Loss:1405966.7500\n",
      "Epoch [61/200], Step[11/17] Loss:3619768.0000\n",
      "Epoch [61/200], Step[12/17] Loss:816221.9375\n",
      "Epoch [61/200], Step[13/17] Loss:1815078.1250\n",
      "Epoch [61/200], Step[14/17] Loss:3921572.7500\n",
      "Epoch [61/200], Step[15/17] Loss:3894931.5000\n",
      "Epoch [61/200], Step[16/17] Loss:1296070.8750\n",
      "Epoch [61/200], Step[17/17] Loss:941124.1875\n",
      "Epoch [62/200], Step[1/17] Loss:2933142.5000\n",
      "Epoch [62/200], Step[2/17] Loss:592490.5625\n",
      "Epoch [62/200], Step[3/17] Loss:1868514.2500\n",
      "Epoch [62/200], Step[4/17] Loss:8395405.0000\n",
      "Epoch [62/200], Step[5/17] Loss:2341441.7500\n",
      "Epoch [62/200], Step[6/17] Loss:1265889.1250\n",
      "Epoch [62/200], Step[7/17] Loss:886513.3750\n",
      "Epoch [62/200], Step[8/17] Loss:1671798.1250\n",
      "Epoch [62/200], Step[9/17] Loss:1478074.0000\n",
      "Epoch [62/200], Step[10/17] Loss:1199385.3750\n",
      "Epoch [62/200], Step[11/17] Loss:926957.4375\n",
      "Epoch [62/200], Step[12/17] Loss:1667974.6250\n",
      "Epoch [62/200], Step[13/17] Loss:1666989.5000\n",
      "Epoch [62/200], Step[14/17] Loss:6892674.0000\n",
      "Epoch [62/200], Step[15/17] Loss:1441630.5000\n",
      "Epoch [62/200], Step[16/17] Loss:871812.4375\n",
      "Epoch [62/200], Step[17/17] Loss:4408112.5000\n",
      "Epoch [63/200], Step[1/17] Loss:2192969.7500\n",
      "Epoch [63/200], Step[2/17] Loss:3259751.7500\n",
      "Epoch [63/200], Step[3/17] Loss:1818314.1250\n",
      "Epoch [63/200], Step[4/17] Loss:1107712.5000\n",
      "Epoch [63/200], Step[5/17] Loss:2980738.5000\n",
      "Epoch [63/200], Step[6/17] Loss:3846521.2500\n",
      "Epoch [63/200], Step[7/17] Loss:3753953.2500\n",
      "Epoch [63/200], Step[8/17] Loss:3246815.5000\n",
      "Epoch [63/200], Step[9/17] Loss:7994595.0000\n",
      "Epoch [63/200], Step[10/17] Loss:1014023.0000\n",
      "Epoch [63/200], Step[11/17] Loss:558508.5625\n",
      "Epoch [63/200], Step[12/17] Loss:935225.0000\n",
      "Epoch [63/200], Step[13/17] Loss:821927.5000\n",
      "Epoch [63/200], Step[14/17] Loss:1894245.0000\n",
      "Epoch [63/200], Step[15/17] Loss:1617313.1250\n",
      "Epoch [63/200], Step[16/17] Loss:1096740.7500\n",
      "Epoch [63/200], Step[17/17] Loss:1898988.5000\n",
      "Epoch [64/200], Step[1/17] Loss:3102822.5000\n",
      "Epoch [64/200], Step[2/17] Loss:3885853.7500\n",
      "Epoch [64/200], Step[3/17] Loss:1222142.6250\n",
      "Epoch [64/200], Step[4/17] Loss:1477168.3750\n",
      "Epoch [64/200], Step[5/17] Loss:1843501.6250\n",
      "Epoch [64/200], Step[6/17] Loss:1242516.7500\n",
      "Epoch [64/200], Step[7/17] Loss:843725.1250\n",
      "Epoch [64/200], Step[8/17] Loss:3352273.5000\n",
      "Epoch [64/200], Step[9/17] Loss:870269.1250\n",
      "Epoch [64/200], Step[10/17] Loss:634644.3125\n",
      "Epoch [64/200], Step[11/17] Loss:2429135.2500\n",
      "Epoch [64/200], Step[12/17] Loss:11013832.0000\n",
      "Epoch [64/200], Step[13/17] Loss:1663052.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/200], Step[14/17] Loss:844839.0000\n",
      "Epoch [64/200], Step[15/17] Loss:1412774.0000\n",
      "Epoch [64/200], Step[16/17] Loss:2752274.7500\n",
      "Epoch [64/200], Step[17/17] Loss:1343333.1250\n",
      "Epoch [65/200], Step[1/17] Loss:1266119.8750\n",
      "Epoch [65/200], Step[2/17] Loss:865709.8750\n",
      "Epoch [65/200], Step[3/17] Loss:2474774.2500\n",
      "Epoch [65/200], Step[4/17] Loss:9424577.0000\n",
      "Epoch [65/200], Step[5/17] Loss:1968224.1250\n",
      "Epoch [65/200], Step[6/17] Loss:1557919.3750\n",
      "Epoch [65/200], Step[7/17] Loss:1444280.7500\n",
      "Epoch [65/200], Step[8/17] Loss:1420039.1250\n",
      "Epoch [65/200], Step[9/17] Loss:2639439.2500\n",
      "Epoch [65/200], Step[10/17] Loss:934745.3750\n",
      "Epoch [65/200], Step[11/17] Loss:1304071.2500\n",
      "Epoch [65/200], Step[12/17] Loss:4984722.5000\n",
      "Epoch [65/200], Step[13/17] Loss:651039.6250\n",
      "Epoch [65/200], Step[14/17] Loss:2253906.2500\n",
      "Epoch [65/200], Step[15/17] Loss:2230869.2500\n",
      "Epoch [65/200], Step[16/17] Loss:732558.1875\n",
      "Epoch [65/200], Step[17/17] Loss:4343739.0000\n",
      "Epoch [66/200], Step[1/17] Loss:3571729.2500\n",
      "Epoch [66/200], Step[2/17] Loss:1291125.6250\n",
      "Epoch [66/200], Step[3/17] Loss:1300424.7500\n",
      "Epoch [66/200], Step[4/17] Loss:2605284.0000\n",
      "Epoch [66/200], Step[5/17] Loss:1234499.7500\n",
      "Epoch [66/200], Step[6/17] Loss:2365842.0000\n",
      "Epoch [66/200], Step[7/17] Loss:2412884.2500\n",
      "Epoch [66/200], Step[8/17] Loss:1727013.5000\n",
      "Epoch [66/200], Step[9/17] Loss:7948211.0000\n",
      "Epoch [66/200], Step[10/17] Loss:1606721.0000\n",
      "Epoch [66/200], Step[11/17] Loss:1583453.8750\n",
      "Epoch [66/200], Step[12/17] Loss:1322201.7500\n",
      "Epoch [66/200], Step[13/17] Loss:3092827.7500\n",
      "Epoch [66/200], Step[14/17] Loss:2008787.6250\n",
      "Epoch [66/200], Step[15/17] Loss:2247660.2500\n",
      "Epoch [66/200], Step[16/17] Loss:2511381.5000\n",
      "Epoch [66/200], Step[17/17] Loss:1048906.5000\n",
      "Epoch [67/200], Step[1/17] Loss:998484.5000\n",
      "Epoch [67/200], Step[2/17] Loss:1086996.0000\n",
      "Epoch [67/200], Step[3/17] Loss:917108.4375\n",
      "Epoch [67/200], Step[4/17] Loss:1158171.6250\n",
      "Epoch [67/200], Step[5/17] Loss:2718456.7500\n",
      "Epoch [67/200], Step[6/17] Loss:4296910.0000\n",
      "Epoch [67/200], Step[7/17] Loss:1451011.0000\n",
      "Epoch [67/200], Step[8/17] Loss:967806.0625\n",
      "Epoch [67/200], Step[9/17] Loss:1473405.0000\n",
      "Epoch [67/200], Step[10/17] Loss:8410384.0000\n",
      "Epoch [67/200], Step[11/17] Loss:3108930.0000\n",
      "Epoch [67/200], Step[12/17] Loss:2028921.0000\n",
      "Epoch [67/200], Step[13/17] Loss:4379480.5000\n",
      "Epoch [67/200], Step[14/17] Loss:1648275.7500\n",
      "Epoch [67/200], Step[15/17] Loss:643022.1250\n",
      "Epoch [67/200], Step[16/17] Loss:3483580.7500\n",
      "Epoch [67/200], Step[17/17] Loss:1121648.6250\n",
      "Epoch [68/200], Step[1/17] Loss:707589.8750\n",
      "Epoch [68/200], Step[2/17] Loss:1990729.1250\n",
      "Epoch [68/200], Step[3/17] Loss:1905389.0000\n",
      "Epoch [68/200], Step[4/17] Loss:2125486.2500\n",
      "Epoch [68/200], Step[5/17] Loss:2389069.2500\n",
      "Epoch [68/200], Step[6/17] Loss:3018402.2500\n",
      "Epoch [68/200], Step[7/17] Loss:913941.6875\n",
      "Epoch [68/200], Step[8/17] Loss:2547738.0000\n",
      "Epoch [68/200], Step[9/17] Loss:1425837.0000\n",
      "Epoch [68/200], Step[10/17] Loss:1931598.0000\n",
      "Epoch [68/200], Step[11/17] Loss:1105984.7500\n",
      "Epoch [68/200], Step[12/17] Loss:1058587.1250\n",
      "Epoch [68/200], Step[13/17] Loss:2534308.7500\n",
      "Epoch [68/200], Step[14/17] Loss:10374771.0000\n",
      "Epoch [68/200], Step[15/17] Loss:2139239.2500\n",
      "Epoch [68/200], Step[16/17] Loss:2564253.7500\n",
      "Epoch [68/200], Step[17/17] Loss:1168440.1250\n",
      "Epoch [69/200], Step[1/17] Loss:2959090.5000\n",
      "Epoch [69/200], Step[2/17] Loss:2025593.7500\n",
      "Epoch [69/200], Step[3/17] Loss:1342640.6250\n",
      "Epoch [69/200], Step[4/17] Loss:2368349.7500\n",
      "Epoch [69/200], Step[5/17] Loss:1854147.8750\n",
      "Epoch [69/200], Step[6/17] Loss:1007653.5625\n",
      "Epoch [69/200], Step[7/17] Loss:7586099.0000\n",
      "Epoch [69/200], Step[8/17] Loss:2771918.2500\n",
      "Epoch [69/200], Step[9/17] Loss:2065679.3750\n",
      "Epoch [69/200], Step[10/17] Loss:1826662.8750\n",
      "Epoch [69/200], Step[11/17] Loss:1018228.0000\n",
      "Epoch [69/200], Step[12/17] Loss:2137724.7500\n",
      "Epoch [69/200], Step[13/17] Loss:3605353.5000\n",
      "Epoch [69/200], Step[14/17] Loss:874061.0625\n",
      "Epoch [69/200], Step[15/17] Loss:1772612.8750\n",
      "Epoch [69/200], Step[16/17] Loss:1135736.0000\n",
      "Epoch [69/200], Step[17/17] Loss:4099362.2500\n",
      "Epoch [70/200], Step[1/17] Loss:1558053.7500\n",
      "Epoch [70/200], Step[2/17] Loss:547694.1250\n",
      "Epoch [70/200], Step[3/17] Loss:8898152.0000\n",
      "Epoch [70/200], Step[4/17] Loss:2803107.5000\n",
      "Epoch [70/200], Step[5/17] Loss:1370803.1250\n",
      "Epoch [70/200], Step[6/17] Loss:1982577.3750\n",
      "Epoch [70/200], Step[7/17] Loss:690919.5625\n",
      "Epoch [70/200], Step[8/17] Loss:744638.4375\n",
      "Epoch [70/200], Step[9/17] Loss:2269614.5000\n",
      "Epoch [70/200], Step[10/17] Loss:1692378.7500\n",
      "Epoch [70/200], Step[11/17] Loss:1596264.2500\n",
      "Epoch [70/200], Step[12/17] Loss:5135326.5000\n",
      "Epoch [70/200], Step[13/17] Loss:3153227.7500\n",
      "Epoch [70/200], Step[14/17] Loss:1431271.0000\n",
      "Epoch [70/200], Step[15/17] Loss:3335271.5000\n",
      "Epoch [70/200], Step[16/17] Loss:1696548.3750\n",
      "Epoch [70/200], Step[17/17] Loss:955612.0000\n",
      "Epoch [71/200], Step[1/17] Loss:1082881.5000\n",
      "Epoch [71/200], Step[2/17] Loss:1269050.1250\n",
      "Epoch [71/200], Step[3/17] Loss:629807.1875\n",
      "Epoch [71/200], Step[4/17] Loss:5048462.0000\n",
      "Epoch [71/200], Step[5/17] Loss:3749260.5000\n",
      "Epoch [71/200], Step[6/17] Loss:1673969.1250\n",
      "Epoch [71/200], Step[7/17] Loss:7553420.0000\n",
      "Epoch [71/200], Step[8/17] Loss:1364044.6250\n",
      "Epoch [71/200], Step[9/17] Loss:2830753.5000\n",
      "Epoch [71/200], Step[10/17] Loss:777894.9375\n",
      "Epoch [71/200], Step[11/17] Loss:2999722.0000\n",
      "Epoch [71/200], Step[12/17] Loss:1788822.8750\n",
      "Epoch [71/200], Step[13/17] Loss:1558226.1250\n",
      "Epoch [71/200], Step[14/17] Loss:1451835.8750\n",
      "Epoch [71/200], Step[15/17] Loss:1132733.8750\n",
      "Epoch [71/200], Step[16/17] Loss:2865586.2500\n",
      "Epoch [71/200], Step[17/17] Loss:2345614.2500\n",
      "Epoch [72/200], Step[1/17] Loss:1659848.0000\n",
      "Epoch [72/200], Step[2/17] Loss:2315613.0000\n",
      "Epoch [72/200], Step[3/17] Loss:1767188.7500\n",
      "Epoch [72/200], Step[4/17] Loss:7105712.5000\n",
      "Epoch [72/200], Step[5/17] Loss:1254126.2500\n",
      "Epoch [72/200], Step[6/17] Loss:739461.5625\n",
      "Epoch [72/200], Step[7/17] Loss:1090409.3750\n",
      "Epoch [72/200], Step[8/17] Loss:1810046.6250\n",
      "Epoch [72/200], Step[9/17] Loss:8115638.5000\n",
      "Epoch [72/200], Step[10/17] Loss:1148880.7500\n",
      "Epoch [72/200], Step[11/17] Loss:2902651.0000\n",
      "Epoch [72/200], Step[12/17] Loss:2914674.5000\n",
      "Epoch [72/200], Step[13/17] Loss:1181628.6250\n",
      "Epoch [72/200], Step[14/17] Loss:3077586.7500\n",
      "Epoch [72/200], Step[15/17] Loss:790926.1875\n",
      "Epoch [72/200], Step[16/17] Loss:798860.1250\n",
      "Epoch [72/200], Step[17/17] Loss:1241882.8750\n",
      "Epoch [73/200], Step[1/17] Loss:1162828.6250\n",
      "Epoch [73/200], Step[2/17] Loss:2873894.2500\n",
      "Epoch [73/200], Step[3/17] Loss:1515998.3750\n",
      "Epoch [73/200], Step[4/17] Loss:2168575.2500\n",
      "Epoch [73/200], Step[5/17] Loss:898117.8125\n",
      "Epoch [73/200], Step[6/17] Loss:1514367.0000\n",
      "Epoch [73/200], Step[7/17] Loss:4294909.0000\n",
      "Epoch [73/200], Step[8/17] Loss:1614403.2500\n",
      "Epoch [73/200], Step[9/17] Loss:2147113.7500\n",
      "Epoch [73/200], Step[10/17] Loss:9847503.0000\n",
      "Epoch [73/200], Step[11/17] Loss:1277325.0000\n",
      "Epoch [73/200], Step[12/17] Loss:823054.1250\n",
      "Epoch [73/200], Step[13/17] Loss:2964958.5000\n",
      "Epoch [73/200], Step[14/17] Loss:1913197.5000\n",
      "Epoch [73/200], Step[15/17] Loss:1025315.2500\n",
      "Epoch [73/200], Step[16/17] Loss:3017544.2500\n",
      "Epoch [73/200], Step[17/17] Loss:766988.0625\n",
      "Epoch [74/200], Step[1/17] Loss:3323301.2500\n",
      "Epoch [74/200], Step[2/17] Loss:1167918.2500\n",
      "Epoch [74/200], Step[3/17] Loss:3212738.2500\n",
      "Epoch [74/200], Step[4/17] Loss:2301855.7500\n",
      "Epoch [74/200], Step[5/17] Loss:1113429.6250\n",
      "Epoch [74/200], Step[6/17] Loss:4171677.0000\n",
      "Epoch [74/200], Step[7/17] Loss:2887851.5000\n",
      "Epoch [74/200], Step[8/17] Loss:4550076.5000\n",
      "Epoch [74/200], Step[9/17] Loss:657931.1875\n",
      "Epoch [74/200], Step[10/17] Loss:736325.3750\n",
      "Epoch [74/200], Step[11/17] Loss:1833125.2500\n",
      "Epoch [74/200], Step[12/17] Loss:1723073.0000\n",
      "Epoch [74/200], Step[13/17] Loss:7291972.0000\n",
      "Epoch [74/200], Step[14/17] Loss:1016137.3125\n",
      "Epoch [74/200], Step[15/17] Loss:1563495.0000\n",
      "Epoch [74/200], Step[16/17] Loss:961605.5625\n",
      "Epoch [74/200], Step[17/17] Loss:1439717.1250\n",
      "Epoch [75/200], Step[1/17] Loss:1972722.7500\n",
      "Epoch [75/200], Step[2/17] Loss:1147654.6250\n",
      "Epoch [75/200], Step[3/17] Loss:3200757.2500\n",
      "Epoch [75/200], Step[4/17] Loss:1294632.8750\n",
      "Epoch [75/200], Step[5/17] Loss:1959169.1250\n",
      "Epoch [75/200], Step[6/17] Loss:1261046.8750\n",
      "Epoch [75/200], Step[7/17] Loss:2608027.2500\n",
      "Epoch [75/200], Step[8/17] Loss:5197482.5000\n",
      "Epoch [75/200], Step[9/17] Loss:7967588.5000\n",
      "Epoch [75/200], Step[10/17] Loss:1537227.6250\n",
      "Epoch [75/200], Step[11/17] Loss:2803107.0000\n",
      "Epoch [75/200], Step[12/17] Loss:2413115.2500\n",
      "Epoch [75/200], Step[13/17] Loss:1759214.8750\n",
      "Epoch [75/200], Step[14/17] Loss:1168997.0000\n",
      "Epoch [75/200], Step[15/17] Loss:1720216.5000\n",
      "Epoch [75/200], Step[16/17] Loss:638677.8125\n",
      "Epoch [75/200], Step[17/17] Loss:1270946.7500\n",
      "Epoch [76/200], Step[1/17] Loss:366921.1875\n",
      "Epoch [76/200], Step[2/17] Loss:3794710.2500\n",
      "Epoch [76/200], Step[3/17] Loss:2335430.7500\n",
      "Epoch [76/200], Step[4/17] Loss:3111267.2500\n",
      "Epoch [76/200], Step[5/17] Loss:1795408.3750\n",
      "Epoch [76/200], Step[6/17] Loss:3255418.7500\n",
      "Epoch [76/200], Step[7/17] Loss:1090348.7500\n",
      "Epoch [76/200], Step[8/17] Loss:910008.5000\n",
      "Epoch [76/200], Step[9/17] Loss:2515144.2500\n",
      "Epoch [76/200], Step[10/17] Loss:1430858.5000\n",
      "Epoch [76/200], Step[11/17] Loss:773660.3125\n",
      "Epoch [76/200], Step[12/17] Loss:2834256.2500\n",
      "Epoch [76/200], Step[13/17] Loss:997677.0625\n",
      "Epoch [76/200], Step[14/17] Loss:7551005.0000\n",
      "Epoch [76/200], Step[15/17] Loss:1566298.1250\n",
      "Epoch [76/200], Step[16/17] Loss:3659121.7500\n",
      "Epoch [76/200], Step[17/17] Loss:2085843.1250\n",
      "Epoch [77/200], Step[1/17] Loss:3641842.5000\n",
      "Epoch [77/200], Step[2/17] Loss:1594357.5000\n",
      "Epoch [77/200], Step[3/17] Loss:1040540.3750\n",
      "Epoch [77/200], Step[4/17] Loss:7375781.5000\n",
      "Epoch [77/200], Step[5/17] Loss:3076384.2500\n",
      "Epoch [77/200], Step[6/17] Loss:771846.0625\n",
      "Epoch [77/200], Step[7/17] Loss:4962320.5000\n",
      "Epoch [77/200], Step[8/17] Loss:2486573.2500\n",
      "Epoch [77/200], Step[9/17] Loss:1210969.6250\n",
      "Epoch [77/200], Step[10/17] Loss:1765855.0000\n",
      "Epoch [77/200], Step[11/17] Loss:764200.5625\n",
      "Epoch [77/200], Step[12/17] Loss:784836.5625\n",
      "Epoch [77/200], Step[13/17] Loss:1135116.7500\n",
      "Epoch [77/200], Step[14/17] Loss:2499151.7500\n",
      "Epoch [77/200], Step[15/17] Loss:3139138.5000\n",
      "Epoch [77/200], Step[16/17] Loss:1980433.2500\n",
      "Epoch [77/200], Step[17/17] Loss:1788228.6250\n",
      "Epoch [78/200], Step[1/17] Loss:1860901.2500\n",
      "Epoch [78/200], Step[2/17] Loss:1087786.6250\n",
      "Epoch [78/200], Step[3/17] Loss:1873738.3750\n",
      "Epoch [78/200], Step[4/17] Loss:5071360.0000\n",
      "Epoch [78/200], Step[5/17] Loss:1924401.8750\n",
      "Epoch [78/200], Step[6/17] Loss:813752.1875\n",
      "Epoch [78/200], Step[7/17] Loss:1370659.1250\n",
      "Epoch [78/200], Step[8/17] Loss:2283310.2500\n",
      "Epoch [78/200], Step[9/17] Loss:3343824.5000\n",
      "Epoch [78/200], Step[10/17] Loss:1154297.5000\n",
      "Epoch [78/200], Step[11/17] Loss:1255359.1250\n",
      "Epoch [78/200], Step[12/17] Loss:2428609.7500\n",
      "Epoch [78/200], Step[13/17] Loss:9777586.0000\n",
      "Epoch [78/200], Step[14/17] Loss:1449959.5000\n",
      "Epoch [78/200], Step[15/17] Loss:1442854.2500\n",
      "Epoch [78/200], Step[16/17] Loss:1692418.7500\n",
      "Epoch [78/200], Step[17/17] Loss:1047956.1250\n",
      "Epoch [79/200], Step[1/17] Loss:1588716.6250\n",
      "Epoch [79/200], Step[2/17] Loss:4174862.0000\n",
      "Epoch [79/200], Step[3/17] Loss:2471394.2500\n",
      "Epoch [79/200], Step[4/17] Loss:632999.3125\n",
      "Epoch [79/200], Step[5/17] Loss:1891394.5000\n",
      "Epoch [79/200], Step[6/17] Loss:1372285.2500\n",
      "Epoch [79/200], Step[7/17] Loss:971607.7500\n",
      "Epoch [79/200], Step[8/17] Loss:3347726.0000\n",
      "Epoch [79/200], Step[9/17] Loss:1077818.6250\n",
      "Epoch [79/200], Step[10/17] Loss:6093687.0000\n",
      "Epoch [79/200], Step[11/17] Loss:1615215.5000\n",
      "Epoch [79/200], Step[12/17] Loss:603924.6875\n",
      "Epoch [79/200], Step[13/17] Loss:1575593.6250\n",
      "Epoch [79/200], Step[14/17] Loss:998724.6875\n",
      "Epoch [79/200], Step[15/17] Loss:1942430.5000\n",
      "Epoch [79/200], Step[16/17] Loss:7741660.0000\n",
      "Epoch [79/200], Step[17/17] Loss:1947374.5000\n",
      "Epoch [80/200], Step[1/17] Loss:2653927.5000\n",
      "Epoch [80/200], Step[2/17] Loss:887818.5625\n",
      "Epoch [80/200], Step[3/17] Loss:743216.2500\n",
      "Epoch [80/200], Step[4/17] Loss:2625497.5000\n",
      "Epoch [80/200], Step[5/17] Loss:952273.3750\n",
      "Epoch [80/200], Step[6/17] Loss:3068383.7500\n",
      "Epoch [80/200], Step[7/17] Loss:2474313.0000\n",
      "Epoch [80/200], Step[8/17] Loss:986820.3125\n",
      "Epoch [80/200], Step[9/17] Loss:1507757.2500\n",
      "Epoch [80/200], Step[10/17] Loss:1318169.1250\n",
      "Epoch [80/200], Step[11/17] Loss:7851120.0000\n",
      "Epoch [80/200], Step[12/17] Loss:3715129.2500\n",
      "Epoch [80/200], Step[13/17] Loss:824916.7500\n",
      "Epoch [80/200], Step[14/17] Loss:1853000.6250\n",
      "Epoch [80/200], Step[15/17] Loss:1813993.6250\n",
      "Epoch [80/200], Step[16/17] Loss:1037926.4375\n",
      "Epoch [80/200], Step[17/17] Loss:6606792.5000\n",
      "Epoch [81/200], Step[1/17] Loss:4496689.0000\n",
      "Epoch [81/200], Step[2/17] Loss:3875048.2500\n",
      "Epoch [81/200], Step[3/17] Loss:2529115.7500\n",
      "Epoch [81/200], Step[4/17] Loss:1662484.2500\n",
      "Epoch [81/200], Step[5/17] Loss:2094429.7500\n",
      "Epoch [81/200], Step[6/17] Loss:1666753.3750\n",
      "Epoch [81/200], Step[7/17] Loss:2954535.0000\n",
      "Epoch [81/200], Step[8/17] Loss:1474148.1250\n",
      "Epoch [81/200], Step[9/17] Loss:1327615.3750\n",
      "Epoch [81/200], Step[10/17] Loss:774409.3125\n",
      "Epoch [81/200], Step[11/17] Loss:753515.8750\n",
      "Epoch [81/200], Step[12/17] Loss:1448999.0000\n",
      "Epoch [81/200], Step[13/17] Loss:1147760.5000\n",
      "Epoch [81/200], Step[14/17] Loss:7587581.5000\n",
      "Epoch [81/200], Step[15/17] Loss:2294102.7500\n",
      "Epoch [81/200], Step[16/17] Loss:737783.8125\n",
      "Epoch [81/200], Step[17/17] Loss:3516690.7500\n",
      "Epoch [82/200], Step[1/17] Loss:4336495.5000\n",
      "Epoch [82/200], Step[2/17] Loss:674557.3125\n",
      "Epoch [82/200], Step[3/17] Loss:2715303.7500\n",
      "Epoch [82/200], Step[4/17] Loss:1619687.8750\n",
      "Epoch [82/200], Step[5/17] Loss:3531684.7500\n",
      "Epoch [82/200], Step[6/17] Loss:1993408.7500\n",
      "Epoch [82/200], Step[7/17] Loss:1143666.6250\n",
      "Epoch [82/200], Step[8/17] Loss:1284709.7500\n",
      "Epoch [82/200], Step[9/17] Loss:1659789.6250\n",
      "Epoch [82/200], Step[10/17] Loss:878058.5625\n",
      "Epoch [82/200], Step[11/17] Loss:997234.2500\n",
      "Epoch [82/200], Step[12/17] Loss:1302745.7500\n",
      "Epoch [82/200], Step[13/17] Loss:7915296.5000\n",
      "Epoch [82/200], Step[14/17] Loss:4912859.0000\n",
      "Epoch [82/200], Step[15/17] Loss:1045977.7500\n",
      "Epoch [82/200], Step[16/17] Loss:2056105.2500\n",
      "Epoch [82/200], Step[17/17] Loss:1987324.6250\n",
      "Epoch [83/200], Step[1/17] Loss:1266177.3750\n",
      "Epoch [83/200], Step[2/17] Loss:897272.0625\n",
      "Epoch [83/200], Step[3/17] Loss:3169058.0000\n",
      "Epoch [83/200], Step[4/17] Loss:1103928.5000\n",
      "Epoch [83/200], Step[5/17] Loss:1063550.5000\n",
      "Epoch [83/200], Step[6/17] Loss:3067746.0000\n",
      "Epoch [83/200], Step[7/17] Loss:2123424.7500\n",
      "Epoch [83/200], Step[8/17] Loss:1146300.6250\n",
      "Epoch [83/200], Step[9/17] Loss:3144446.7500\n",
      "Epoch [83/200], Step[10/17] Loss:1649732.5000\n",
      "Epoch [83/200], Step[11/17] Loss:2720945.0000\n",
      "Epoch [83/200], Step[12/17] Loss:1257692.3750\n",
      "Epoch [83/200], Step[13/17] Loss:3595660.5000\n",
      "Epoch [83/200], Step[14/17] Loss:7928664.5000\n",
      "Epoch [83/200], Step[15/17] Loss:3112809.7500\n",
      "Epoch [83/200], Step[16/17] Loss:1243669.1250\n",
      "Epoch [83/200], Step[17/17] Loss:1466097.3750\n",
      "Epoch [84/200], Step[1/17] Loss:1716893.7500\n",
      "Epoch [84/200], Step[2/17] Loss:2545078.0000\n",
      "Epoch [84/200], Step[3/17] Loss:1036598.0000\n",
      "Epoch [84/200], Step[4/17] Loss:2057186.2500\n",
      "Epoch [84/200], Step[5/17] Loss:3336552.7500\n",
      "Epoch [84/200], Step[6/17] Loss:988101.6875\n",
      "Epoch [84/200], Step[7/17] Loss:1141981.5000\n",
      "Epoch [84/200], Step[8/17] Loss:3031413.2500\n",
      "Epoch [84/200], Step[9/17] Loss:1312429.1250\n",
      "Epoch [84/200], Step[10/17] Loss:2193850.2500\n",
      "Epoch [84/200], Step[11/17] Loss:971849.8750\n",
      "Epoch [84/200], Step[12/17] Loss:2050489.2500\n",
      "Epoch [84/200], Step[13/17] Loss:7767626.5000\n",
      "Epoch [84/200], Step[14/17] Loss:469305.8438\n",
      "Epoch [84/200], Step[15/17] Loss:1019505.9375\n",
      "Epoch [84/200], Step[16/17] Loss:875038.3750\n",
      "Epoch [84/200], Step[17/17] Loss:8822624.0000\n",
      "Epoch [85/200], Step[1/17] Loss:2079699.3750\n",
      "Epoch [85/200], Step[2/17] Loss:2018148.3750\n",
      "Epoch [85/200], Step[3/17] Loss:1674343.8750\n",
      "Epoch [85/200], Step[4/17] Loss:9815794.0000\n",
      "Epoch [85/200], Step[5/17] Loss:1419359.6250\n",
      "Epoch [85/200], Step[6/17] Loss:1222797.2500\n",
      "Epoch [85/200], Step[7/17] Loss:2709728.0000\n",
      "Epoch [85/200], Step[8/17] Loss:1963930.1250\n",
      "Epoch [85/200], Step[9/17] Loss:1136232.7500\n",
      "Epoch [85/200], Step[10/17] Loss:1187211.8750\n",
      "Epoch [85/200], Step[11/17] Loss:3251920.5000\n",
      "Epoch [85/200], Step[12/17] Loss:1679068.3750\n",
      "Epoch [85/200], Step[13/17] Loss:688645.0000\n",
      "Epoch [85/200], Step[14/17] Loss:2970122.7500\n",
      "Epoch [85/200], Step[15/17] Loss:1036982.8125\n",
      "Epoch [85/200], Step[16/17] Loss:3530865.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/200], Step[17/17] Loss:1596840.0000\n",
      "Epoch [86/200], Step[1/17] Loss:1110929.5000\n",
      "Epoch [86/200], Step[2/17] Loss:1085036.2500\n",
      "Epoch [86/200], Step[3/17] Loss:3387545.5000\n",
      "Epoch [86/200], Step[4/17] Loss:2282383.5000\n",
      "Epoch [86/200], Step[5/17] Loss:2904655.0000\n",
      "Epoch [86/200], Step[6/17] Loss:1189505.8750\n",
      "Epoch [86/200], Step[7/17] Loss:1452307.1250\n",
      "Epoch [86/200], Step[8/17] Loss:1944193.0000\n",
      "Epoch [86/200], Step[9/17] Loss:2209854.0000\n",
      "Epoch [86/200], Step[10/17] Loss:1174881.0000\n",
      "Epoch [86/200], Step[11/17] Loss:3401516.7500\n",
      "Epoch [86/200], Step[12/17] Loss:1663614.1250\n",
      "Epoch [86/200], Step[13/17] Loss:2669120.5000\n",
      "Epoch [86/200], Step[14/17] Loss:3714570.5000\n",
      "Epoch [86/200], Step[15/17] Loss:962647.0625\n",
      "Epoch [86/200], Step[16/17] Loss:7548797.0000\n",
      "Epoch [86/200], Step[17/17] Loss:1207046.7500\n",
      "Epoch [87/200], Step[1/17] Loss:1947259.0000\n",
      "Epoch [87/200], Step[2/17] Loss:1674209.7500\n",
      "Epoch [87/200], Step[3/17] Loss:2116014.0000\n",
      "Epoch [87/200], Step[4/17] Loss:1880681.5000\n",
      "Epoch [87/200], Step[5/17] Loss:904657.8125\n",
      "Epoch [87/200], Step[6/17] Loss:1652274.8750\n",
      "Epoch [87/200], Step[7/17] Loss:6115385.0000\n",
      "Epoch [87/200], Step[8/17] Loss:1040158.3750\n",
      "Epoch [87/200], Step[9/17] Loss:1171809.3750\n",
      "Epoch [87/200], Step[10/17] Loss:2160314.5000\n",
      "Epoch [87/200], Step[11/17] Loss:683984.1875\n",
      "Epoch [87/200], Step[12/17] Loss:772126.0000\n",
      "Epoch [87/200], Step[13/17] Loss:7332232.0000\n",
      "Epoch [87/200], Step[14/17] Loss:1346150.0000\n",
      "Epoch [87/200], Step[15/17] Loss:2432201.0000\n",
      "Epoch [87/200], Step[16/17] Loss:4989388.5000\n",
      "Epoch [87/200], Step[17/17] Loss:1801152.7500\n",
      "Epoch [88/200], Step[1/17] Loss:1201168.5000\n",
      "Epoch [88/200], Step[2/17] Loss:869953.0000\n",
      "Epoch [88/200], Step[3/17] Loss:7182651.5000\n",
      "Epoch [88/200], Step[4/17] Loss:3021205.0000\n",
      "Epoch [88/200], Step[5/17] Loss:3813869.0000\n",
      "Epoch [88/200], Step[6/17] Loss:835891.8750\n",
      "Epoch [88/200], Step[7/17] Loss:3170046.0000\n",
      "Epoch [88/200], Step[8/17] Loss:916528.2500\n",
      "Epoch [88/200], Step[9/17] Loss:3404596.0000\n",
      "Epoch [88/200], Step[10/17] Loss:1538346.7500\n",
      "Epoch [88/200], Step[11/17] Loss:4596345.0000\n",
      "Epoch [88/200], Step[12/17] Loss:904880.1250\n",
      "Epoch [88/200], Step[13/17] Loss:1402180.8750\n",
      "Epoch [88/200], Step[14/17] Loss:2154189.5000\n",
      "Epoch [88/200], Step[15/17] Loss:1455666.7500\n",
      "Epoch [88/200], Step[16/17] Loss:2281646.7500\n",
      "Epoch [88/200], Step[17/17] Loss:1148452.6250\n",
      "Epoch [89/200], Step[1/17] Loss:3016689.2500\n",
      "Epoch [89/200], Step[2/17] Loss:3042389.7500\n",
      "Epoch [89/200], Step[3/17] Loss:1177425.7500\n",
      "Epoch [89/200], Step[4/17] Loss:1467077.6250\n",
      "Epoch [89/200], Step[5/17] Loss:2520039.2500\n",
      "Epoch [89/200], Step[6/17] Loss:903035.3750\n",
      "Epoch [89/200], Step[7/17] Loss:4113018.0000\n",
      "Epoch [89/200], Step[8/17] Loss:1476845.3750\n",
      "Epoch [89/200], Step[9/17] Loss:7741619.5000\n",
      "Epoch [89/200], Step[10/17] Loss:5079713.0000\n",
      "Epoch [89/200], Step[11/17] Loss:527437.1250\n",
      "Epoch [89/200], Step[12/17] Loss:2535224.7500\n",
      "Epoch [89/200], Step[13/17] Loss:965094.6250\n",
      "Epoch [89/200], Step[14/17] Loss:1473791.1250\n",
      "Epoch [89/200], Step[15/17] Loss:1715612.0000\n",
      "Epoch [89/200], Step[16/17] Loss:1275089.5000\n",
      "Epoch [89/200], Step[17/17] Loss:802683.0000\n",
      "Epoch [90/200], Step[1/17] Loss:1151673.6250\n",
      "Epoch [90/200], Step[2/17] Loss:2727627.7500\n",
      "Epoch [90/200], Step[3/17] Loss:7757767.5000\n",
      "Epoch [90/200], Step[4/17] Loss:1109757.5000\n",
      "Epoch [90/200], Step[5/17] Loss:1453060.6250\n",
      "Epoch [90/200], Step[6/17] Loss:1996194.7500\n",
      "Epoch [90/200], Step[7/17] Loss:645250.5625\n",
      "Epoch [90/200], Step[8/17] Loss:1716708.7500\n",
      "Epoch [90/200], Step[9/17] Loss:952972.3125\n",
      "Epoch [90/200], Step[10/17] Loss:3007736.2500\n",
      "Epoch [90/200], Step[11/17] Loss:2331772.2500\n",
      "Epoch [90/200], Step[12/17] Loss:1454387.5000\n",
      "Epoch [90/200], Step[13/17] Loss:2949241.0000\n",
      "Epoch [90/200], Step[14/17] Loss:4671061.5000\n",
      "Epoch [90/200], Step[15/17] Loss:3487989.5000\n",
      "Epoch [90/200], Step[16/17] Loss:1274531.3750\n",
      "Epoch [90/200], Step[17/17] Loss:1224060.2500\n",
      "Epoch [91/200], Step[1/17] Loss:1894386.0000\n",
      "Epoch [91/200], Step[2/17] Loss:729534.5000\n",
      "Epoch [91/200], Step[3/17] Loss:2439546.0000\n",
      "Epoch [91/200], Step[4/17] Loss:4043715.5000\n",
      "Epoch [91/200], Step[5/17] Loss:1658479.6250\n",
      "Epoch [91/200], Step[6/17] Loss:1304722.5000\n",
      "Epoch [91/200], Step[7/17] Loss:7537506.5000\n",
      "Epoch [91/200], Step[8/17] Loss:1485531.6250\n",
      "Epoch [91/200], Step[9/17] Loss:7304603.0000\n",
      "Epoch [91/200], Step[10/17] Loss:858430.3125\n",
      "Epoch [91/200], Step[11/17] Loss:1011918.4375\n",
      "Epoch [91/200], Step[12/17] Loss:1813849.7500\n",
      "Epoch [91/200], Step[13/17] Loss:1984351.0000\n",
      "Epoch [91/200], Step[14/17] Loss:1834563.2500\n",
      "Epoch [91/200], Step[15/17] Loss:871820.8750\n",
      "Epoch [91/200], Step[16/17] Loss:1283158.8750\n",
      "Epoch [91/200], Step[17/17] Loss:2001432.7500\n",
      "Epoch [92/200], Step[1/17] Loss:1152423.2500\n",
      "Epoch [92/200], Step[2/17] Loss:960383.2500\n",
      "Epoch [92/200], Step[3/17] Loss:2308973.5000\n",
      "Epoch [92/200], Step[4/17] Loss:759545.8125\n",
      "Epoch [92/200], Step[5/17] Loss:1003523.2500\n",
      "Epoch [92/200], Step[6/17] Loss:934376.4375\n",
      "Epoch [92/200], Step[7/17] Loss:783069.3125\n",
      "Epoch [92/200], Step[8/17] Loss:869895.9375\n",
      "Epoch [92/200], Step[9/17] Loss:1013553.2500\n",
      "Epoch [92/200], Step[10/17] Loss:3007004.2500\n",
      "Epoch [92/200], Step[11/17] Loss:3329850.5000\n",
      "Epoch [92/200], Step[12/17] Loss:1592259.2500\n",
      "Epoch [92/200], Step[13/17] Loss:7204145.0000\n",
      "Epoch [92/200], Step[14/17] Loss:7587350.5000\n",
      "Epoch [92/200], Step[15/17] Loss:1990691.3750\n",
      "Epoch [92/200], Step[16/17] Loss:1457734.8750\n",
      "Epoch [92/200], Step[17/17] Loss:4587696.0000\n",
      "Epoch [93/200], Step[1/17] Loss:935134.6250\n",
      "Epoch [93/200], Step[2/17] Loss:1599798.8750\n",
      "Epoch [93/200], Step[3/17] Loss:3039981.0000\n",
      "Epoch [93/200], Step[4/17] Loss:1033358.6875\n",
      "Epoch [93/200], Step[5/17] Loss:1807025.7500\n",
      "Epoch [93/200], Step[6/17] Loss:2458310.2500\n",
      "Epoch [93/200], Step[7/17] Loss:1624544.2500\n",
      "Epoch [93/200], Step[8/17] Loss:1103139.8750\n",
      "Epoch [93/200], Step[9/17] Loss:2124440.2500\n",
      "Epoch [93/200], Step[10/17] Loss:971925.8750\n",
      "Epoch [93/200], Step[11/17] Loss:2041139.5000\n",
      "Epoch [93/200], Step[12/17] Loss:9204447.0000\n",
      "Epoch [93/200], Step[13/17] Loss:2273213.0000\n",
      "Epoch [93/200], Step[14/17] Loss:3182249.2500\n",
      "Epoch [93/200], Step[15/17] Loss:1025110.0000\n",
      "Epoch [93/200], Step[16/17] Loss:1346263.3750\n",
      "Epoch [93/200], Step[17/17] Loss:4815017.0000\n",
      "Epoch [94/200], Step[1/17] Loss:3116880.5000\n",
      "Epoch [94/200], Step[2/17] Loss:2180566.0000\n",
      "Epoch [94/200], Step[3/17] Loss:3891063.7500\n",
      "Epoch [94/200], Step[4/17] Loss:3297156.7500\n",
      "Epoch [94/200], Step[5/17] Loss:991735.0625\n",
      "Epoch [94/200], Step[6/17] Loss:1944209.7500\n",
      "Epoch [94/200], Step[7/17] Loss:7892876.5000\n",
      "Epoch [94/200], Step[8/17] Loss:1631372.0000\n",
      "Epoch [94/200], Step[9/17] Loss:1047982.8750\n",
      "Epoch [94/200], Step[10/17] Loss:1043033.8125\n",
      "Epoch [94/200], Step[11/17] Loss:840228.1875\n",
      "Epoch [94/200], Step[12/17] Loss:1319209.3750\n",
      "Epoch [94/200], Step[13/17] Loss:2187485.7500\n",
      "Epoch [94/200], Step[14/17] Loss:2714607.7500\n",
      "Epoch [94/200], Step[15/17] Loss:2744871.0000\n",
      "Epoch [94/200], Step[16/17] Loss:1209995.0000\n",
      "Epoch [94/200], Step[17/17] Loss:2004931.8750\n",
      "Epoch [95/200], Step[1/17] Loss:1668200.1250\n",
      "Epoch [95/200], Step[2/17] Loss:2893200.7500\n",
      "Epoch [95/200], Step[3/17] Loss:3141341.7500\n",
      "Epoch [95/200], Step[4/17] Loss:960993.7500\n",
      "Epoch [95/200], Step[5/17] Loss:1245173.8750\n",
      "Epoch [95/200], Step[6/17] Loss:4361481.0000\n",
      "Epoch [95/200], Step[7/17] Loss:439974.7812\n",
      "Epoch [95/200], Step[8/17] Loss:1351290.6250\n",
      "Epoch [95/200], Step[9/17] Loss:1664590.5000\n",
      "Epoch [95/200], Step[10/17] Loss:7532218.0000\n",
      "Epoch [95/200], Step[11/17] Loss:1412284.7500\n",
      "Epoch [95/200], Step[12/17] Loss:2908499.2500\n",
      "Epoch [95/200], Step[13/17] Loss:3788701.0000\n",
      "Epoch [95/200], Step[14/17] Loss:1683009.7500\n",
      "Epoch [95/200], Step[15/17] Loss:1125095.2500\n",
      "Epoch [95/200], Step[16/17] Loss:1293886.5000\n",
      "Epoch [95/200], Step[17/17] Loss:2722879.5000\n",
      "Epoch [96/200], Step[1/17] Loss:1352346.7500\n",
      "Epoch [96/200], Step[2/17] Loss:1879947.5000\n",
      "Epoch [96/200], Step[3/17] Loss:1071303.6250\n",
      "Epoch [96/200], Step[4/17] Loss:876202.1250\n",
      "Epoch [96/200], Step[5/17] Loss:4059522.0000\n",
      "Epoch [96/200], Step[6/17] Loss:7501308.5000\n",
      "Epoch [96/200], Step[7/17] Loss:1333145.3750\n",
      "Epoch [96/200], Step[8/17] Loss:4961815.5000\n",
      "Epoch [96/200], Step[9/17] Loss:2365392.5000\n",
      "Epoch [96/200], Step[10/17] Loss:3649317.2500\n",
      "Epoch [96/200], Step[11/17] Loss:2322943.2500\n",
      "Epoch [96/200], Step[12/17] Loss:1738369.6250\n",
      "Epoch [96/200], Step[13/17] Loss:1190805.7500\n",
      "Epoch [96/200], Step[14/17] Loss:640211.0000\n",
      "Epoch [96/200], Step[15/17] Loss:2823007.5000\n",
      "Epoch [96/200], Step[16/17] Loss:704947.3125\n",
      "Epoch [96/200], Step[17/17] Loss:1491318.7500\n",
      "Epoch [97/200], Step[1/17] Loss:2613729.2500\n",
      "Epoch [97/200], Step[2/17] Loss:1195667.7500\n",
      "Epoch [97/200], Step[3/17] Loss:2898207.5000\n",
      "Epoch [97/200], Step[4/17] Loss:1056660.0000\n",
      "Epoch [97/200], Step[5/17] Loss:1648229.2500\n",
      "Epoch [97/200], Step[6/17] Loss:8017073.0000\n",
      "Epoch [97/200], Step[7/17] Loss:3774358.0000\n",
      "Epoch [97/200], Step[8/17] Loss:2441232.0000\n",
      "Epoch [97/200], Step[9/17] Loss:2626199.7500\n",
      "Epoch [97/200], Step[10/17] Loss:1327543.3750\n",
      "Epoch [97/200], Step[11/17] Loss:1253526.0000\n",
      "Epoch [97/200], Step[12/17] Loss:1559486.6250\n",
      "Epoch [97/200], Step[13/17] Loss:1077875.1250\n",
      "Epoch [97/200], Step[14/17] Loss:2083808.3750\n",
      "Epoch [97/200], Step[15/17] Loss:3246026.5000\n",
      "Epoch [97/200], Step[16/17] Loss:2176351.7500\n",
      "Epoch [97/200], Step[17/17] Loss:844687.0625\n",
      "Epoch [98/200], Step[1/17] Loss:927233.3125\n",
      "Epoch [98/200], Step[2/17] Loss:2928170.5000\n",
      "Epoch [98/200], Step[3/17] Loss:1349563.8750\n",
      "Epoch [98/200], Step[4/17] Loss:3244154.5000\n",
      "Epoch [98/200], Step[5/17] Loss:941827.1875\n",
      "Epoch [98/200], Step[6/17] Loss:1438335.2500\n",
      "Epoch [98/200], Step[7/17] Loss:1229976.7500\n",
      "Epoch [98/200], Step[8/17] Loss:942288.0625\n",
      "Epoch [98/200], Step[9/17] Loss:2086495.0000\n",
      "Epoch [98/200], Step[10/17] Loss:4838856.0000\n",
      "Epoch [98/200], Step[11/17] Loss:1223870.0000\n",
      "Epoch [98/200], Step[12/17] Loss:1812370.3750\n",
      "Epoch [98/200], Step[13/17] Loss:3587517.0000\n",
      "Epoch [98/200], Step[14/17] Loss:2486363.7500\n",
      "Epoch [98/200], Step[15/17] Loss:7412053.0000\n",
      "Epoch [98/200], Step[16/17] Loss:2702600.2500\n",
      "Epoch [98/200], Step[17/17] Loss:653053.1875\n",
      "Epoch [99/200], Step[1/17] Loss:1829991.3750\n",
      "Epoch [99/200], Step[2/17] Loss:7076153.0000\n",
      "Epoch [99/200], Step[3/17] Loss:1797733.7500\n",
      "Epoch [99/200], Step[4/17] Loss:875863.8750\n",
      "Epoch [99/200], Step[5/17] Loss:4743573.5000\n",
      "Epoch [99/200], Step[6/17] Loss:897470.6250\n",
      "Epoch [99/200], Step[7/17] Loss:857179.0625\n",
      "Epoch [99/200], Step[8/17] Loss:2015154.2500\n",
      "Epoch [99/200], Step[9/17] Loss:1552740.5000\n",
      "Epoch [99/200], Step[10/17] Loss:2602907.7500\n",
      "Epoch [99/200], Step[11/17] Loss:4081253.0000\n",
      "Epoch [99/200], Step[12/17] Loss:477884.7812\n",
      "Epoch [99/200], Step[13/17] Loss:2182575.0000\n",
      "Epoch [99/200], Step[14/17] Loss:2048809.2500\n",
      "Epoch [99/200], Step[15/17] Loss:2310083.0000\n",
      "Epoch [99/200], Step[16/17] Loss:2958249.7500\n",
      "Epoch [99/200], Step[17/17] Loss:1691888.3750\n",
      "Epoch [100/200], Step[1/17] Loss:1761106.3750\n",
      "Epoch [100/200], Step[2/17] Loss:1613829.7500\n",
      "Epoch [100/200], Step[3/17] Loss:842615.6875\n",
      "Epoch [100/200], Step[4/17] Loss:2239727.5000\n",
      "Epoch [100/200], Step[5/17] Loss:2659900.7500\n",
      "Epoch [100/200], Step[6/17] Loss:3913072.7500\n",
      "Epoch [100/200], Step[7/17] Loss:6966785.5000\n",
      "Epoch [100/200], Step[8/17] Loss:1318338.3750\n",
      "Epoch [100/200], Step[9/17] Loss:2038979.0000\n",
      "Epoch [100/200], Step[10/17] Loss:1656608.3750\n",
      "Epoch [100/200], Step[11/17] Loss:1710095.8750\n",
      "Epoch [100/200], Step[12/17] Loss:3146122.7500\n",
      "Epoch [100/200], Step[13/17] Loss:1386076.8750\n",
      "Epoch [100/200], Step[14/17] Loss:1889894.5000\n",
      "Epoch [100/200], Step[15/17] Loss:3483130.5000\n",
      "Epoch [100/200], Step[16/17] Loss:2365200.5000\n",
      "Epoch [100/200], Step[17/17] Loss:850212.0000\n",
      "Epoch [101/200], Step[1/17] Loss:2843832.0000\n",
      "Epoch [101/200], Step[2/17] Loss:7883960.5000\n",
      "Epoch [101/200], Step[3/17] Loss:2154282.7500\n",
      "Epoch [101/200], Step[4/17] Loss:741600.3750\n",
      "Epoch [101/200], Step[5/17] Loss:3468274.0000\n",
      "Epoch [101/200], Step[6/17] Loss:1337627.7500\n",
      "Epoch [101/200], Step[7/17] Loss:4812594.5000\n",
      "Epoch [101/200], Step[8/17] Loss:2629389.7500\n",
      "Epoch [101/200], Step[9/17] Loss:3647864.2500\n",
      "Epoch [101/200], Step[10/17] Loss:1365183.5000\n",
      "Epoch [101/200], Step[11/17] Loss:2113829.7500\n",
      "Epoch [101/200], Step[12/17] Loss:767072.6250\n",
      "Epoch [101/200], Step[13/17] Loss:1807081.8750\n",
      "Epoch [101/200], Step[14/17] Loss:1743995.5000\n",
      "Epoch [101/200], Step[15/17] Loss:776902.3125\n",
      "Epoch [101/200], Step[16/17] Loss:1210125.2500\n",
      "Epoch [101/200], Step[17/17] Loss:466048.6250\n",
      "Epoch [102/200], Step[1/17] Loss:1952618.8750\n",
      "Epoch [102/200], Step[2/17] Loss:2178907.7500\n",
      "Epoch [102/200], Step[3/17] Loss:1198747.2500\n",
      "Epoch [102/200], Step[4/17] Loss:5107791.5000\n",
      "Epoch [102/200], Step[5/17] Loss:7548845.0000\n",
      "Epoch [102/200], Step[6/17] Loss:2538230.0000\n",
      "Epoch [102/200], Step[7/17] Loss:1761729.0000\n",
      "Epoch [102/200], Step[8/17] Loss:1100750.5000\n",
      "Epoch [102/200], Step[9/17] Loss:1436526.2500\n",
      "Epoch [102/200], Step[10/17] Loss:1651792.1250\n",
      "Epoch [102/200], Step[11/17] Loss:1339887.7500\n",
      "Epoch [102/200], Step[12/17] Loss:4058323.0000\n",
      "Epoch [102/200], Step[13/17] Loss:735384.6250\n",
      "Epoch [102/200], Step[14/17] Loss:3658425.2500\n",
      "Epoch [102/200], Step[15/17] Loss:1046492.3125\n",
      "Epoch [102/200], Step[16/17] Loss:1329708.8750\n",
      "Epoch [102/200], Step[17/17] Loss:1277686.7500\n",
      "Epoch [103/200], Step[1/17] Loss:1427484.8750\n",
      "Epoch [103/200], Step[2/17] Loss:3700333.5000\n",
      "Epoch [103/200], Step[3/17] Loss:899781.6250\n",
      "Epoch [103/200], Step[4/17] Loss:2975665.2500\n",
      "Epoch [103/200], Step[5/17] Loss:1377788.7500\n",
      "Epoch [103/200], Step[6/17] Loss:1338205.5000\n",
      "Epoch [103/200], Step[7/17] Loss:3746122.0000\n",
      "Epoch [103/200], Step[8/17] Loss:1973769.3750\n",
      "Epoch [103/200], Step[9/17] Loss:992497.3750\n",
      "Epoch [103/200], Step[10/17] Loss:7147560.5000\n",
      "Epoch [103/200], Step[11/17] Loss:925288.1875\n",
      "Epoch [103/200], Step[12/17] Loss:2205904.0000\n",
      "Epoch [103/200], Step[13/17] Loss:3278306.7500\n",
      "Epoch [103/200], Step[14/17] Loss:1925729.6250\n",
      "Epoch [103/200], Step[15/17] Loss:1695894.8750\n",
      "Epoch [103/200], Step[16/17] Loss:2878805.2500\n",
      "Epoch [103/200], Step[17/17] Loss:1468485.1250\n",
      "Epoch [104/200], Step[1/17] Loss:1261402.5000\n",
      "Epoch [104/200], Step[2/17] Loss:915448.9375\n",
      "Epoch [104/200], Step[3/17] Loss:994535.5625\n",
      "Epoch [104/200], Step[4/17] Loss:2866237.7500\n",
      "Epoch [104/200], Step[5/17] Loss:3715374.0000\n",
      "Epoch [104/200], Step[6/17] Loss:1582439.2500\n",
      "Epoch [104/200], Step[7/17] Loss:667601.0000\n",
      "Epoch [104/200], Step[8/17] Loss:1210461.7500\n",
      "Epoch [104/200], Step[9/17] Loss:760054.5625\n",
      "Epoch [104/200], Step[10/17] Loss:3679780.7500\n",
      "Epoch [104/200], Step[11/17] Loss:3689484.7500\n",
      "Epoch [104/200], Step[12/17] Loss:1114115.0000\n",
      "Epoch [104/200], Step[13/17] Loss:1492178.6250\n",
      "Epoch [104/200], Step[14/17] Loss:1880708.0000\n",
      "Epoch [104/200], Step[15/17] Loss:2137814.0000\n",
      "Epoch [104/200], Step[16/17] Loss:10861831.0000\n",
      "Epoch [104/200], Step[17/17] Loss:1049618.0000\n",
      "Epoch [105/200], Step[1/17] Loss:7517577.0000\n",
      "Epoch [105/200], Step[2/17] Loss:2364660.5000\n",
      "Epoch [105/200], Step[3/17] Loss:3364730.2500\n",
      "Epoch [105/200], Step[4/17] Loss:3800851.0000\n",
      "Epoch [105/200], Step[5/17] Loss:1916485.0000\n",
      "Epoch [105/200], Step[6/17] Loss:3038911.2500\n",
      "Epoch [105/200], Step[7/17] Loss:2421622.0000\n",
      "Epoch [105/200], Step[8/17] Loss:1289966.6250\n",
      "Epoch [105/200], Step[9/17] Loss:1595957.5000\n",
      "Epoch [105/200], Step[10/17] Loss:1047108.0625\n",
      "Epoch [105/200], Step[11/17] Loss:729225.3750\n",
      "Epoch [105/200], Step[12/17] Loss:3442657.7500\n",
      "Epoch [105/200], Step[13/17] Loss:3478733.5000\n",
      "Epoch [105/200], Step[14/17] Loss:551385.6250\n",
      "Epoch [105/200], Step[15/17] Loss:1006629.5000\n",
      "Epoch [105/200], Step[16/17] Loss:899201.3125\n",
      "Epoch [105/200], Step[17/17] Loss:1497328.3750\n",
      "Epoch [106/200], Step[1/17] Loss:953934.2500\n",
      "Epoch [106/200], Step[2/17] Loss:2038647.1250\n",
      "Epoch [106/200], Step[3/17] Loss:1431176.1250\n",
      "Epoch [106/200], Step[4/17] Loss:726934.0000\n",
      "Epoch [106/200], Step[5/17] Loss:1349237.0000\n",
      "Epoch [106/200], Step[6/17] Loss:2751240.0000\n",
      "Epoch [106/200], Step[7/17] Loss:4293052.0000\n",
      "Epoch [106/200], Step[8/17] Loss:2683385.5000\n",
      "Epoch [106/200], Step[9/17] Loss:2104637.7500\n",
      "Epoch [106/200], Step[10/17] Loss:7691912.0000\n",
      "Epoch [106/200], Step[11/17] Loss:1061726.0000\n",
      "Epoch [106/200], Step[12/17] Loss:1499175.6250\n",
      "Epoch [106/200], Step[13/17] Loss:3985633.2500\n",
      "Epoch [106/200], Step[14/17] Loss:1654634.0000\n",
      "Epoch [106/200], Step[15/17] Loss:2807066.7500\n",
      "Epoch [106/200], Step[16/17] Loss:1336779.0000\n",
      "Epoch [106/200], Step[17/17] Loss:1616135.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [107/200], Step[1/17] Loss:1153454.5000\n",
      "Epoch [107/200], Step[2/17] Loss:798600.0625\n",
      "Epoch [107/200], Step[3/17] Loss:3647249.0000\n",
      "Epoch [107/200], Step[4/17] Loss:834654.7500\n",
      "Epoch [107/200], Step[5/17] Loss:1238929.8750\n",
      "Epoch [107/200], Step[6/17] Loss:3460146.0000\n",
      "Epoch [107/200], Step[7/17] Loss:2941105.7500\n",
      "Epoch [107/200], Step[8/17] Loss:957854.2500\n",
      "Epoch [107/200], Step[9/17] Loss:882825.8750\n",
      "Epoch [107/200], Step[10/17] Loss:1087575.0000\n",
      "Epoch [107/200], Step[11/17] Loss:2338598.2500\n",
      "Epoch [107/200], Step[12/17] Loss:1956643.6250\n",
      "Epoch [107/200], Step[13/17] Loss:1588045.0000\n",
      "Epoch [107/200], Step[14/17] Loss:1925129.1250\n",
      "Epoch [107/200], Step[15/17] Loss:962165.5625\n",
      "Epoch [107/200], Step[16/17] Loss:8399291.0000\n",
      "Epoch [107/200], Step[17/17] Loss:6781554.5000\n",
      "Epoch [108/200], Step[1/17] Loss:1602382.6250\n",
      "Epoch [108/200], Step[2/17] Loss:3286143.2500\n",
      "Epoch [108/200], Step[3/17] Loss:1251764.5000\n",
      "Epoch [108/200], Step[4/17] Loss:1797359.2500\n",
      "Epoch [108/200], Step[5/17] Loss:3521276.2500\n",
      "Epoch [108/200], Step[6/17] Loss:1392411.8750\n",
      "Epoch [108/200], Step[7/17] Loss:1389259.3750\n",
      "Epoch [108/200], Step[8/17] Loss:856479.7500\n",
      "Epoch [108/200], Step[9/17] Loss:2465235.2500\n",
      "Epoch [108/200], Step[10/17] Loss:1897118.6250\n",
      "Epoch [108/200], Step[11/17] Loss:1680491.2500\n",
      "Epoch [108/200], Step[12/17] Loss:7155908.5000\n",
      "Epoch [108/200], Step[13/17] Loss:1048988.0000\n",
      "Epoch [108/200], Step[14/17] Loss:2447894.0000\n",
      "Epoch [108/200], Step[15/17] Loss:642964.3750\n",
      "Epoch [108/200], Step[16/17] Loss:6290497.0000\n",
      "Epoch [108/200], Step[17/17] Loss:1176746.8750\n",
      "Epoch [109/200], Step[1/17] Loss:1583762.6250\n",
      "Epoch [109/200], Step[2/17] Loss:2569949.7500\n",
      "Epoch [109/200], Step[3/17] Loss:729015.0625\n",
      "Epoch [109/200], Step[4/17] Loss:3379906.0000\n",
      "Epoch [109/200], Step[5/17] Loss:1624710.3750\n",
      "Epoch [109/200], Step[6/17] Loss:3530629.5000\n",
      "Epoch [109/200], Step[7/17] Loss:3385880.7500\n",
      "Epoch [109/200], Step[8/17] Loss:2007639.7500\n",
      "Epoch [109/200], Step[9/17] Loss:858929.0000\n",
      "Epoch [109/200], Step[10/17] Loss:3029218.2500\n",
      "Epoch [109/200], Step[11/17] Loss:1302886.0000\n",
      "Epoch [109/200], Step[12/17] Loss:1849082.7500\n",
      "Epoch [109/200], Step[13/17] Loss:1275241.2500\n",
      "Epoch [109/200], Step[14/17] Loss:1987636.1250\n",
      "Epoch [109/200], Step[15/17] Loss:8531714.0000\n",
      "Epoch [109/200], Step[16/17] Loss:1051766.5000\n",
      "Epoch [109/200], Step[17/17] Loss:1211462.6250\n",
      "Epoch [110/200], Step[1/17] Loss:1254829.6250\n",
      "Epoch [110/200], Step[2/17] Loss:2645924.2500\n",
      "Epoch [110/200], Step[3/17] Loss:1200990.2500\n",
      "Epoch [110/200], Step[4/17] Loss:972918.5625\n",
      "Epoch [110/200], Step[5/17] Loss:999963.5625\n",
      "Epoch [110/200], Step[6/17] Loss:2475103.5000\n",
      "Epoch [110/200], Step[7/17] Loss:1521214.7500\n",
      "Epoch [110/200], Step[8/17] Loss:4589498.5000\n",
      "Epoch [110/200], Step[9/17] Loss:3545606.2500\n",
      "Epoch [110/200], Step[10/17] Loss:3028833.2500\n",
      "Epoch [110/200], Step[11/17] Loss:1237559.0000\n",
      "Epoch [110/200], Step[12/17] Loss:8790706.0000\n",
      "Epoch [110/200], Step[13/17] Loss:1525929.7500\n",
      "Epoch [110/200], Step[14/17] Loss:1284500.2500\n",
      "Epoch [110/200], Step[15/17] Loss:689577.4375\n",
      "Epoch [110/200], Step[16/17] Loss:978604.2500\n",
      "Epoch [110/200], Step[17/17] Loss:3619103.5000\n",
      "Epoch [111/200], Step[1/17] Loss:5359779.0000\n",
      "Epoch [111/200], Step[2/17] Loss:1334475.1250\n",
      "Epoch [111/200], Step[3/17] Loss:3200151.2500\n",
      "Epoch [111/200], Step[4/17] Loss:1212429.0000\n",
      "Epoch [111/200], Step[5/17] Loss:1514866.8750\n",
      "Epoch [111/200], Step[6/17] Loss:1446095.8750\n",
      "Epoch [111/200], Step[7/17] Loss:3315468.0000\n",
      "Epoch [111/200], Step[8/17] Loss:3622188.0000\n",
      "Epoch [111/200], Step[9/17] Loss:865623.8125\n",
      "Epoch [111/200], Step[10/17] Loss:996873.1875\n",
      "Epoch [111/200], Step[11/17] Loss:716778.0625\n",
      "Epoch [111/200], Step[12/17] Loss:782072.5625\n",
      "Epoch [111/200], Step[13/17] Loss:945159.2500\n",
      "Epoch [111/200], Step[14/17] Loss:8181036.5000\n",
      "Epoch [111/200], Step[15/17] Loss:2668163.5000\n",
      "Epoch [111/200], Step[16/17] Loss:1377113.8750\n",
      "Epoch [111/200], Step[17/17] Loss:2638777.7500\n",
      "Epoch [112/200], Step[1/17] Loss:2015792.2500\n",
      "Epoch [112/200], Step[2/17] Loss:579823.9375\n",
      "Epoch [112/200], Step[3/17] Loss:2651658.2500\n",
      "Epoch [112/200], Step[4/17] Loss:903285.5625\n",
      "Epoch [112/200], Step[5/17] Loss:1447556.2500\n",
      "Epoch [112/200], Step[6/17] Loss:7346763.0000\n",
      "Epoch [112/200], Step[7/17] Loss:757887.8125\n",
      "Epoch [112/200], Step[8/17] Loss:2641349.7500\n",
      "Epoch [112/200], Step[9/17] Loss:1764825.5000\n",
      "Epoch [112/200], Step[10/17] Loss:3579745.7500\n",
      "Epoch [112/200], Step[11/17] Loss:1974579.2500\n",
      "Epoch [112/200], Step[12/17] Loss:5682270.5000\n",
      "Epoch [112/200], Step[13/17] Loss:1252591.3750\n",
      "Epoch [112/200], Step[14/17] Loss:1293922.6250\n",
      "Epoch [112/200], Step[15/17] Loss:3264460.7500\n",
      "Epoch [112/200], Step[16/17] Loss:990419.8750\n",
      "Epoch [112/200], Step[17/17] Loss:1889658.5000\n",
      "Epoch [113/200], Step[1/17] Loss:1005522.1875\n",
      "Epoch [113/200], Step[2/17] Loss:1150243.8750\n",
      "Epoch [113/200], Step[3/17] Loss:1775920.6250\n",
      "Epoch [113/200], Step[4/17] Loss:4042501.5000\n",
      "Epoch [113/200], Step[5/17] Loss:1159297.7500\n",
      "Epoch [113/200], Step[6/17] Loss:1053513.1250\n",
      "Epoch [113/200], Step[7/17] Loss:2045971.1250\n",
      "Epoch [113/200], Step[8/17] Loss:3463757.2500\n",
      "Epoch [113/200], Step[9/17] Loss:540200.4375\n",
      "Epoch [113/200], Step[10/17] Loss:3403910.7500\n",
      "Epoch [113/200], Step[11/17] Loss:1380120.3750\n",
      "Epoch [113/200], Step[12/17] Loss:1486568.1250\n",
      "Epoch [113/200], Step[13/17] Loss:1195478.6250\n",
      "Epoch [113/200], Step[14/17] Loss:1393244.6250\n",
      "Epoch [113/200], Step[15/17] Loss:3098257.5000\n",
      "Epoch [113/200], Step[16/17] Loss:10429960.0000\n",
      "Epoch [113/200], Step[17/17] Loss:1301924.6250\n",
      "Epoch [114/200], Step[1/17] Loss:4305395.0000\n",
      "Epoch [114/200], Step[2/17] Loss:5440189.0000\n",
      "Epoch [114/200], Step[3/17] Loss:2091074.7500\n",
      "Epoch [114/200], Step[4/17] Loss:3151098.5000\n",
      "Epoch [114/200], Step[5/17] Loss:499052.3438\n",
      "Epoch [114/200], Step[6/17] Loss:1397648.5000\n",
      "Epoch [114/200], Step[7/17] Loss:1136667.3750\n",
      "Epoch [114/200], Step[8/17] Loss:1468079.5000\n",
      "Epoch [114/200], Step[9/17] Loss:954873.1875\n",
      "Epoch [114/200], Step[10/17] Loss:2739058.7500\n",
      "Epoch [114/200], Step[11/17] Loss:1741338.3750\n",
      "Epoch [114/200], Step[12/17] Loss:725919.2500\n",
      "Epoch [114/200], Step[13/17] Loss:949266.8750\n",
      "Epoch [114/200], Step[14/17] Loss:7564675.0000\n",
      "Epoch [114/200], Step[15/17] Loss:3184306.2500\n",
      "Epoch [114/200], Step[16/17] Loss:1357515.1250\n",
      "Epoch [114/200], Step[17/17] Loss:1201383.1250\n",
      "Epoch [115/200], Step[1/17] Loss:1591244.3750\n",
      "Epoch [115/200], Step[2/17] Loss:2837096.5000\n",
      "Epoch [115/200], Step[3/17] Loss:1069940.1250\n",
      "Epoch [115/200], Step[4/17] Loss:3893973.5000\n",
      "Epoch [115/200], Step[5/17] Loss:1226186.2500\n",
      "Epoch [115/200], Step[6/17] Loss:2743994.2500\n",
      "Epoch [115/200], Step[7/17] Loss:850730.3750\n",
      "Epoch [115/200], Step[8/17] Loss:3789956.7500\n",
      "Epoch [115/200], Step[9/17] Loss:2541332.0000\n",
      "Epoch [115/200], Step[10/17] Loss:1380611.0000\n",
      "Epoch [115/200], Step[11/17] Loss:1129261.1250\n",
      "Epoch [115/200], Step[12/17] Loss:2003526.7500\n",
      "Epoch [115/200], Step[13/17] Loss:881895.9375\n",
      "Epoch [115/200], Step[14/17] Loss:3410575.7500\n",
      "Epoch [115/200], Step[15/17] Loss:2097689.7500\n",
      "Epoch [115/200], Step[16/17] Loss:7088474.0000\n",
      "Epoch [115/200], Step[17/17] Loss:1410207.0000\n",
      "Epoch [116/200], Step[1/17] Loss:772454.0625\n",
      "Epoch [116/200], Step[2/17] Loss:766297.5000\n",
      "Epoch [116/200], Step[3/17] Loss:1299402.5000\n",
      "Epoch [116/200], Step[4/17] Loss:892238.3125\n",
      "Epoch [116/200], Step[5/17] Loss:2109598.0000\n",
      "Epoch [116/200], Step[6/17] Loss:872796.5000\n",
      "Epoch [116/200], Step[7/17] Loss:1101990.2500\n",
      "Epoch [116/200], Step[8/17] Loss:2902910.2500\n",
      "Epoch [116/200], Step[9/17] Loss:1199108.5000\n",
      "Epoch [116/200], Step[10/17] Loss:8578894.0000\n",
      "Epoch [116/200], Step[11/17] Loss:1886613.7500\n",
      "Epoch [116/200], Step[12/17] Loss:3165418.2500\n",
      "Epoch [116/200], Step[13/17] Loss:5416354.0000\n",
      "Epoch [116/200], Step[14/17] Loss:2857276.2500\n",
      "Epoch [116/200], Step[15/17] Loss:2564051.2500\n",
      "Epoch [116/200], Step[16/17] Loss:1965727.6250\n",
      "Epoch [116/200], Step[17/17] Loss:1638338.0000\n",
      "Epoch [117/200], Step[1/17] Loss:1412630.3750\n",
      "Epoch [117/200], Step[2/17] Loss:568401.4375\n",
      "Epoch [117/200], Step[3/17] Loss:3622987.2500\n",
      "Epoch [117/200], Step[4/17] Loss:1522775.3750\n",
      "Epoch [117/200], Step[5/17] Loss:1731024.0000\n",
      "Epoch [117/200], Step[6/17] Loss:1477928.6250\n",
      "Epoch [117/200], Step[7/17] Loss:8078774.5000\n",
      "Epoch [117/200], Step[8/17] Loss:773600.6875\n",
      "Epoch [117/200], Step[9/17] Loss:1468247.0000\n",
      "Epoch [117/200], Step[10/17] Loss:2863383.2500\n",
      "Epoch [117/200], Step[11/17] Loss:2950593.2500\n",
      "Epoch [117/200], Step[12/17] Loss:3153499.2500\n",
      "Epoch [117/200], Step[13/17] Loss:725659.4375\n",
      "Epoch [117/200], Step[14/17] Loss:2000905.5000\n",
      "Epoch [117/200], Step[15/17] Loss:3156890.5000\n",
      "Epoch [117/200], Step[16/17] Loss:3571281.5000\n",
      "Epoch [117/200], Step[17/17] Loss:743014.6250\n",
      "Epoch [118/200], Step[1/17] Loss:1051549.2500\n",
      "Epoch [118/200], Step[2/17] Loss:1758940.3750\n",
      "Epoch [118/200], Step[3/17] Loss:2873436.2500\n",
      "Epoch [118/200], Step[4/17] Loss:6006595.5000\n",
      "Epoch [118/200], Step[5/17] Loss:3211929.2500\n",
      "Epoch [118/200], Step[6/17] Loss:7768802.5000\n",
      "Epoch [118/200], Step[7/17] Loss:946694.7500\n",
      "Epoch [118/200], Step[8/17] Loss:1607145.1250\n",
      "Epoch [118/200], Step[9/17] Loss:1420714.6250\n",
      "Epoch [118/200], Step[10/17] Loss:764420.3125\n",
      "Epoch [118/200], Step[11/17] Loss:2178477.7500\n",
      "Epoch [118/200], Step[12/17] Loss:1283708.3750\n",
      "Epoch [118/200], Step[13/17] Loss:2661543.0000\n",
      "Epoch [118/200], Step[14/17] Loss:741064.2500\n",
      "Epoch [118/200], Step[15/17] Loss:3092999.0000\n",
      "Epoch [118/200], Step[16/17] Loss:1058095.5000\n",
      "Epoch [118/200], Step[17/17] Loss:1546049.1250\n",
      "Epoch [119/200], Step[1/17] Loss:1308204.3750\n",
      "Epoch [119/200], Step[2/17] Loss:1099204.1250\n",
      "Epoch [119/200], Step[3/17] Loss:1127718.8750\n",
      "Epoch [119/200], Step[4/17] Loss:2052596.8750\n",
      "Epoch [119/200], Step[5/17] Loss:9182692.0000\n",
      "Epoch [119/200], Step[6/17] Loss:1413521.5000\n",
      "Epoch [119/200], Step[7/17] Loss:2575201.7500\n",
      "Epoch [119/200], Step[8/17] Loss:941194.3750\n",
      "Epoch [119/200], Step[9/17] Loss:2311530.7500\n",
      "Epoch [119/200], Step[10/17] Loss:4601490.0000\n",
      "Epoch [119/200], Step[11/17] Loss:2324435.7500\n",
      "Epoch [119/200], Step[12/17] Loss:3486113.0000\n",
      "Epoch [119/200], Step[13/17] Loss:1085552.6250\n",
      "Epoch [119/200], Step[14/17] Loss:895274.2500\n",
      "Epoch [119/200], Step[15/17] Loss:1385777.7500\n",
      "Epoch [119/200], Step[16/17] Loss:813733.2500\n",
      "Epoch [119/200], Step[17/17] Loss:3788358.0000\n",
      "Epoch [120/200], Step[1/17] Loss:4229359.5000\n",
      "Epoch [120/200], Step[2/17] Loss:994716.0625\n",
      "Epoch [120/200], Step[3/17] Loss:1555323.8750\n",
      "Epoch [120/200], Step[4/17] Loss:1746199.3750\n",
      "Epoch [120/200], Step[5/17] Loss:1647289.1250\n",
      "Epoch [120/200], Step[6/17] Loss:4485321.0000\n",
      "Epoch [120/200], Step[7/17] Loss:1406810.1250\n",
      "Epoch [120/200], Step[8/17] Loss:3381054.7500\n",
      "Epoch [120/200], Step[9/17] Loss:1532162.2500\n",
      "Epoch [120/200], Step[10/17] Loss:1861373.3750\n",
      "Epoch [120/200], Step[11/17] Loss:1810714.8750\n",
      "Epoch [120/200], Step[12/17] Loss:1359541.1250\n",
      "Epoch [120/200], Step[13/17] Loss:1086095.3750\n",
      "Epoch [120/200], Step[14/17] Loss:1189779.6250\n",
      "Epoch [120/200], Step[15/17] Loss:9535262.0000\n",
      "Epoch [120/200], Step[16/17] Loss:724820.5000\n",
      "Epoch [120/200], Step[17/17] Loss:1398717.7500\n",
      "Epoch [121/200], Step[1/17] Loss:1621180.2500\n",
      "Epoch [121/200], Step[2/17] Loss:7871845.5000\n",
      "Epoch [121/200], Step[3/17] Loss:1013693.4375\n",
      "Epoch [121/200], Step[4/17] Loss:2694490.7500\n",
      "Epoch [121/200], Step[5/17] Loss:1569458.1250\n",
      "Epoch [121/200], Step[6/17] Loss:1157349.7500\n",
      "Epoch [121/200], Step[7/17] Loss:742811.9375\n",
      "Epoch [121/200], Step[8/17] Loss:2379005.0000\n",
      "Epoch [121/200], Step[9/17] Loss:1368242.8750\n",
      "Epoch [121/200], Step[10/17] Loss:5544734.5000\n",
      "Epoch [121/200], Step[11/17] Loss:3413152.5000\n",
      "Epoch [121/200], Step[12/17] Loss:2261302.0000\n",
      "Epoch [121/200], Step[13/17] Loss:729201.5625\n",
      "Epoch [121/200], Step[14/17] Loss:1472349.3750\n",
      "Epoch [121/200], Step[15/17] Loss:3556042.5000\n",
      "Epoch [121/200], Step[16/17] Loss:1266257.7500\n",
      "Epoch [121/200], Step[17/17] Loss:1256815.5000\n",
      "Epoch [122/200], Step[1/17] Loss:1737870.8750\n",
      "Epoch [122/200], Step[2/17] Loss:2908931.2500\n",
      "Epoch [122/200], Step[3/17] Loss:3824371.0000\n",
      "Epoch [122/200], Step[4/17] Loss:817101.1250\n",
      "Epoch [122/200], Step[5/17] Loss:1282471.5000\n",
      "Epoch [122/200], Step[6/17] Loss:806778.0000\n",
      "Epoch [122/200], Step[7/17] Loss:2553122.0000\n",
      "Epoch [122/200], Step[8/17] Loss:883749.9375\n",
      "Epoch [122/200], Step[9/17] Loss:2623454.2500\n",
      "Epoch [122/200], Step[10/17] Loss:4344283.0000\n",
      "Epoch [122/200], Step[11/17] Loss:1281030.1250\n",
      "Epoch [122/200], Step[12/17] Loss:9156649.0000\n",
      "Epoch [122/200], Step[13/17] Loss:1842895.6250\n",
      "Epoch [122/200], Step[14/17] Loss:811445.6250\n",
      "Epoch [122/200], Step[15/17] Loss:1418678.8750\n",
      "Epoch [122/200], Step[16/17] Loss:2385543.7500\n",
      "Epoch [122/200], Step[17/17] Loss:1235575.0000\n",
      "Epoch [123/200], Step[1/17] Loss:2122752.0000\n",
      "Epoch [123/200], Step[2/17] Loss:601989.3750\n",
      "Epoch [123/200], Step[3/17] Loss:2289371.2500\n",
      "Epoch [123/200], Step[4/17] Loss:968072.2500\n",
      "Epoch [123/200], Step[5/17] Loss:3581037.0000\n",
      "Epoch [123/200], Step[6/17] Loss:1750643.2500\n",
      "Epoch [123/200], Step[7/17] Loss:4646078.0000\n",
      "Epoch [123/200], Step[8/17] Loss:3333803.2500\n",
      "Epoch [123/200], Step[9/17] Loss:1939606.1250\n",
      "Epoch [123/200], Step[10/17] Loss:1504746.8750\n",
      "Epoch [123/200], Step[11/17] Loss:1235822.8750\n",
      "Epoch [123/200], Step[12/17] Loss:1108754.7500\n",
      "Epoch [123/200], Step[13/17] Loss:3252717.5000\n",
      "Epoch [123/200], Step[14/17] Loss:8236844.5000\n",
      "Epoch [123/200], Step[15/17] Loss:1019154.2500\n",
      "Epoch [123/200], Step[16/17] Loss:860342.8750\n",
      "Epoch [123/200], Step[17/17] Loss:1514516.5000\n",
      "Epoch [124/200], Step[1/17] Loss:1132597.0000\n",
      "Epoch [124/200], Step[2/17] Loss:1079818.3750\n",
      "Epoch [124/200], Step[3/17] Loss:2244001.5000\n",
      "Epoch [124/200], Step[4/17] Loss:922219.0625\n",
      "Epoch [124/200], Step[5/17] Loss:1384916.2500\n",
      "Epoch [124/200], Step[6/17] Loss:1132344.6250\n",
      "Epoch [124/200], Step[7/17] Loss:2223453.0000\n",
      "Epoch [124/200], Step[8/17] Loss:4064276.5000\n",
      "Epoch [124/200], Step[9/17] Loss:739920.0000\n",
      "Epoch [124/200], Step[10/17] Loss:1055126.1250\n",
      "Epoch [124/200], Step[11/17] Loss:2253887.7500\n",
      "Epoch [124/200], Step[12/17] Loss:1417432.2500\n",
      "Epoch [124/200], Step[13/17] Loss:5418709.0000\n",
      "Epoch [124/200], Step[14/17] Loss:7865982.5000\n",
      "Epoch [124/200], Step[15/17] Loss:3393641.2500\n",
      "Epoch [124/200], Step[16/17] Loss:2024591.3750\n",
      "Epoch [124/200], Step[17/17] Loss:1636139.5000\n",
      "Epoch [125/200], Step[1/17] Loss:3688376.7500\n",
      "Epoch [125/200], Step[2/17] Loss:1370197.6250\n",
      "Epoch [125/200], Step[3/17] Loss:3769276.0000\n",
      "Epoch [125/200], Step[4/17] Loss:2627023.5000\n",
      "Epoch [125/200], Step[5/17] Loss:2853514.7500\n",
      "Epoch [125/200], Step[6/17] Loss:740824.4375\n",
      "Epoch [125/200], Step[7/17] Loss:7720962.0000\n",
      "Epoch [125/200], Step[8/17] Loss:1590126.1250\n",
      "Epoch [125/200], Step[9/17] Loss:1332859.5000\n",
      "Epoch [125/200], Step[10/17] Loss:1080179.5000\n",
      "Epoch [125/200], Step[11/17] Loss:1233971.3750\n",
      "Epoch [125/200], Step[12/17] Loss:1253030.5000\n",
      "Epoch [125/200], Step[13/17] Loss:1588653.6250\n",
      "Epoch [125/200], Step[14/17] Loss:758359.4375\n",
      "Epoch [125/200], Step[15/17] Loss:4049298.7500\n",
      "Epoch [125/200], Step[16/17] Loss:1922408.2500\n",
      "Epoch [125/200], Step[17/17] Loss:2588576.7500\n",
      "Epoch [126/200], Step[1/17] Loss:705336.9375\n",
      "Epoch [126/200], Step[2/17] Loss:2450939.0000\n",
      "Epoch [126/200], Step[3/17] Loss:1549367.2500\n",
      "Epoch [126/200], Step[4/17] Loss:1161564.3750\n",
      "Epoch [126/200], Step[5/17] Loss:1054810.7500\n",
      "Epoch [126/200], Step[6/17] Loss:4030667.0000\n",
      "Epoch [126/200], Step[7/17] Loss:1036374.3750\n",
      "Epoch [126/200], Step[8/17] Loss:3713731.0000\n",
      "Epoch [126/200], Step[9/17] Loss:2087111.5000\n",
      "Epoch [126/200], Step[10/17] Loss:614626.1250\n",
      "Epoch [126/200], Step[11/17] Loss:4253873.0000\n",
      "Epoch [126/200], Step[12/17] Loss:2507507.7500\n",
      "Epoch [126/200], Step[13/17] Loss:7364321.0000\n",
      "Epoch [126/200], Step[14/17] Loss:1251026.0000\n",
      "Epoch [126/200], Step[15/17] Loss:2437795.7500\n",
      "Epoch [126/200], Step[16/17] Loss:1960174.7500\n",
      "Epoch [126/200], Step[17/17] Loss:1849913.7500\n",
      "Epoch [127/200], Step[1/17] Loss:594619.1250\n",
      "Epoch [127/200], Step[2/17] Loss:1168652.0000\n",
      "Epoch [127/200], Step[3/17] Loss:3308765.7500\n",
      "Epoch [127/200], Step[4/17] Loss:1771836.6250\n",
      "Epoch [127/200], Step[5/17] Loss:777350.3750\n",
      "Epoch [127/200], Step[6/17] Loss:998963.8750\n",
      "Epoch [127/200], Step[7/17] Loss:1153264.1250\n",
      "Epoch [127/200], Step[8/17] Loss:3701725.5000\n",
      "Epoch [127/200], Step[9/17] Loss:4867992.0000\n",
      "Epoch [127/200], Step[10/17] Loss:3111098.7500\n",
      "Epoch [127/200], Step[11/17] Loss:7772253.0000\n",
      "Epoch [127/200], Step[12/17] Loss:712586.7500\n",
      "Epoch [127/200], Step[13/17] Loss:2007213.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [127/200], Step[14/17] Loss:793021.1250\n",
      "Epoch [127/200], Step[15/17] Loss:3606249.0000\n",
      "Epoch [127/200], Step[16/17] Loss:2112612.2500\n",
      "Epoch [127/200], Step[17/17] Loss:1506556.2500\n",
      "Epoch [128/200], Step[1/17] Loss:993430.1250\n",
      "Epoch [128/200], Step[2/17] Loss:2690883.0000\n",
      "Epoch [128/200], Step[3/17] Loss:2051068.5000\n",
      "Epoch [128/200], Step[4/17] Loss:964133.0625\n",
      "Epoch [128/200], Step[5/17] Loss:4894984.0000\n",
      "Epoch [128/200], Step[6/17] Loss:2730431.7500\n",
      "Epoch [128/200], Step[7/17] Loss:1898909.1250\n",
      "Epoch [128/200], Step[8/17] Loss:1780357.2500\n",
      "Epoch [128/200], Step[9/17] Loss:1674582.6250\n",
      "Epoch [128/200], Step[10/17] Loss:1231537.1250\n",
      "Epoch [128/200], Step[11/17] Loss:826971.3125\n",
      "Epoch [128/200], Step[12/17] Loss:1134317.6250\n",
      "Epoch [128/200], Step[13/17] Loss:2962082.5000\n",
      "Epoch [128/200], Step[14/17] Loss:8744248.0000\n",
      "Epoch [128/200], Step[15/17] Loss:2445824.5000\n",
      "Epoch [128/200], Step[16/17] Loss:988815.6875\n",
      "Epoch [128/200], Step[17/17] Loss:2055021.7500\n",
      "Epoch [129/200], Step[1/17] Loss:1558387.5000\n",
      "Epoch [129/200], Step[2/17] Loss:900286.1875\n",
      "Epoch [129/200], Step[3/17] Loss:701161.3750\n",
      "Epoch [129/200], Step[4/17] Loss:3884018.2500\n",
      "Epoch [129/200], Step[5/17] Loss:2308816.7500\n",
      "Epoch [129/200], Step[6/17] Loss:2148425.7500\n",
      "Epoch [129/200], Step[7/17] Loss:1940535.7500\n",
      "Epoch [129/200], Step[8/17] Loss:2569678.7500\n",
      "Epoch [129/200], Step[9/17] Loss:1817000.8750\n",
      "Epoch [129/200], Step[10/17] Loss:1168487.2500\n",
      "Epoch [129/200], Step[11/17] Loss:3776558.5000\n",
      "Epoch [129/200], Step[12/17] Loss:1809097.3750\n",
      "Epoch [129/200], Step[13/17] Loss:965592.3125\n",
      "Epoch [129/200], Step[14/17] Loss:2328216.0000\n",
      "Epoch [129/200], Step[15/17] Loss:1816574.6250\n",
      "Epoch [129/200], Step[16/17] Loss:2308947.0000\n",
      "Epoch [129/200], Step[17/17] Loss:9452919.0000\n",
      "Epoch [130/200], Step[1/17] Loss:1638057.3750\n",
      "Epoch [130/200], Step[2/17] Loss:872489.7500\n",
      "Epoch [130/200], Step[3/17] Loss:2411616.0000\n",
      "Epoch [130/200], Step[4/17] Loss:1893863.2500\n",
      "Epoch [130/200], Step[5/17] Loss:960877.4375\n",
      "Epoch [130/200], Step[6/17] Loss:3199493.2500\n",
      "Epoch [130/200], Step[7/17] Loss:6724197.5000\n",
      "Epoch [130/200], Step[8/17] Loss:3096082.0000\n",
      "Epoch [130/200], Step[9/17] Loss:1349937.1250\n",
      "Epoch [130/200], Step[10/17] Loss:1001581.8750\n",
      "Epoch [130/200], Step[11/17] Loss:2538916.5000\n",
      "Epoch [130/200], Step[12/17] Loss:1269841.7500\n",
      "Epoch [130/200], Step[13/17] Loss:749702.2500\n",
      "Epoch [130/200], Step[14/17] Loss:1096254.5000\n",
      "Epoch [130/200], Step[15/17] Loss:1831262.5000\n",
      "Epoch [130/200], Step[16/17] Loss:8265765.5000\n",
      "Epoch [130/200], Step[17/17] Loss:962883.7500\n",
      "Epoch [131/200], Step[1/17] Loss:1900460.3750\n",
      "Epoch [131/200], Step[2/17] Loss:1431515.2500\n",
      "Epoch [131/200], Step[3/17] Loss:2706713.2500\n",
      "Epoch [131/200], Step[4/17] Loss:874164.0000\n",
      "Epoch [131/200], Step[5/17] Loss:663049.0000\n",
      "Epoch [131/200], Step[6/17] Loss:2375077.0000\n",
      "Epoch [131/200], Step[7/17] Loss:4611425.0000\n",
      "Epoch [131/200], Step[8/17] Loss:7955730.5000\n",
      "Epoch [131/200], Step[9/17] Loss:1460367.8750\n",
      "Epoch [131/200], Step[10/17] Loss:3540654.5000\n",
      "Epoch [131/200], Step[11/17] Loss:2535706.2500\n",
      "Epoch [131/200], Step[12/17] Loss:1990111.2500\n",
      "Epoch [131/200], Step[13/17] Loss:1325413.1250\n",
      "Epoch [131/200], Step[14/17] Loss:2028605.8750\n",
      "Epoch [131/200], Step[15/17] Loss:1427943.3750\n",
      "Epoch [131/200], Step[16/17] Loss:934346.6250\n",
      "Epoch [131/200], Step[17/17] Loss:2364305.0000\n",
      "Epoch [132/200], Step[1/17] Loss:1050188.6250\n",
      "Epoch [132/200], Step[2/17] Loss:768869.5000\n",
      "Epoch [132/200], Step[3/17] Loss:1064385.3750\n",
      "Epoch [132/200], Step[4/17] Loss:2023865.7500\n",
      "Epoch [132/200], Step[5/17] Loss:1526414.7500\n",
      "Epoch [132/200], Step[6/17] Loss:3044165.2500\n",
      "Epoch [132/200], Step[7/17] Loss:1421123.1250\n",
      "Epoch [132/200], Step[8/17] Loss:674154.6875\n",
      "Epoch [132/200], Step[9/17] Loss:7845362.0000\n",
      "Epoch [132/200], Step[10/17] Loss:5010270.5000\n",
      "Epoch [132/200], Step[11/17] Loss:1778904.5000\n",
      "Epoch [132/200], Step[12/17] Loss:3711249.5000\n",
      "Epoch [132/200], Step[13/17] Loss:2804146.7500\n",
      "Epoch [132/200], Step[14/17] Loss:1045664.3750\n",
      "Epoch [132/200], Step[15/17] Loss:2943360.7500\n",
      "Epoch [132/200], Step[16/17] Loss:1244728.8750\n",
      "Epoch [132/200], Step[17/17] Loss:2123602.0000\n",
      "Epoch [133/200], Step[1/17] Loss:767616.5000\n",
      "Epoch [133/200], Step[2/17] Loss:1568396.2500\n",
      "Epoch [133/200], Step[3/17] Loss:1321428.0000\n",
      "Epoch [133/200], Step[4/17] Loss:1830114.0000\n",
      "Epoch [133/200], Step[5/17] Loss:1783687.3750\n",
      "Epoch [133/200], Step[6/17] Loss:3745753.7500\n",
      "Epoch [133/200], Step[7/17] Loss:1570159.3750\n",
      "Epoch [133/200], Step[8/17] Loss:894450.1250\n",
      "Epoch [133/200], Step[9/17] Loss:892040.6875\n",
      "Epoch [133/200], Step[10/17] Loss:808880.5625\n",
      "Epoch [133/200], Step[11/17] Loss:3479792.5000\n",
      "Epoch [133/200], Step[12/17] Loss:1460677.8750\n",
      "Epoch [133/200], Step[13/17] Loss:8494109.0000\n",
      "Epoch [133/200], Step[14/17] Loss:1071093.6250\n",
      "Epoch [133/200], Step[15/17] Loss:3536039.2500\n",
      "Epoch [133/200], Step[16/17] Loss:1108787.6250\n",
      "Epoch [133/200], Step[17/17] Loss:6583697.5000\n",
      "Epoch [134/200], Step[1/17] Loss:1918596.7500\n",
      "Epoch [134/200], Step[2/17] Loss:1871004.2500\n",
      "Epoch [134/200], Step[3/17] Loss:3252472.0000\n",
      "Epoch [134/200], Step[4/17] Loss:2802173.7500\n",
      "Epoch [134/200], Step[5/17] Loss:2695562.2500\n",
      "Epoch [134/200], Step[6/17] Loss:1708551.7500\n",
      "Epoch [134/200], Step[7/17] Loss:1988623.6250\n",
      "Epoch [134/200], Step[8/17] Loss:1354601.6250\n",
      "Epoch [134/200], Step[9/17] Loss:1508733.6250\n",
      "Epoch [134/200], Step[10/17] Loss:1221024.8750\n",
      "Epoch [134/200], Step[11/17] Loss:7871436.0000\n",
      "Epoch [134/200], Step[12/17] Loss:2307350.2500\n",
      "Epoch [134/200], Step[13/17] Loss:910623.6875\n",
      "Epoch [134/200], Step[14/17] Loss:2643740.5000\n",
      "Epoch [134/200], Step[15/17] Loss:1471177.6250\n",
      "Epoch [134/200], Step[16/17] Loss:1518165.6250\n",
      "Epoch [134/200], Step[17/17] Loss:3247314.0000\n",
      "Epoch [135/200], Step[1/17] Loss:1276742.1250\n",
      "Epoch [135/200], Step[2/17] Loss:4233502.5000\n",
      "Epoch [135/200], Step[3/17] Loss:2186420.5000\n",
      "Epoch [135/200], Step[4/17] Loss:1361946.7500\n",
      "Epoch [135/200], Step[5/17] Loss:2467289.5000\n",
      "Epoch [135/200], Step[6/17] Loss:2150654.2500\n",
      "Epoch [135/200], Step[7/17] Loss:2672991.7500\n",
      "Epoch [135/200], Step[8/17] Loss:1476347.8750\n",
      "Epoch [135/200], Step[9/17] Loss:1871471.0000\n",
      "Epoch [135/200], Step[10/17] Loss:1191930.5000\n",
      "Epoch [135/200], Step[11/17] Loss:2678308.0000\n",
      "Epoch [135/200], Step[12/17] Loss:3374233.5000\n",
      "Epoch [135/200], Step[13/17] Loss:7255612.0000\n",
      "Epoch [135/200], Step[14/17] Loss:1082827.0000\n",
      "Epoch [135/200], Step[15/17] Loss:1012279.8750\n",
      "Epoch [135/200], Step[16/17] Loss:2854008.7500\n",
      "Epoch [135/200], Step[17/17] Loss:659341.1875\n",
      "Epoch [136/200], Step[1/17] Loss:4485652.5000\n",
      "Epoch [136/200], Step[2/17] Loss:1440875.6250\n",
      "Epoch [136/200], Step[3/17] Loss:958142.8125\n",
      "Epoch [136/200], Step[4/17] Loss:2173046.0000\n",
      "Epoch [136/200], Step[5/17] Loss:1367265.8750\n",
      "Epoch [136/200], Step[6/17] Loss:1861444.8750\n",
      "Epoch [136/200], Step[7/17] Loss:7884811.0000\n",
      "Epoch [136/200], Step[8/17] Loss:1425543.6250\n",
      "Epoch [136/200], Step[9/17] Loss:1224207.5000\n",
      "Epoch [136/200], Step[10/17] Loss:1476307.8750\n",
      "Epoch [136/200], Step[11/17] Loss:3481051.2500\n",
      "Epoch [136/200], Step[12/17] Loss:1362897.0000\n",
      "Epoch [136/200], Step[13/17] Loss:810891.8750\n",
      "Epoch [136/200], Step[14/17] Loss:834829.6250\n",
      "Epoch [136/200], Step[15/17] Loss:4783640.0000\n",
      "Epoch [136/200], Step[16/17] Loss:3190783.5000\n",
      "Epoch [136/200], Step[17/17] Loss:1133402.3750\n",
      "Epoch [137/200], Step[1/17] Loss:2594463.0000\n",
      "Epoch [137/200], Step[2/17] Loss:1852965.2500\n",
      "Epoch [137/200], Step[3/17] Loss:1378017.0000\n",
      "Epoch [137/200], Step[4/17] Loss:2448376.2500\n",
      "Epoch [137/200], Step[5/17] Loss:1146084.1250\n",
      "Epoch [137/200], Step[6/17] Loss:1596946.6250\n",
      "Epoch [137/200], Step[7/17] Loss:2782183.2500\n",
      "Epoch [137/200], Step[8/17] Loss:7300862.0000\n",
      "Epoch [137/200], Step[9/17] Loss:2394119.0000\n",
      "Epoch [137/200], Step[10/17] Loss:1478072.6250\n",
      "Epoch [137/200], Step[11/17] Loss:821835.7500\n",
      "Epoch [137/200], Step[12/17] Loss:620551.6250\n",
      "Epoch [137/200], Step[13/17] Loss:936728.8125\n",
      "Epoch [137/200], Step[14/17] Loss:5588450.0000\n",
      "Epoch [137/200], Step[15/17] Loss:1627273.3750\n",
      "Epoch [137/200], Step[16/17] Loss:1069828.1250\n",
      "Epoch [137/200], Step[17/17] Loss:4979106.0000\n",
      "Epoch [138/200], Step[1/17] Loss:1055434.2500\n",
      "Epoch [138/200], Step[2/17] Loss:826362.4375\n",
      "Epoch [138/200], Step[3/17] Loss:723015.2500\n",
      "Epoch [138/200], Step[4/17] Loss:1009361.3750\n",
      "Epoch [138/200], Step[5/17] Loss:2776488.7500\n",
      "Epoch [138/200], Step[6/17] Loss:1737785.7500\n",
      "Epoch [138/200], Step[7/17] Loss:1101205.0000\n",
      "Epoch [138/200], Step[8/17] Loss:3643740.0000\n",
      "Epoch [138/200], Step[9/17] Loss:1364686.0000\n",
      "Epoch [138/200], Step[10/17] Loss:3435445.0000\n",
      "Epoch [138/200], Step[11/17] Loss:908334.2500\n",
      "Epoch [138/200], Step[12/17] Loss:3242360.7500\n",
      "Epoch [138/200], Step[13/17] Loss:2870532.2500\n",
      "Epoch [138/200], Step[14/17] Loss:1174521.8750\n",
      "Epoch [138/200], Step[15/17] Loss:3092779.7500\n",
      "Epoch [138/200], Step[16/17] Loss:3019299.7500\n",
      "Epoch [138/200], Step[17/17] Loss:9478065.0000\n",
      "Epoch [139/200], Step[1/17] Loss:1712296.1250\n",
      "Epoch [139/200], Step[2/17] Loss:1221693.6250\n",
      "Epoch [139/200], Step[3/17] Loss:3526934.5000\n",
      "Epoch [139/200], Step[4/17] Loss:1440008.8750\n",
      "Epoch [139/200], Step[5/17] Loss:3174371.7500\n",
      "Epoch [139/200], Step[6/17] Loss:2120557.7500\n",
      "Epoch [139/200], Step[7/17] Loss:2101483.2500\n",
      "Epoch [139/200], Step[8/17] Loss:868189.8125\n",
      "Epoch [139/200], Step[9/17] Loss:1455054.0000\n",
      "Epoch [139/200], Step[10/17] Loss:7061439.5000\n",
      "Epoch [139/200], Step[11/17] Loss:594455.6875\n",
      "Epoch [139/200], Step[12/17] Loss:1201698.6250\n",
      "Epoch [139/200], Step[13/17] Loss:2532307.7500\n",
      "Epoch [139/200], Step[14/17] Loss:4937718.5000\n",
      "Epoch [139/200], Step[15/17] Loss:2675652.0000\n",
      "Epoch [139/200], Step[16/17] Loss:2090066.1250\n",
      "Epoch [139/200], Step[17/17] Loss:1191818.1250\n",
      "Epoch [140/200], Step[1/17] Loss:2169646.7500\n",
      "Epoch [140/200], Step[2/17] Loss:2954566.5000\n",
      "Epoch [140/200], Step[3/17] Loss:2279998.5000\n",
      "Epoch [140/200], Step[4/17] Loss:1877300.2500\n",
      "Epoch [140/200], Step[5/17] Loss:597598.8125\n",
      "Epoch [140/200], Step[6/17] Loss:1349043.2500\n",
      "Epoch [140/200], Step[7/17] Loss:1346833.5000\n",
      "Epoch [140/200], Step[8/17] Loss:3764329.7500\n",
      "Epoch [140/200], Step[9/17] Loss:2660716.7500\n",
      "Epoch [140/200], Step[10/17] Loss:775988.0000\n",
      "Epoch [140/200], Step[11/17] Loss:963787.0000\n",
      "Epoch [140/200], Step[12/17] Loss:8378660.5000\n",
      "Epoch [140/200], Step[13/17] Loss:1406626.2500\n",
      "Epoch [140/200], Step[14/17] Loss:1154631.7500\n",
      "Epoch [140/200], Step[15/17] Loss:1812088.7500\n",
      "Epoch [140/200], Step[16/17] Loss:5208877.5000\n",
      "Epoch [140/200], Step[17/17] Loss:1208107.2500\n",
      "Epoch [141/200], Step[1/17] Loss:2090239.1250\n",
      "Epoch [141/200], Step[2/17] Loss:1336013.7500\n",
      "Epoch [141/200], Step[3/17] Loss:2442723.5000\n",
      "Epoch [141/200], Step[4/17] Loss:1017339.3750\n",
      "Epoch [141/200], Step[5/17] Loss:4734891.5000\n",
      "Epoch [141/200], Step[6/17] Loss:2180795.2500\n",
      "Epoch [141/200], Step[7/17] Loss:7849525.0000\n",
      "Epoch [141/200], Step[8/17] Loss:3820139.5000\n",
      "Epoch [141/200], Step[9/17] Loss:1071894.8750\n",
      "Epoch [141/200], Step[10/17] Loss:1770157.8750\n",
      "Epoch [141/200], Step[11/17] Loss:781122.0625\n",
      "Epoch [141/200], Step[12/17] Loss:1262815.3750\n",
      "Epoch [141/200], Step[13/17] Loss:4311664.5000\n",
      "Epoch [141/200], Step[14/17] Loss:1190848.3750\n",
      "Epoch [141/200], Step[15/17] Loss:1553547.6250\n",
      "Epoch [141/200], Step[16/17] Loss:1531602.3750\n",
      "Epoch [141/200], Step[17/17] Loss:907028.8750\n",
      "Epoch [142/200], Step[1/17] Loss:2632071.7500\n",
      "Epoch [142/200], Step[2/17] Loss:3772461.2500\n",
      "Epoch [142/200], Step[3/17] Loss:3848714.5000\n",
      "Epoch [142/200], Step[4/17] Loss:964132.0625\n",
      "Epoch [142/200], Step[5/17] Loss:3851572.2500\n",
      "Epoch [142/200], Step[6/17] Loss:1305707.2500\n",
      "Epoch [142/200], Step[7/17] Loss:851086.6875\n",
      "Epoch [142/200], Step[8/17] Loss:1165826.8750\n",
      "Epoch [142/200], Step[9/17] Loss:3199562.7500\n",
      "Epoch [142/200], Step[10/17] Loss:1888536.6250\n",
      "Epoch [142/200], Step[11/17] Loss:681197.0000\n",
      "Epoch [142/200], Step[12/17] Loss:1790818.5000\n",
      "Epoch [142/200], Step[13/17] Loss:1197341.7500\n",
      "Epoch [142/200], Step[14/17] Loss:1567901.6250\n",
      "Epoch [142/200], Step[15/17] Loss:1837963.0000\n",
      "Epoch [142/200], Step[16/17] Loss:1840017.7500\n",
      "Epoch [142/200], Step[17/17] Loss:8969069.0000\n",
      "Epoch [143/200], Step[1/17] Loss:1463572.8750\n",
      "Epoch [143/200], Step[2/17] Loss:1323652.3750\n",
      "Epoch [143/200], Step[3/17] Loss:1365349.1250\n",
      "Epoch [143/200], Step[4/17] Loss:992271.0000\n",
      "Epoch [143/200], Step[5/17] Loss:4092054.2500\n",
      "Epoch [143/200], Step[6/17] Loss:1351608.3750\n",
      "Epoch [143/200], Step[7/17] Loss:7785488.0000\n",
      "Epoch [143/200], Step[8/17] Loss:3118473.2500\n",
      "Epoch [143/200], Step[9/17] Loss:920607.5625\n",
      "Epoch [143/200], Step[10/17] Loss:2314677.7500\n",
      "Epoch [143/200], Step[11/17] Loss:835413.8750\n",
      "Epoch [143/200], Step[12/17] Loss:2941492.2500\n",
      "Epoch [143/200], Step[13/17] Loss:1520790.1250\n",
      "Epoch [143/200], Step[14/17] Loss:1303214.8750\n",
      "Epoch [143/200], Step[15/17] Loss:3440448.2500\n",
      "Epoch [143/200], Step[16/17] Loss:2310269.2500\n",
      "Epoch [143/200], Step[17/17] Loss:3203567.0000\n",
      "Epoch [144/200], Step[1/17] Loss:7427642.0000\n",
      "Epoch [144/200], Step[2/17] Loss:1395755.5000\n",
      "Epoch [144/200], Step[3/17] Loss:2470629.5000\n",
      "Epoch [144/200], Step[4/17] Loss:2372646.7500\n",
      "Epoch [144/200], Step[5/17] Loss:1831705.8750\n",
      "Epoch [144/200], Step[6/17] Loss:1117471.5000\n",
      "Epoch [144/200], Step[7/17] Loss:793517.1250\n",
      "Epoch [144/200], Step[8/17] Loss:1183160.7500\n",
      "Epoch [144/200], Step[9/17] Loss:3112321.0000\n",
      "Epoch [144/200], Step[10/17] Loss:2767287.2500\n",
      "Epoch [144/200], Step[11/17] Loss:3053033.2500\n",
      "Epoch [144/200], Step[12/17] Loss:1920775.0000\n",
      "Epoch [144/200], Step[13/17] Loss:1379617.3750\n",
      "Epoch [144/200], Step[14/17] Loss:1713271.6250\n",
      "Epoch [144/200], Step[15/17] Loss:2599488.2500\n",
      "Epoch [144/200], Step[16/17] Loss:1095235.6250\n",
      "Epoch [144/200], Step[17/17] Loss:4244581.0000\n",
      "Epoch [145/200], Step[1/17] Loss:8378889.5000\n",
      "Epoch [145/200], Step[2/17] Loss:521165.2188\n",
      "Epoch [145/200], Step[3/17] Loss:1072078.2500\n",
      "Epoch [145/200], Step[4/17] Loss:1138259.8750\n",
      "Epoch [145/200], Step[5/17] Loss:3717115.7500\n",
      "Epoch [145/200], Step[6/17] Loss:1120881.6250\n",
      "Epoch [145/200], Step[7/17] Loss:835668.0625\n",
      "Epoch [145/200], Step[8/17] Loss:923212.2500\n",
      "Epoch [145/200], Step[9/17] Loss:3639233.5000\n",
      "Epoch [145/200], Step[10/17] Loss:4381878.0000\n",
      "Epoch [145/200], Step[11/17] Loss:3295465.0000\n",
      "Epoch [145/200], Step[12/17] Loss:1492366.6250\n",
      "Epoch [145/200], Step[13/17] Loss:2179062.7500\n",
      "Epoch [145/200], Step[14/17] Loss:1890705.7500\n",
      "Epoch [145/200], Step[15/17] Loss:2580375.2500\n",
      "Epoch [145/200], Step[16/17] Loss:1620710.0000\n",
      "Epoch [145/200], Step[17/17] Loss:1101800.5000\n",
      "Epoch [146/200], Step[1/17] Loss:1328015.1250\n",
      "Epoch [146/200], Step[2/17] Loss:1872119.8750\n",
      "Epoch [146/200], Step[3/17] Loss:855270.1250\n",
      "Epoch [146/200], Step[4/17] Loss:1266607.6250\n",
      "Epoch [146/200], Step[5/17] Loss:2650479.7500\n",
      "Epoch [146/200], Step[6/17] Loss:1351087.5000\n",
      "Epoch [146/200], Step[7/17] Loss:2009001.5000\n",
      "Epoch [146/200], Step[8/17] Loss:1461119.2500\n",
      "Epoch [146/200], Step[9/17] Loss:3793693.5000\n",
      "Epoch [146/200], Step[10/17] Loss:1708041.7500\n",
      "Epoch [146/200], Step[11/17] Loss:7975238.0000\n",
      "Epoch [146/200], Step[12/17] Loss:1234514.5000\n",
      "Epoch [146/200], Step[13/17] Loss:4010478.7500\n",
      "Epoch [146/200], Step[14/17] Loss:4388967.5000\n",
      "Epoch [146/200], Step[15/17] Loss:1835830.2500\n",
      "Epoch [146/200], Step[16/17] Loss:1309378.6250\n",
      "Epoch [146/200], Step[17/17] Loss:778383.1875\n",
      "Epoch [147/200], Step[1/17] Loss:1589707.1250\n",
      "Epoch [147/200], Step[2/17] Loss:744142.7500\n",
      "Epoch [147/200], Step[3/17] Loss:1687796.1250\n",
      "Epoch [147/200], Step[4/17] Loss:1849175.0000\n",
      "Epoch [147/200], Step[5/17] Loss:3399633.0000\n",
      "Epoch [147/200], Step[6/17] Loss:8649631.0000\n",
      "Epoch [147/200], Step[7/17] Loss:3451386.5000\n",
      "Epoch [147/200], Step[8/17] Loss:1101543.6250\n",
      "Epoch [147/200], Step[9/17] Loss:3836513.2500\n",
      "Epoch [147/200], Step[10/17] Loss:1053715.7500\n",
      "Epoch [147/200], Step[11/17] Loss:2014344.6250\n",
      "Epoch [147/200], Step[12/17] Loss:771442.5000\n",
      "Epoch [147/200], Step[13/17] Loss:804264.3750\n",
      "Epoch [147/200], Step[14/17] Loss:2713696.2500\n",
      "Epoch [147/200], Step[15/17] Loss:2963595.5000\n",
      "Epoch [147/200], Step[16/17] Loss:792818.5000\n",
      "Epoch [147/200], Step[17/17] Loss:2780154.0000\n",
      "Epoch [148/200], Step[1/17] Loss:977352.9375\n",
      "Epoch [148/200], Step[2/17] Loss:1241235.8750\n",
      "Epoch [148/200], Step[3/17] Loss:2873918.7500\n",
      "Epoch [148/200], Step[4/17] Loss:3739928.7500\n",
      "Epoch [148/200], Step[5/17] Loss:2284585.7500\n",
      "Epoch [148/200], Step[6/17] Loss:1332581.3750\n",
      "Epoch [148/200], Step[7/17] Loss:1062963.8750\n",
      "Epoch [148/200], Step[8/17] Loss:1124556.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [148/200], Step[9/17] Loss:1392603.7500\n",
      "Epoch [148/200], Step[10/17] Loss:1500162.6250\n",
      "Epoch [148/200], Step[11/17] Loss:761812.8125\n",
      "Epoch [148/200], Step[12/17] Loss:740315.0625\n",
      "Epoch [148/200], Step[13/17] Loss:1773006.2500\n",
      "Epoch [148/200], Step[14/17] Loss:1391994.1250\n",
      "Epoch [148/200], Step[15/17] Loss:831787.0000\n",
      "Epoch [148/200], Step[16/17] Loss:13006539.0000\n",
      "Epoch [148/200], Step[17/17] Loss:4488537.5000\n",
      "Epoch [149/200], Step[1/17] Loss:887585.9375\n",
      "Epoch [149/200], Step[2/17] Loss:9771950.0000\n",
      "Epoch [149/200], Step[3/17] Loss:712046.3750\n",
      "Epoch [149/200], Step[4/17] Loss:1109559.3750\n",
      "Epoch [149/200], Step[5/17] Loss:3480792.7500\n",
      "Epoch [149/200], Step[6/17] Loss:2245722.2500\n",
      "Epoch [149/200], Step[7/17] Loss:1440656.8750\n",
      "Epoch [149/200], Step[8/17] Loss:2930899.7500\n",
      "Epoch [149/200], Step[9/17] Loss:1295447.8750\n",
      "Epoch [149/200], Step[10/17] Loss:1343870.0000\n",
      "Epoch [149/200], Step[11/17] Loss:1296006.2500\n",
      "Epoch [149/200], Step[12/17] Loss:1400740.3750\n",
      "Epoch [149/200], Step[13/17] Loss:1927725.0000\n",
      "Epoch [149/200], Step[14/17] Loss:1940969.0000\n",
      "Epoch [149/200], Step[15/17] Loss:3792436.5000\n",
      "Epoch [149/200], Step[16/17] Loss:2672457.0000\n",
      "Epoch [149/200], Step[17/17] Loss:1764204.7500\n",
      "Epoch [150/200], Step[1/17] Loss:7949041.0000\n",
      "Epoch [150/200], Step[2/17] Loss:1183573.5000\n",
      "Epoch [150/200], Step[3/17] Loss:965656.9375\n",
      "Epoch [150/200], Step[4/17] Loss:4777657.5000\n",
      "Epoch [150/200], Step[5/17] Loss:1976637.0000\n",
      "Epoch [150/200], Step[6/17] Loss:2801904.7500\n",
      "Epoch [150/200], Step[7/17] Loss:470372.2812\n",
      "Epoch [150/200], Step[8/17] Loss:1310845.5000\n",
      "Epoch [150/200], Step[9/17] Loss:1060179.8750\n",
      "Epoch [150/200], Step[10/17] Loss:678417.4375\n",
      "Epoch [150/200], Step[11/17] Loss:3487672.2500\n",
      "Epoch [150/200], Step[12/17] Loss:1800928.2500\n",
      "Epoch [150/200], Step[13/17] Loss:1845488.0000\n",
      "Epoch [150/200], Step[14/17] Loss:2649393.0000\n",
      "Epoch [150/200], Step[15/17] Loss:892033.0000\n",
      "Epoch [150/200], Step[16/17] Loss:1835686.1250\n",
      "Epoch [150/200], Step[17/17] Loss:4919131.0000\n",
      "Epoch [151/200], Step[1/17] Loss:1266040.1250\n",
      "Epoch [151/200], Step[2/17] Loss:1032606.9375\n",
      "Epoch [151/200], Step[3/17] Loss:3735489.2500\n",
      "Epoch [151/200], Step[4/17] Loss:1817977.1250\n",
      "Epoch [151/200], Step[5/17] Loss:2529304.2500\n",
      "Epoch [151/200], Step[6/17] Loss:1022976.8125\n",
      "Epoch [151/200], Step[7/17] Loss:1758182.0000\n",
      "Epoch [151/200], Step[8/17] Loss:7697448.5000\n",
      "Epoch [151/200], Step[9/17] Loss:2925878.0000\n",
      "Epoch [151/200], Step[10/17] Loss:1392708.7500\n",
      "Epoch [151/200], Step[11/17] Loss:1460224.3750\n",
      "Epoch [151/200], Step[12/17] Loss:1022391.5000\n",
      "Epoch [151/200], Step[13/17] Loss:2275062.0000\n",
      "Epoch [151/200], Step[14/17] Loss:5572120.5000\n",
      "Epoch [151/200], Step[15/17] Loss:1612318.8750\n",
      "Epoch [151/200], Step[16/17] Loss:793988.6875\n",
      "Epoch [151/200], Step[17/17] Loss:2175461.0000\n",
      "Epoch [152/200], Step[1/17] Loss:1737719.5000\n",
      "Epoch [152/200], Step[2/17] Loss:1775115.3750\n",
      "Epoch [152/200], Step[3/17] Loss:4616883.0000\n",
      "Epoch [152/200], Step[4/17] Loss:1140005.5000\n",
      "Epoch [152/200], Step[5/17] Loss:1374411.3750\n",
      "Epoch [152/200], Step[6/17] Loss:6072856.0000\n",
      "Epoch [152/200], Step[7/17] Loss:1198631.5000\n",
      "Epoch [152/200], Step[8/17] Loss:1692241.0000\n",
      "Epoch [152/200], Step[9/17] Loss:1093148.6250\n",
      "Epoch [152/200], Step[10/17] Loss:1371200.6250\n",
      "Epoch [152/200], Step[11/17] Loss:925020.3125\n",
      "Epoch [152/200], Step[12/17] Loss:7626621.5000\n",
      "Epoch [152/200], Step[13/17] Loss:1880314.1250\n",
      "Epoch [152/200], Step[14/17] Loss:1356937.5000\n",
      "Epoch [152/200], Step[15/17] Loss:856571.8125\n",
      "Epoch [152/200], Step[16/17] Loss:1052895.2500\n",
      "Epoch [152/200], Step[17/17] Loss:4814408.0000\n",
      "Epoch [153/200], Step[1/17] Loss:1146337.2500\n",
      "Epoch [153/200], Step[2/17] Loss:2777900.5000\n",
      "Epoch [153/200], Step[3/17] Loss:2218475.2500\n",
      "Epoch [153/200], Step[4/17] Loss:1223036.5000\n",
      "Epoch [153/200], Step[5/17] Loss:1957325.0000\n",
      "Epoch [153/200], Step[6/17] Loss:2503148.5000\n",
      "Epoch [153/200], Step[7/17] Loss:1816391.7500\n",
      "Epoch [153/200], Step[8/17] Loss:2475938.5000\n",
      "Epoch [153/200], Step[9/17] Loss:2895050.0000\n",
      "Epoch [153/200], Step[10/17] Loss:1133824.8750\n",
      "Epoch [153/200], Step[11/17] Loss:1997832.3750\n",
      "Epoch [153/200], Step[12/17] Loss:1038591.0625\n",
      "Epoch [153/200], Step[13/17] Loss:1610673.6250\n",
      "Epoch [153/200], Step[14/17] Loss:2422587.5000\n",
      "Epoch [153/200], Step[15/17] Loss:1154263.0000\n",
      "Epoch [153/200], Step[16/17] Loss:3479386.7500\n",
      "Epoch [153/200], Step[17/17] Loss:9638791.0000\n",
      "Epoch [154/200], Step[1/17] Loss:1915374.3750\n",
      "Epoch [154/200], Step[2/17] Loss:1640064.3750\n",
      "Epoch [154/200], Step[3/17] Loss:3116755.7500\n",
      "Epoch [154/200], Step[4/17] Loss:4081302.0000\n",
      "Epoch [154/200], Step[5/17] Loss:3421571.7500\n",
      "Epoch [154/200], Step[6/17] Loss:3606193.7500\n",
      "Epoch [154/200], Step[7/17] Loss:3643843.0000\n",
      "Epoch [154/200], Step[8/17] Loss:1041946.7500\n",
      "Epoch [154/200], Step[9/17] Loss:2041607.0000\n",
      "Epoch [154/200], Step[10/17] Loss:837948.4375\n",
      "Epoch [154/200], Step[11/17] Loss:838361.6250\n",
      "Epoch [154/200], Step[12/17] Loss:1185564.3750\n",
      "Epoch [154/200], Step[13/17] Loss:1132750.8750\n",
      "Epoch [154/200], Step[14/17] Loss:1492285.6250\n",
      "Epoch [154/200], Step[15/17] Loss:1658884.6250\n",
      "Epoch [154/200], Step[16/17] Loss:880051.0625\n",
      "Epoch [154/200], Step[17/17] Loss:8797262.0000\n",
      "Epoch [155/200], Step[1/17] Loss:955857.3750\n",
      "Epoch [155/200], Step[2/17] Loss:3167458.7500\n",
      "Epoch [155/200], Step[3/17] Loss:1494150.8750\n",
      "Epoch [155/200], Step[4/17] Loss:1025010.7500\n",
      "Epoch [155/200], Step[5/17] Loss:1955637.1250\n",
      "Epoch [155/200], Step[6/17] Loss:1121686.3750\n",
      "Epoch [155/200], Step[7/17] Loss:1965964.6250\n",
      "Epoch [155/200], Step[8/17] Loss:3523175.0000\n",
      "Epoch [155/200], Step[9/17] Loss:1718027.1250\n",
      "Epoch [155/200], Step[10/17] Loss:4360305.0000\n",
      "Epoch [155/200], Step[11/17] Loss:1577321.7500\n",
      "Epoch [155/200], Step[12/17] Loss:1883798.0000\n",
      "Epoch [155/200], Step[13/17] Loss:1105391.0000\n",
      "Epoch [155/200], Step[14/17] Loss:799420.6250\n",
      "Epoch [155/200], Step[15/17] Loss:7516907.5000\n",
      "Epoch [155/200], Step[16/17] Loss:4623691.5000\n",
      "Epoch [155/200], Step[17/17] Loss:1093509.1250\n",
      "Epoch [156/200], Step[1/17] Loss:3347391.5000\n",
      "Epoch [156/200], Step[2/17] Loss:3149168.5000\n",
      "Epoch [156/200], Step[3/17] Loss:1261814.8750\n",
      "Epoch [156/200], Step[4/17] Loss:3770065.5000\n",
      "Epoch [156/200], Step[5/17] Loss:1219773.7500\n",
      "Epoch [156/200], Step[6/17] Loss:795072.1875\n",
      "Epoch [156/200], Step[7/17] Loss:1379035.8750\n",
      "Epoch [156/200], Step[8/17] Loss:1791293.3750\n",
      "Epoch [156/200], Step[9/17] Loss:1453226.5000\n",
      "Epoch [156/200], Step[10/17] Loss:1654958.8750\n",
      "Epoch [156/200], Step[11/17] Loss:7388286.5000\n",
      "Epoch [156/200], Step[12/17] Loss:1529904.1250\n",
      "Epoch [156/200], Step[13/17] Loss:1656398.5000\n",
      "Epoch [156/200], Step[14/17] Loss:999748.1875\n",
      "Epoch [156/200], Step[15/17] Loss:994448.1875\n",
      "Epoch [156/200], Step[16/17] Loss:4411997.0000\n",
      "Epoch [156/200], Step[17/17] Loss:3544242.5000\n",
      "Epoch [157/200], Step[1/17] Loss:2312816.2500\n",
      "Epoch [157/200], Step[2/17] Loss:4390503.5000\n",
      "Epoch [157/200], Step[3/17] Loss:1185936.6250\n",
      "Epoch [157/200], Step[4/17] Loss:1095283.0000\n",
      "Epoch [157/200], Step[5/17] Loss:965944.0000\n",
      "Epoch [157/200], Step[6/17] Loss:2994940.2500\n",
      "Epoch [157/200], Step[7/17] Loss:2183930.7500\n",
      "Epoch [157/200], Step[8/17] Loss:1352267.6250\n",
      "Epoch [157/200], Step[9/17] Loss:2294777.7500\n",
      "Epoch [157/200], Step[10/17] Loss:7872312.0000\n",
      "Epoch [157/200], Step[11/17] Loss:4200659.5000\n",
      "Epoch [157/200], Step[12/17] Loss:2277686.0000\n",
      "Epoch [157/200], Step[13/17] Loss:617042.6250\n",
      "Epoch [157/200], Step[14/17] Loss:1069982.1250\n",
      "Epoch [157/200], Step[15/17] Loss:1273335.2500\n",
      "Epoch [157/200], Step[16/17] Loss:2280796.7500\n",
      "Epoch [157/200], Step[17/17] Loss:1617312.0000\n",
      "Epoch [158/200], Step[1/17] Loss:2196849.0000\n",
      "Epoch [158/200], Step[2/17] Loss:1347892.8750\n",
      "Epoch [158/200], Step[3/17] Loss:1105654.8750\n",
      "Epoch [158/200], Step[4/17] Loss:1844168.1250\n",
      "Epoch [158/200], Step[5/17] Loss:1251632.5000\n",
      "Epoch [158/200], Step[6/17] Loss:1072423.1250\n",
      "Epoch [158/200], Step[7/17] Loss:2134632.5000\n",
      "Epoch [158/200], Step[8/17] Loss:4054491.7500\n",
      "Epoch [158/200], Step[9/17] Loss:2719014.2500\n",
      "Epoch [158/200], Step[10/17] Loss:658214.8750\n",
      "Epoch [158/200], Step[11/17] Loss:7268910.5000\n",
      "Epoch [158/200], Step[12/17] Loss:4119950.5000\n",
      "Epoch [158/200], Step[13/17] Loss:841358.0000\n",
      "Epoch [158/200], Step[14/17] Loss:4193185.5000\n",
      "Epoch [158/200], Step[15/17] Loss:1104002.1250\n",
      "Epoch [158/200], Step[16/17] Loss:1485684.5000\n",
      "Epoch [158/200], Step[17/17] Loss:2811341.5000\n",
      "Epoch [159/200], Step[1/17] Loss:1517307.8750\n",
      "Epoch [159/200], Step[2/17] Loss:1938976.1250\n",
      "Epoch [159/200], Step[3/17] Loss:5534098.0000\n",
      "Epoch [159/200], Step[4/17] Loss:1265526.5000\n",
      "Epoch [159/200], Step[5/17] Loss:726028.1250\n",
      "Epoch [159/200], Step[6/17] Loss:937303.5625\n",
      "Epoch [159/200], Step[7/17] Loss:3757048.7500\n",
      "Epoch [159/200], Step[8/17] Loss:1009777.6875\n",
      "Epoch [159/200], Step[9/17] Loss:1864640.3750\n",
      "Epoch [159/200], Step[10/17] Loss:1253639.0000\n",
      "Epoch [159/200], Step[11/17] Loss:2101952.2500\n",
      "Epoch [159/200], Step[12/17] Loss:4042336.7500\n",
      "Epoch [159/200], Step[13/17] Loss:1725223.8750\n",
      "Epoch [159/200], Step[14/17] Loss:7320300.5000\n",
      "Epoch [159/200], Step[15/17] Loss:2012929.1250\n",
      "Epoch [159/200], Step[16/17] Loss:1071684.8750\n",
      "Epoch [159/200], Step[17/17] Loss:1973546.1250\n",
      "Epoch [160/200], Step[1/17] Loss:2119884.0000\n",
      "Epoch [160/200], Step[2/17] Loss:2280352.0000\n",
      "Epoch [160/200], Step[3/17] Loss:3959655.2500\n",
      "Epoch [160/200], Step[4/17] Loss:1227253.1250\n",
      "Epoch [160/200], Step[5/17] Loss:1308885.6250\n",
      "Epoch [160/200], Step[6/17] Loss:1718257.8750\n",
      "Epoch [160/200], Step[7/17] Loss:844662.8125\n",
      "Epoch [160/200], Step[8/17] Loss:3318643.5000\n",
      "Epoch [160/200], Step[9/17] Loss:2211517.7500\n",
      "Epoch [160/200], Step[10/17] Loss:2708308.2500\n",
      "Epoch [160/200], Step[11/17] Loss:7781129.0000\n",
      "Epoch [160/200], Step[12/17] Loss:1334227.5000\n",
      "Epoch [160/200], Step[13/17] Loss:537458.6875\n",
      "Epoch [160/200], Step[14/17] Loss:3709574.0000\n",
      "Epoch [160/200], Step[15/17] Loss:952029.3750\n",
      "Epoch [160/200], Step[16/17] Loss:768724.0000\n",
      "Epoch [160/200], Step[17/17] Loss:3571344.7500\n",
      "Epoch [161/200], Step[1/17] Loss:962393.0000\n",
      "Epoch [161/200], Step[2/17] Loss:688901.9375\n",
      "Epoch [161/200], Step[3/17] Loss:2764874.5000\n",
      "Epoch [161/200], Step[4/17] Loss:1141223.3750\n",
      "Epoch [161/200], Step[5/17] Loss:1319641.8750\n",
      "Epoch [161/200], Step[6/17] Loss:1450059.7500\n",
      "Epoch [161/200], Step[7/17] Loss:7873265.0000\n",
      "Epoch [161/200], Step[8/17] Loss:3301511.5000\n",
      "Epoch [161/200], Step[9/17] Loss:1433591.6250\n",
      "Epoch [161/200], Step[10/17] Loss:3488500.5000\n",
      "Epoch [161/200], Step[11/17] Loss:1551633.7500\n",
      "Epoch [161/200], Step[12/17] Loss:2233633.2500\n",
      "Epoch [161/200], Step[13/17] Loss:4102786.2500\n",
      "Epoch [161/200], Step[14/17] Loss:2360630.5000\n",
      "Epoch [161/200], Step[15/17] Loss:1175662.7500\n",
      "Epoch [161/200], Step[16/17] Loss:2775616.0000\n",
      "Epoch [161/200], Step[17/17] Loss:1302590.3750\n",
      "Epoch [162/200], Step[1/17] Loss:3628652.7500\n",
      "Epoch [162/200], Step[2/17] Loss:1849084.3750\n",
      "Epoch [162/200], Step[3/17] Loss:3160609.2500\n",
      "Epoch [162/200], Step[4/17] Loss:940497.2500\n",
      "Epoch [162/200], Step[5/17] Loss:2393952.2500\n",
      "Epoch [162/200], Step[6/17] Loss:797033.5625\n",
      "Epoch [162/200], Step[7/17] Loss:968844.8125\n",
      "Epoch [162/200], Step[8/17] Loss:2338918.2500\n",
      "Epoch [162/200], Step[9/17] Loss:2738664.2500\n",
      "Epoch [162/200], Step[10/17] Loss:9761077.0000\n",
      "Epoch [162/200], Step[11/17] Loss:1536728.7500\n",
      "Epoch [162/200], Step[12/17] Loss:1840535.8750\n",
      "Epoch [162/200], Step[13/17] Loss:1300569.8750\n",
      "Epoch [162/200], Step[14/17] Loss:1608487.7500\n",
      "Epoch [162/200], Step[15/17] Loss:2106423.7500\n",
      "Epoch [162/200], Step[16/17] Loss:1441045.3750\n",
      "Epoch [162/200], Step[17/17] Loss:1564499.3750\n",
      "Epoch [163/200], Step[1/17] Loss:1246912.0000\n",
      "Epoch [163/200], Step[2/17] Loss:1743975.6250\n",
      "Epoch [163/200], Step[3/17] Loss:2482463.2500\n",
      "Epoch [163/200], Step[4/17] Loss:3232793.2500\n",
      "Epoch [163/200], Step[5/17] Loss:1081291.0000\n",
      "Epoch [163/200], Step[6/17] Loss:7172815.0000\n",
      "Epoch [163/200], Step[7/17] Loss:1256183.5000\n",
      "Epoch [163/200], Step[8/17] Loss:707833.6250\n",
      "Epoch [163/200], Step[9/17] Loss:3259636.0000\n",
      "Epoch [163/200], Step[10/17] Loss:698322.1875\n",
      "Epoch [163/200], Step[11/17] Loss:2497737.7500\n",
      "Epoch [163/200], Step[12/17] Loss:1179152.1250\n",
      "Epoch [163/200], Step[13/17] Loss:1696860.3750\n",
      "Epoch [163/200], Step[14/17] Loss:5393552.5000\n",
      "Epoch [163/200], Step[15/17] Loss:3370060.0000\n",
      "Epoch [163/200], Step[16/17] Loss:943799.0625\n",
      "Epoch [163/200], Step[17/17] Loss:2115561.2500\n",
      "Epoch [164/200], Step[1/17] Loss:2471023.2500\n",
      "Epoch [164/200], Step[2/17] Loss:2803260.0000\n",
      "Epoch [164/200], Step[3/17] Loss:1804229.7500\n",
      "Epoch [164/200], Step[4/17] Loss:786941.4375\n",
      "Epoch [164/200], Step[5/17] Loss:1269395.3750\n",
      "Epoch [164/200], Step[6/17] Loss:3617330.7500\n",
      "Epoch [164/200], Step[7/17] Loss:982028.2500\n",
      "Epoch [164/200], Step[8/17] Loss:2171890.2500\n",
      "Epoch [164/200], Step[9/17] Loss:1077297.7500\n",
      "Epoch [164/200], Step[10/17] Loss:2785153.7500\n",
      "Epoch [164/200], Step[11/17] Loss:870940.3125\n",
      "Epoch [164/200], Step[12/17] Loss:1740678.1250\n",
      "Epoch [164/200], Step[13/17] Loss:1463625.3750\n",
      "Epoch [164/200], Step[14/17] Loss:2232755.7500\n",
      "Epoch [164/200], Step[15/17] Loss:1164353.7500\n",
      "Epoch [164/200], Step[16/17] Loss:9138755.0000\n",
      "Epoch [164/200], Step[17/17] Loss:4064765.2500\n",
      "Epoch [165/200], Step[1/17] Loss:1316052.2500\n",
      "Epoch [165/200], Step[2/17] Loss:974515.3125\n",
      "Epoch [165/200], Step[3/17] Loss:1983851.7500\n",
      "Epoch [165/200], Step[4/17] Loss:3007928.0000\n",
      "Epoch [165/200], Step[5/17] Loss:2063960.0000\n",
      "Epoch [165/200], Step[6/17] Loss:1365933.3750\n",
      "Epoch [165/200], Step[7/17] Loss:4980453.5000\n",
      "Epoch [165/200], Step[8/17] Loss:1824400.1250\n",
      "Epoch [165/200], Step[9/17] Loss:1235955.6250\n",
      "Epoch [165/200], Step[10/17] Loss:744475.4375\n",
      "Epoch [165/200], Step[11/17] Loss:926401.1250\n",
      "Epoch [165/200], Step[12/17] Loss:1053838.0000\n",
      "Epoch [165/200], Step[13/17] Loss:3875727.5000\n",
      "Epoch [165/200], Step[14/17] Loss:1537167.5000\n",
      "Epoch [165/200], Step[15/17] Loss:3730763.0000\n",
      "Epoch [165/200], Step[16/17] Loss:7752197.0000\n",
      "Epoch [165/200], Step[17/17] Loss:1610658.6250\n",
      "Epoch [166/200], Step[1/17] Loss:694153.7500\n",
      "Epoch [166/200], Step[2/17] Loss:1536186.7500\n",
      "Epoch [166/200], Step[3/17] Loss:1462864.0000\n",
      "Epoch [166/200], Step[4/17] Loss:810003.6875\n",
      "Epoch [166/200], Step[5/17] Loss:2489683.7500\n",
      "Epoch [166/200], Step[6/17] Loss:1648933.5000\n",
      "Epoch [166/200], Step[7/17] Loss:645046.8750\n",
      "Epoch [166/200], Step[8/17] Loss:7751067.0000\n",
      "Epoch [166/200], Step[9/17] Loss:3134853.2500\n",
      "Epoch [166/200], Step[10/17] Loss:2924513.7500\n",
      "Epoch [166/200], Step[11/17] Loss:2089357.5000\n",
      "Epoch [166/200], Step[12/17] Loss:1641826.6250\n",
      "Epoch [166/200], Step[13/17] Loss:3655714.2500\n",
      "Epoch [166/200], Step[14/17] Loss:2253230.2500\n",
      "Epoch [166/200], Step[15/17] Loss:2829737.2500\n",
      "Epoch [166/200], Step[16/17] Loss:2111471.2500\n",
      "Epoch [166/200], Step[17/17] Loss:2466014.2500\n",
      "Epoch [167/200], Step[1/17] Loss:819241.8750\n",
      "Epoch [167/200], Step[2/17] Loss:2876646.5000\n",
      "Epoch [167/200], Step[3/17] Loss:3513714.0000\n",
      "Epoch [167/200], Step[4/17] Loss:2020859.6250\n",
      "Epoch [167/200], Step[5/17] Loss:939969.7500\n",
      "Epoch [167/200], Step[6/17] Loss:2884080.2500\n",
      "Epoch [167/200], Step[7/17] Loss:1932271.7500\n",
      "Epoch [167/200], Step[8/17] Loss:821857.6875\n",
      "Epoch [167/200], Step[9/17] Loss:4259260.5000\n",
      "Epoch [167/200], Step[10/17] Loss:2440845.0000\n",
      "Epoch [167/200], Step[11/17] Loss:2280493.2500\n",
      "Epoch [167/200], Step[12/17] Loss:7781768.0000\n",
      "Epoch [167/200], Step[13/17] Loss:2109329.2500\n",
      "Epoch [167/200], Step[14/17] Loss:2426810.7500\n",
      "Epoch [167/200], Step[15/17] Loss:661442.3750\n",
      "Epoch [167/200], Step[16/17] Loss:1237486.1250\n",
      "Epoch [167/200], Step[17/17] Loss:832250.3750\n",
      "Epoch [168/200], Step[1/17] Loss:2813712.7500\n",
      "Epoch [168/200], Step[2/17] Loss:1371028.6250\n",
      "Epoch [168/200], Step[3/17] Loss:1809965.1250\n",
      "Epoch [168/200], Step[4/17] Loss:826725.5000\n",
      "Epoch [168/200], Step[5/17] Loss:1040161.7500\n",
      "Epoch [168/200], Step[6/17] Loss:7447686.0000\n",
      "Epoch [168/200], Step[7/17] Loss:1589465.8750\n",
      "Epoch [168/200], Step[8/17] Loss:1297917.5000\n",
      "Epoch [168/200], Step[9/17] Loss:3906845.7500\n",
      "Epoch [168/200], Step[10/17] Loss:926156.4375\n",
      "Epoch [168/200], Step[11/17] Loss:1868965.7500\n",
      "Epoch [168/200], Step[12/17] Loss:2802833.2500\n",
      "Epoch [168/200], Step[13/17] Loss:2608984.5000\n",
      "Epoch [168/200], Step[14/17] Loss:2558099.5000\n",
      "Epoch [168/200], Step[15/17] Loss:713841.1875\n",
      "Epoch [168/200], Step[16/17] Loss:3029960.7500\n",
      "Epoch [168/200], Step[17/17] Loss:3778375.7500\n",
      "Epoch [169/200], Step[1/17] Loss:4900284.0000\n",
      "Epoch [169/200], Step[2/17] Loss:1589252.1250\n",
      "Epoch [169/200], Step[3/17] Loss:812062.1250\n",
      "Epoch [169/200], Step[4/17] Loss:2929116.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [169/200], Step[5/17] Loss:1191249.2500\n",
      "Epoch [169/200], Step[6/17] Loss:2732758.5000\n",
      "Epoch [169/200], Step[7/17] Loss:1066322.5000\n",
      "Epoch [169/200], Step[8/17] Loss:2145864.5000\n",
      "Epoch [169/200], Step[9/17] Loss:1346766.6250\n",
      "Epoch [169/200], Step[10/17] Loss:3196315.2500\n",
      "Epoch [169/200], Step[11/17] Loss:2413125.0000\n",
      "Epoch [169/200], Step[12/17] Loss:1270343.7500\n",
      "Epoch [169/200], Step[13/17] Loss:1201898.1250\n",
      "Epoch [169/200], Step[14/17] Loss:2713942.5000\n",
      "Epoch [169/200], Step[15/17] Loss:564857.0000\n",
      "Epoch [169/200], Step[16/17] Loss:2242259.2500\n",
      "Epoch [169/200], Step[17/17] Loss:9065678.0000\n",
      "Epoch [170/200], Step[1/17] Loss:1830550.2500\n",
      "Epoch [170/200], Step[2/17] Loss:603070.3125\n",
      "Epoch [170/200], Step[3/17] Loss:1829611.1250\n",
      "Epoch [170/200], Step[4/17] Loss:6329718.0000\n",
      "Epoch [170/200], Step[5/17] Loss:1337368.0000\n",
      "Epoch [170/200], Step[6/17] Loss:2642572.7500\n",
      "Epoch [170/200], Step[7/17] Loss:3437496.0000\n",
      "Epoch [170/200], Step[8/17] Loss:1033496.4375\n",
      "Epoch [170/200], Step[9/17] Loss:8097047.5000\n",
      "Epoch [170/200], Step[10/17] Loss:1098620.1250\n",
      "Epoch [170/200], Step[11/17] Loss:1981555.0000\n",
      "Epoch [170/200], Step[12/17] Loss:3314023.0000\n",
      "Epoch [170/200], Step[13/17] Loss:2050119.7500\n",
      "Epoch [170/200], Step[14/17] Loss:826355.1250\n",
      "Epoch [170/200], Step[15/17] Loss:1623637.5000\n",
      "Epoch [170/200], Step[16/17] Loss:536269.8125\n",
      "Epoch [170/200], Step[17/17] Loss:1367101.8750\n",
      "Epoch [171/200], Step[1/17] Loss:8029725.0000\n",
      "Epoch [171/200], Step[2/17] Loss:3428208.0000\n",
      "Epoch [171/200], Step[3/17] Loss:1683268.0000\n",
      "Epoch [171/200], Step[4/17] Loss:1487646.3750\n",
      "Epoch [171/200], Step[5/17] Loss:1057704.7500\n",
      "Epoch [171/200], Step[6/17] Loss:3068264.5000\n",
      "Epoch [171/200], Step[7/17] Loss:2966143.0000\n",
      "Epoch [171/200], Step[8/17] Loss:3039890.5000\n",
      "Epoch [171/200], Step[9/17] Loss:1615369.5000\n",
      "Epoch [171/200], Step[10/17] Loss:3465340.2500\n",
      "Epoch [171/200], Step[11/17] Loss:1211456.2500\n",
      "Epoch [171/200], Step[12/17] Loss:568117.2500\n",
      "Epoch [171/200], Step[13/17] Loss:2676638.7500\n",
      "Epoch [171/200], Step[14/17] Loss:1268194.6250\n",
      "Epoch [171/200], Step[15/17] Loss:1099482.5000\n",
      "Epoch [171/200], Step[16/17] Loss:2171469.2500\n",
      "Epoch [171/200], Step[17/17] Loss:1040445.6250\n",
      "Epoch [172/200], Step[1/17] Loss:8689454.0000\n",
      "Epoch [172/200], Step[2/17] Loss:1257351.5000\n",
      "Epoch [172/200], Step[3/17] Loss:3789513.5000\n",
      "Epoch [172/200], Step[4/17] Loss:3631732.5000\n",
      "Epoch [172/200], Step[5/17] Loss:1319922.8750\n",
      "Epoch [172/200], Step[6/17] Loss:993050.4375\n",
      "Epoch [172/200], Step[7/17] Loss:1770784.3750\n",
      "Epoch [172/200], Step[8/17] Loss:2207357.7500\n",
      "Epoch [172/200], Step[9/17] Loss:1979782.5000\n",
      "Epoch [172/200], Step[10/17] Loss:1554968.5000\n",
      "Epoch [172/200], Step[11/17] Loss:1625174.6250\n",
      "Epoch [172/200], Step[12/17] Loss:1212602.3750\n",
      "Epoch [172/200], Step[13/17] Loss:2437507.0000\n",
      "Epoch [172/200], Step[14/17] Loss:813288.6250\n",
      "Epoch [172/200], Step[15/17] Loss:3381611.5000\n",
      "Epoch [172/200], Step[16/17] Loss:1615522.8750\n",
      "Epoch [172/200], Step[17/17] Loss:1726345.6250\n",
      "Epoch [173/200], Step[1/17] Loss:895798.6250\n",
      "Epoch [173/200], Step[2/17] Loss:1178470.3750\n",
      "Epoch [173/200], Step[3/17] Loss:3017757.5000\n",
      "Epoch [173/200], Step[4/17] Loss:3828337.2500\n",
      "Epoch [173/200], Step[5/17] Loss:7013095.0000\n",
      "Epoch [173/200], Step[6/17] Loss:2148067.5000\n",
      "Epoch [173/200], Step[7/17] Loss:909821.0625\n",
      "Epoch [173/200], Step[8/17] Loss:3543377.0000\n",
      "Epoch [173/200], Step[9/17] Loss:2627820.2500\n",
      "Epoch [173/200], Step[10/17] Loss:848184.8125\n",
      "Epoch [173/200], Step[11/17] Loss:1735092.5000\n",
      "Epoch [173/200], Step[12/17] Loss:1595581.0000\n",
      "Epoch [173/200], Step[13/17] Loss:1204978.0000\n",
      "Epoch [173/200], Step[14/17] Loss:1545146.2500\n",
      "Epoch [173/200], Step[15/17] Loss:5477934.0000\n",
      "Epoch [173/200], Step[16/17] Loss:1717460.7500\n",
      "Epoch [173/200], Step[17/17] Loss:486595.7188\n",
      "Epoch [174/200], Step[1/17] Loss:1324241.8750\n",
      "Epoch [174/200], Step[2/17] Loss:897858.6250\n",
      "Epoch [174/200], Step[3/17] Loss:961075.6250\n",
      "Epoch [174/200], Step[4/17] Loss:813363.0000\n",
      "Epoch [174/200], Step[5/17] Loss:7882256.0000\n",
      "Epoch [174/200], Step[6/17] Loss:1195507.5000\n",
      "Epoch [174/200], Step[7/17] Loss:2755632.0000\n",
      "Epoch [174/200], Step[8/17] Loss:1088239.8750\n",
      "Epoch [174/200], Step[9/17] Loss:3067633.7500\n",
      "Epoch [174/200], Step[10/17] Loss:766456.6875\n",
      "Epoch [174/200], Step[11/17] Loss:1289411.0000\n",
      "Epoch [174/200], Step[12/17] Loss:779558.3125\n",
      "Epoch [174/200], Step[13/17] Loss:3671691.0000\n",
      "Epoch [174/200], Step[14/17] Loss:1738895.3750\n",
      "Epoch [174/200], Step[15/17] Loss:5051092.0000\n",
      "Epoch [174/200], Step[16/17] Loss:722980.2500\n",
      "Epoch [174/200], Step[17/17] Loss:6986322.0000\n",
      "Epoch [175/200], Step[1/17] Loss:1437771.7500\n",
      "Epoch [175/200], Step[2/17] Loss:1818066.6250\n",
      "Epoch [175/200], Step[3/17] Loss:1653956.3750\n",
      "Epoch [175/200], Step[4/17] Loss:960109.5000\n",
      "Epoch [175/200], Step[5/17] Loss:3236621.2500\n",
      "Epoch [175/200], Step[6/17] Loss:2867922.5000\n",
      "Epoch [175/200], Step[7/17] Loss:1752037.8750\n",
      "Epoch [175/200], Step[8/17] Loss:7115195.5000\n",
      "Epoch [175/200], Step[9/17] Loss:4719923.0000\n",
      "Epoch [175/200], Step[10/17] Loss:2055786.3750\n",
      "Epoch [175/200], Step[11/17] Loss:1521785.6250\n",
      "Epoch [175/200], Step[12/17] Loss:2155222.2500\n",
      "Epoch [175/200], Step[13/17] Loss:1072412.0000\n",
      "Epoch [175/200], Step[14/17] Loss:1493511.5000\n",
      "Epoch [175/200], Step[15/17] Loss:2433625.0000\n",
      "Epoch [175/200], Step[16/17] Loss:2096109.8750\n",
      "Epoch [175/200], Step[17/17] Loss:1590428.1250\n",
      "Epoch [176/200], Step[1/17] Loss:735181.1250\n",
      "Epoch [176/200], Step[2/17] Loss:918582.8125\n",
      "Epoch [176/200], Step[3/17] Loss:1050425.0000\n",
      "Epoch [176/200], Step[4/17] Loss:1740470.8750\n",
      "Epoch [176/200], Step[5/17] Loss:1731732.2500\n",
      "Epoch [176/200], Step[6/17] Loss:1120638.7500\n",
      "Epoch [176/200], Step[7/17] Loss:968039.3750\n",
      "Epoch [176/200], Step[8/17] Loss:10377534.0000\n",
      "Epoch [176/200], Step[9/17] Loss:2101794.5000\n",
      "Epoch [176/200], Step[10/17] Loss:2195317.0000\n",
      "Epoch [176/200], Step[11/17] Loss:4150152.0000\n",
      "Epoch [176/200], Step[12/17] Loss:1391997.0000\n",
      "Epoch [176/200], Step[13/17] Loss:2031505.8750\n",
      "Epoch [176/200], Step[14/17] Loss:6033939.0000\n",
      "Epoch [176/200], Step[15/17] Loss:631168.6250\n",
      "Epoch [176/200], Step[16/17] Loss:1640912.8750\n",
      "Epoch [176/200], Step[17/17] Loss:1062018.1250\n",
      "Epoch [177/200], Step[1/17] Loss:865689.7500\n",
      "Epoch [177/200], Step[2/17] Loss:1229221.1250\n",
      "Epoch [177/200], Step[3/17] Loss:1681081.0000\n",
      "Epoch [177/200], Step[4/17] Loss:1393151.1250\n",
      "Epoch [177/200], Step[5/17] Loss:1173111.7500\n",
      "Epoch [177/200], Step[6/17] Loss:3943407.7500\n",
      "Epoch [177/200], Step[7/17] Loss:879883.0625\n",
      "Epoch [177/200], Step[8/17] Loss:9517347.0000\n",
      "Epoch [177/200], Step[9/17] Loss:1245035.1250\n",
      "Epoch [177/200], Step[10/17] Loss:3527411.2500\n",
      "Epoch [177/200], Step[11/17] Loss:4727143.0000\n",
      "Epoch [177/200], Step[12/17] Loss:1946073.6250\n",
      "Epoch [177/200], Step[13/17] Loss:592486.5000\n",
      "Epoch [177/200], Step[14/17] Loss:1938608.6250\n",
      "Epoch [177/200], Step[15/17] Loss:2515675.2500\n",
      "Epoch [177/200], Step[16/17] Loss:1498889.2500\n",
      "Epoch [177/200], Step[17/17] Loss:1240695.6250\n",
      "Epoch [178/200], Step[1/17] Loss:995438.0000\n",
      "Epoch [178/200], Step[2/17] Loss:1572011.0000\n",
      "Epoch [178/200], Step[3/17] Loss:3057702.7500\n",
      "Epoch [178/200], Step[4/17] Loss:1075121.6250\n",
      "Epoch [178/200], Step[5/17] Loss:980059.1250\n",
      "Epoch [178/200], Step[6/17] Loss:1300949.3750\n",
      "Epoch [178/200], Step[7/17] Loss:674358.2500\n",
      "Epoch [178/200], Step[8/17] Loss:1499336.2500\n",
      "Epoch [178/200], Step[9/17] Loss:1876240.3750\n",
      "Epoch [178/200], Step[10/17] Loss:1358282.5000\n",
      "Epoch [178/200], Step[11/17] Loss:1651858.1250\n",
      "Epoch [178/200], Step[12/17] Loss:4474726.5000\n",
      "Epoch [178/200], Step[13/17] Loss:10214770.0000\n",
      "Epoch [178/200], Step[14/17] Loss:2718242.2500\n",
      "Epoch [178/200], Step[15/17] Loss:2681208.7500\n",
      "Epoch [178/200], Step[16/17] Loss:1505974.8750\n",
      "Epoch [178/200], Step[17/17] Loss:2518155.5000\n",
      "Epoch [179/200], Step[1/17] Loss:1114558.6250\n",
      "Epoch [179/200], Step[2/17] Loss:1099755.1250\n",
      "Epoch [179/200], Step[3/17] Loss:2877037.7500\n",
      "Epoch [179/200], Step[4/17] Loss:1081255.8750\n",
      "Epoch [179/200], Step[5/17] Loss:4433067.5000\n",
      "Epoch [179/200], Step[6/17] Loss:2300145.7500\n",
      "Epoch [179/200], Step[7/17] Loss:2196479.2500\n",
      "Epoch [179/200], Step[8/17] Loss:2741664.0000\n",
      "Epoch [179/200], Step[9/17] Loss:1858222.1250\n",
      "Epoch [179/200], Step[10/17] Loss:1297074.6250\n",
      "Epoch [179/200], Step[11/17] Loss:1268868.2500\n",
      "Epoch [179/200], Step[12/17] Loss:1748608.8750\n",
      "Epoch [179/200], Step[13/17] Loss:9463874.0000\n",
      "Epoch [179/200], Step[14/17] Loss:1107256.3750\n",
      "Epoch [179/200], Step[15/17] Loss:1169449.0000\n",
      "Epoch [179/200], Step[16/17] Loss:2945749.7500\n",
      "Epoch [179/200], Step[17/17] Loss:1205186.7500\n",
      "Epoch [180/200], Step[1/17] Loss:7077383.5000\n",
      "Epoch [180/200], Step[2/17] Loss:1150454.5000\n",
      "Epoch [180/200], Step[3/17] Loss:3981257.5000\n",
      "Epoch [180/200], Step[4/17] Loss:1868617.5000\n",
      "Epoch [180/200], Step[5/17] Loss:1517009.8750\n",
      "Epoch [180/200], Step[6/17] Loss:1574943.5000\n",
      "Epoch [180/200], Step[7/17] Loss:1320278.3750\n",
      "Epoch [180/200], Step[8/17] Loss:2566412.7500\n",
      "Epoch [180/200], Step[9/17] Loss:1405879.7500\n",
      "Epoch [180/200], Step[10/17] Loss:954736.4375\n",
      "Epoch [180/200], Step[11/17] Loss:1026639.2500\n",
      "Epoch [180/200], Step[12/17] Loss:3375769.7500\n",
      "Epoch [180/200], Step[13/17] Loss:1258682.8750\n",
      "Epoch [180/200], Step[14/17] Loss:3223349.2500\n",
      "Epoch [180/200], Step[15/17] Loss:3845748.0000\n",
      "Epoch [180/200], Step[16/17] Loss:1684525.5000\n",
      "Epoch [180/200], Step[17/17] Loss:2277651.2500\n",
      "Epoch [181/200], Step[1/17] Loss:1749655.8750\n",
      "Epoch [181/200], Step[2/17] Loss:1033897.4375\n",
      "Epoch [181/200], Step[3/17] Loss:1895720.8750\n",
      "Epoch [181/200], Step[4/17] Loss:1198337.3750\n",
      "Epoch [181/200], Step[5/17] Loss:629950.6250\n",
      "Epoch [181/200], Step[6/17] Loss:3085352.5000\n",
      "Epoch [181/200], Step[7/17] Loss:1784916.6250\n",
      "Epoch [181/200], Step[8/17] Loss:2179990.5000\n",
      "Epoch [181/200], Step[9/17] Loss:8006042.5000\n",
      "Epoch [181/200], Step[10/17] Loss:3549799.0000\n",
      "Epoch [181/200], Step[11/17] Loss:1153670.2500\n",
      "Epoch [181/200], Step[12/17] Loss:2273413.5000\n",
      "Epoch [181/200], Step[13/17] Loss:4004345.5000\n",
      "Epoch [181/200], Step[14/17] Loss:1259568.7500\n",
      "Epoch [181/200], Step[15/17] Loss:1248387.8750\n",
      "Epoch [181/200], Step[16/17] Loss:3407311.2500\n",
      "Epoch [181/200], Step[17/17] Loss:1503900.6250\n",
      "Epoch [182/200], Step[1/17] Loss:1439004.6250\n",
      "Epoch [182/200], Step[2/17] Loss:950411.8125\n",
      "Epoch [182/200], Step[3/17] Loss:1291166.7500\n",
      "Epoch [182/200], Step[4/17] Loss:1787462.2500\n",
      "Epoch [182/200], Step[5/17] Loss:1177414.0000\n",
      "Epoch [182/200], Step[6/17] Loss:1296238.0000\n",
      "Epoch [182/200], Step[7/17] Loss:1325318.7500\n",
      "Epoch [182/200], Step[8/17] Loss:3093130.7500\n",
      "Epoch [182/200], Step[9/17] Loss:5750726.5000\n",
      "Epoch [182/200], Step[10/17] Loss:1454524.2500\n",
      "Epoch [182/200], Step[11/17] Loss:1636225.8750\n",
      "Epoch [182/200], Step[12/17] Loss:1327668.0000\n",
      "Epoch [182/200], Step[13/17] Loss:2146134.5000\n",
      "Epoch [182/200], Step[14/17] Loss:713005.6875\n",
      "Epoch [182/200], Step[15/17] Loss:10899974.0000\n",
      "Epoch [182/200], Step[16/17] Loss:2972937.0000\n",
      "Epoch [182/200], Step[17/17] Loss:518078.3438\n",
      "Epoch [183/200], Step[1/17] Loss:2883431.2500\n",
      "Epoch [183/200], Step[2/17] Loss:571375.0000\n",
      "Epoch [183/200], Step[3/17] Loss:1131331.1250\n",
      "Epoch [183/200], Step[4/17] Loss:1108794.3750\n",
      "Epoch [183/200], Step[5/17] Loss:2828937.7500\n",
      "Epoch [183/200], Step[6/17] Loss:1534915.5000\n",
      "Epoch [183/200], Step[7/17] Loss:1119347.5000\n",
      "Epoch [183/200], Step[8/17] Loss:976954.4375\n",
      "Epoch [183/200], Step[9/17] Loss:2279560.7500\n",
      "Epoch [183/200], Step[10/17] Loss:3886099.5000\n",
      "Epoch [183/200], Step[11/17] Loss:3285317.0000\n",
      "Epoch [183/200], Step[12/17] Loss:1041203.0625\n",
      "Epoch [183/200], Step[13/17] Loss:7449666.0000\n",
      "Epoch [183/200], Step[14/17] Loss:2025564.3750\n",
      "Epoch [183/200], Step[15/17] Loss:1490795.1250\n",
      "Epoch [183/200], Step[16/17] Loss:1493600.1250\n",
      "Epoch [183/200], Step[17/17] Loss:5631246.5000\n",
      "Epoch [184/200], Step[1/17] Loss:756223.1875\n",
      "Epoch [184/200], Step[2/17] Loss:2323164.5000\n",
      "Epoch [184/200], Step[3/17] Loss:445960.1562\n",
      "Epoch [184/200], Step[4/17] Loss:2135307.0000\n",
      "Epoch [184/200], Step[5/17] Loss:790462.2500\n",
      "Epoch [184/200], Step[6/17] Loss:1006909.1250\n",
      "Epoch [184/200], Step[7/17] Loss:1775585.5000\n",
      "Epoch [184/200], Step[8/17] Loss:1804605.5000\n",
      "Epoch [184/200], Step[9/17] Loss:1341352.2500\n",
      "Epoch [184/200], Step[10/17] Loss:3957260.2500\n",
      "Epoch [184/200], Step[11/17] Loss:1910010.0000\n",
      "Epoch [184/200], Step[12/17] Loss:1938760.5000\n",
      "Epoch [184/200], Step[13/17] Loss:8802701.0000\n",
      "Epoch [184/200], Step[14/17] Loss:1606716.8750\n",
      "Epoch [184/200], Step[15/17] Loss:3887716.5000\n",
      "Epoch [184/200], Step[16/17] Loss:2531179.0000\n",
      "Epoch [184/200], Step[17/17] Loss:3284144.2500\n",
      "Epoch [185/200], Step[1/17] Loss:2692286.5000\n",
      "Epoch [185/200], Step[2/17] Loss:1353084.0000\n",
      "Epoch [185/200], Step[3/17] Loss:2805142.2500\n",
      "Epoch [185/200], Step[4/17] Loss:1014776.8125\n",
      "Epoch [185/200], Step[5/17] Loss:1374654.8750\n",
      "Epoch [185/200], Step[6/17] Loss:1344899.1250\n",
      "Epoch [185/200], Step[7/17] Loss:1071170.6250\n",
      "Epoch [185/200], Step[8/17] Loss:1211492.1250\n",
      "Epoch [185/200], Step[9/17] Loss:1519699.3750\n",
      "Epoch [185/200], Step[10/17] Loss:2210485.0000\n",
      "Epoch [185/200], Step[11/17] Loss:961677.7500\n",
      "Epoch [185/200], Step[12/17] Loss:3665826.5000\n",
      "Epoch [185/200], Step[13/17] Loss:2325981.0000\n",
      "Epoch [185/200], Step[14/17] Loss:10459904.0000\n",
      "Epoch [185/200], Step[15/17] Loss:3972102.5000\n",
      "Epoch [185/200], Step[16/17] Loss:739027.3125\n",
      "Epoch [185/200], Step[17/17] Loss:1181623.3750\n",
      "Epoch [186/200], Step[1/17] Loss:2819902.5000\n",
      "Epoch [186/200], Step[2/17] Loss:1064569.0000\n",
      "Epoch [186/200], Step[3/17] Loss:1365088.7500\n",
      "Epoch [186/200], Step[4/17] Loss:633371.3125\n",
      "Epoch [186/200], Step[5/17] Loss:902076.8750\n",
      "Epoch [186/200], Step[6/17] Loss:1552644.0000\n",
      "Epoch [186/200], Step[7/17] Loss:7746193.5000\n",
      "Epoch [186/200], Step[8/17] Loss:5778872.5000\n",
      "Epoch [186/200], Step[9/17] Loss:1128516.8750\n",
      "Epoch [186/200], Step[10/17] Loss:1651914.5000\n",
      "Epoch [186/200], Step[11/17] Loss:1779240.1250\n",
      "Epoch [186/200], Step[12/17] Loss:793694.0000\n",
      "Epoch [186/200], Step[13/17] Loss:4192962.5000\n",
      "Epoch [186/200], Step[14/17] Loss:1464225.1250\n",
      "Epoch [186/200], Step[15/17] Loss:2190887.2500\n",
      "Epoch [186/200], Step[16/17] Loss:1396548.3750\n",
      "Epoch [186/200], Step[17/17] Loss:3965013.5000\n",
      "Epoch [187/200], Step[1/17] Loss:4041615.0000\n",
      "Epoch [187/200], Step[2/17] Loss:1681247.3750\n",
      "Epoch [187/200], Step[3/17] Loss:1238703.5000\n",
      "Epoch [187/200], Step[4/17] Loss:2852642.2500\n",
      "Epoch [187/200], Step[5/17] Loss:490342.8750\n",
      "Epoch [187/200], Step[6/17] Loss:2770858.2500\n",
      "Epoch [187/200], Step[7/17] Loss:2982465.7500\n",
      "Epoch [187/200], Step[8/17] Loss:748634.4375\n",
      "Epoch [187/200], Step[9/17] Loss:2789258.2500\n",
      "Epoch [187/200], Step[10/17] Loss:2505949.2500\n",
      "Epoch [187/200], Step[11/17] Loss:3067675.7500\n",
      "Epoch [187/200], Step[12/17] Loss:589883.4375\n",
      "Epoch [187/200], Step[13/17] Loss:1731032.1250\n",
      "Epoch [187/200], Step[14/17] Loss:8854251.0000\n",
      "Epoch [187/200], Step[15/17] Loss:1573792.0000\n",
      "Epoch [187/200], Step[16/17] Loss:760563.3125\n",
      "Epoch [187/200], Step[17/17] Loss:1234912.6250\n",
      "Epoch [188/200], Step[1/17] Loss:11326694.0000\n",
      "Epoch [188/200], Step[2/17] Loss:3054215.7500\n",
      "Epoch [188/200], Step[3/17] Loss:2319264.2500\n",
      "Epoch [188/200], Step[4/17] Loss:1290934.1250\n",
      "Epoch [188/200], Step[5/17] Loss:1070978.8750\n",
      "Epoch [188/200], Step[6/17] Loss:1038089.6875\n",
      "Epoch [188/200], Step[7/17] Loss:1339597.3750\n",
      "Epoch [188/200], Step[8/17] Loss:1982651.8750\n",
      "Epoch [188/200], Step[9/17] Loss:1087143.5000\n",
      "Epoch [188/200], Step[10/17] Loss:1283920.3750\n",
      "Epoch [188/200], Step[11/17] Loss:915855.3750\n",
      "Epoch [188/200], Step[12/17] Loss:2467437.0000\n",
      "Epoch [188/200], Step[13/17] Loss:969421.7500\n",
      "Epoch [188/200], Step[14/17] Loss:3779743.5000\n",
      "Epoch [188/200], Step[15/17] Loss:1564877.5000\n",
      "Epoch [188/200], Step[16/17] Loss:3220202.2500\n",
      "Epoch [188/200], Step[17/17] Loss:1195388.1250\n",
      "Epoch [189/200], Step[1/17] Loss:594923.6875\n",
      "Epoch [189/200], Step[2/17] Loss:1471464.7500\n",
      "Epoch [189/200], Step[3/17] Loss:1304550.8750\n",
      "Epoch [189/200], Step[4/17] Loss:2843661.0000\n",
      "Epoch [189/200], Step[5/17] Loss:1522307.5000\n",
      "Epoch [189/200], Step[6/17] Loss:1673045.3750\n",
      "Epoch [189/200], Step[7/17] Loss:1523505.8750\n",
      "Epoch [189/200], Step[8/17] Loss:584028.7500\n",
      "Epoch [189/200], Step[9/17] Loss:3594538.0000\n",
      "Epoch [189/200], Step[10/17] Loss:1317898.2500\n",
      "Epoch [189/200], Step[11/17] Loss:1326986.8750\n",
      "Epoch [189/200], Step[12/17] Loss:1185290.7500\n",
      "Epoch [189/200], Step[13/17] Loss:1640916.6250\n",
      "Epoch [189/200], Step[14/17] Loss:3274095.5000\n",
      "Epoch [189/200], Step[15/17] Loss:5008862.0000\n",
      "Epoch [189/200], Step[16/17] Loss:9802684.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [189/200], Step[17/17] Loss:1247409.3750\n",
      "Epoch [190/200], Step[1/17] Loss:990669.5000\n",
      "Epoch [190/200], Step[2/17] Loss:2235472.5000\n",
      "Epoch [190/200], Step[3/17] Loss:1261801.8750\n",
      "Epoch [190/200], Step[4/17] Loss:3684347.5000\n",
      "Epoch [190/200], Step[5/17] Loss:1120956.7500\n",
      "Epoch [190/200], Step[6/17] Loss:555389.6250\n",
      "Epoch [190/200], Step[7/17] Loss:899247.6250\n",
      "Epoch [190/200], Step[8/17] Loss:4373603.0000\n",
      "Epoch [190/200], Step[9/17] Loss:1237896.3750\n",
      "Epoch [190/200], Step[10/17] Loss:1010882.9375\n",
      "Epoch [190/200], Step[11/17] Loss:1448869.0000\n",
      "Epoch [190/200], Step[12/17] Loss:8657408.0000\n",
      "Epoch [190/200], Step[13/17] Loss:4751017.0000\n",
      "Epoch [190/200], Step[14/17] Loss:964551.9375\n",
      "Epoch [190/200], Step[15/17] Loss:3254946.5000\n",
      "Epoch [190/200], Step[16/17] Loss:2087851.5000\n",
      "Epoch [190/200], Step[17/17] Loss:1412145.6250\n",
      "Epoch [191/200], Step[1/17] Loss:2841453.0000\n",
      "Epoch [191/200], Step[2/17] Loss:1152121.2500\n",
      "Epoch [191/200], Step[3/17] Loss:1076678.2500\n",
      "Epoch [191/200], Step[4/17] Loss:8167602.0000\n",
      "Epoch [191/200], Step[5/17] Loss:1071505.8750\n",
      "Epoch [191/200], Step[6/17] Loss:1513714.2500\n",
      "Epoch [191/200], Step[7/17] Loss:1148684.2500\n",
      "Epoch [191/200], Step[8/17] Loss:2282010.7500\n",
      "Epoch [191/200], Step[9/17] Loss:1763005.0000\n",
      "Epoch [191/200], Step[10/17] Loss:996700.5000\n",
      "Epoch [191/200], Step[11/17] Loss:1128452.7500\n",
      "Epoch [191/200], Step[12/17] Loss:3399432.2500\n",
      "Epoch [191/200], Step[13/17] Loss:1349733.2500\n",
      "Epoch [191/200], Step[14/17] Loss:3708162.2500\n",
      "Epoch [191/200], Step[15/17] Loss:2953407.2500\n",
      "Epoch [191/200], Step[16/17] Loss:2643921.0000\n",
      "Epoch [191/200], Step[17/17] Loss:3059318.5000\n",
      "Epoch [192/200], Step[1/17] Loss:7709089.5000\n",
      "Epoch [192/200], Step[2/17] Loss:1281345.3750\n",
      "Epoch [192/200], Step[3/17] Loss:851305.8750\n",
      "Epoch [192/200], Step[4/17] Loss:1033086.9375\n",
      "Epoch [192/200], Step[5/17] Loss:1940374.2500\n",
      "Epoch [192/200], Step[6/17] Loss:2152628.7500\n",
      "Epoch [192/200], Step[7/17] Loss:1470900.3750\n",
      "Epoch [192/200], Step[8/17] Loss:667552.5625\n",
      "Epoch [192/200], Step[9/17] Loss:2551335.0000\n",
      "Epoch [192/200], Step[10/17] Loss:1537462.1250\n",
      "Epoch [192/200], Step[11/17] Loss:3985645.5000\n",
      "Epoch [192/200], Step[12/17] Loss:1416695.0000\n",
      "Epoch [192/200], Step[13/17] Loss:1461895.5000\n",
      "Epoch [192/200], Step[14/17] Loss:2037634.1250\n",
      "Epoch [192/200], Step[15/17] Loss:4131755.7500\n",
      "Epoch [192/200], Step[16/17] Loss:1733586.3750\n",
      "Epoch [192/200], Step[17/17] Loss:4578446.0000\n",
      "Epoch [193/200], Step[1/17] Loss:2072172.5000\n",
      "Epoch [193/200], Step[2/17] Loss:1002760.7500\n",
      "Epoch [193/200], Step[3/17] Loss:1956948.5000\n",
      "Epoch [193/200], Step[4/17] Loss:8247497.0000\n",
      "Epoch [193/200], Step[5/17] Loss:1796955.0000\n",
      "Epoch [193/200], Step[6/17] Loss:1555954.0000\n",
      "Epoch [193/200], Step[7/17] Loss:2702886.2500\n",
      "Epoch [193/200], Step[8/17] Loss:2584371.0000\n",
      "Epoch [193/200], Step[9/17] Loss:2237788.2500\n",
      "Epoch [193/200], Step[10/17] Loss:2885449.0000\n",
      "Epoch [193/200], Step[11/17] Loss:3514367.5000\n",
      "Epoch [193/200], Step[12/17] Loss:3881947.7500\n",
      "Epoch [193/200], Step[13/17] Loss:1024643.0625\n",
      "Epoch [193/200], Step[14/17] Loss:1526936.6250\n",
      "Epoch [193/200], Step[15/17] Loss:1459729.8750\n",
      "Epoch [193/200], Step[16/17] Loss:614688.2500\n",
      "Epoch [193/200], Step[17/17] Loss:759612.5625\n",
      "Epoch [194/200], Step[1/17] Loss:1082295.1250\n",
      "Epoch [194/200], Step[2/17] Loss:1568063.2500\n",
      "Epoch [194/200], Step[3/17] Loss:1592683.0000\n",
      "Epoch [194/200], Step[4/17] Loss:2737121.5000\n",
      "Epoch [194/200], Step[5/17] Loss:1819783.8750\n",
      "Epoch [194/200], Step[6/17] Loss:2054264.3750\n",
      "Epoch [194/200], Step[7/17] Loss:3626001.2500\n",
      "Epoch [194/200], Step[8/17] Loss:1378227.5000\n",
      "Epoch [194/200], Step[9/17] Loss:994510.0000\n",
      "Epoch [194/200], Step[10/17] Loss:1301849.5000\n",
      "Epoch [194/200], Step[11/17] Loss:1452434.7500\n",
      "Epoch [194/200], Step[12/17] Loss:2128083.5000\n",
      "Epoch [194/200], Step[13/17] Loss:2901972.0000\n",
      "Epoch [194/200], Step[14/17] Loss:3351722.5000\n",
      "Epoch [194/200], Step[15/17] Loss:3388632.2500\n",
      "Epoch [194/200], Step[16/17] Loss:6933589.0000\n",
      "Epoch [194/200], Step[17/17] Loss:1687441.7500\n",
      "Epoch [195/200], Step[1/17] Loss:2574366.0000\n",
      "Epoch [195/200], Step[2/17] Loss:784069.0625\n",
      "Epoch [195/200], Step[3/17] Loss:2111387.7500\n",
      "Epoch [195/200], Step[4/17] Loss:586488.6250\n",
      "Epoch [195/200], Step[5/17] Loss:2586614.0000\n",
      "Epoch [195/200], Step[6/17] Loss:9824124.0000\n",
      "Epoch [195/200], Step[7/17] Loss:1862491.6250\n",
      "Epoch [195/200], Step[8/17] Loss:882415.2500\n",
      "Epoch [195/200], Step[9/17] Loss:2656849.7500\n",
      "Epoch [195/200], Step[10/17] Loss:2592451.5000\n",
      "Epoch [195/200], Step[11/17] Loss:2103836.2500\n",
      "Epoch [195/200], Step[12/17] Loss:739629.5625\n",
      "Epoch [195/200], Step[13/17] Loss:853689.9375\n",
      "Epoch [195/200], Step[14/17] Loss:3044090.0000\n",
      "Epoch [195/200], Step[15/17] Loss:3838848.5000\n",
      "Epoch [195/200], Step[16/17] Loss:1262607.3750\n",
      "Epoch [195/200], Step[17/17] Loss:1696395.2500\n",
      "Epoch [196/200], Step[1/17] Loss:1008886.1250\n",
      "Epoch [196/200], Step[2/17] Loss:1352034.3750\n",
      "Epoch [196/200], Step[3/17] Loss:4050239.7500\n",
      "Epoch [196/200], Step[4/17] Loss:1849596.3750\n",
      "Epoch [196/200], Step[5/17] Loss:822470.5000\n",
      "Epoch [196/200], Step[6/17] Loss:1493902.2500\n",
      "Epoch [196/200], Step[7/17] Loss:2762109.5000\n",
      "Epoch [196/200], Step[8/17] Loss:2667592.0000\n",
      "Epoch [196/200], Step[9/17] Loss:875032.6250\n",
      "Epoch [196/200], Step[10/17] Loss:2877906.0000\n",
      "Epoch [196/200], Step[11/17] Loss:4015601.2500\n",
      "Epoch [196/200], Step[12/17] Loss:8255793.5000\n",
      "Epoch [196/200], Step[13/17] Loss:1458733.7500\n",
      "Epoch [196/200], Step[14/17] Loss:2652485.2500\n",
      "Epoch [196/200], Step[15/17] Loss:1462248.7500\n",
      "Epoch [196/200], Step[16/17] Loss:1332234.2500\n",
      "Epoch [196/200], Step[17/17] Loss:917432.3125\n",
      "Epoch [197/200], Step[1/17] Loss:655316.3125\n",
      "Epoch [197/200], Step[2/17] Loss:1133670.1250\n",
      "Epoch [197/200], Step[3/17] Loss:1940777.6250\n",
      "Epoch [197/200], Step[4/17] Loss:2703116.0000\n",
      "Epoch [197/200], Step[5/17] Loss:3643979.5000\n",
      "Epoch [197/200], Step[6/17] Loss:886668.9375\n",
      "Epoch [197/200], Step[7/17] Loss:7272726.5000\n",
      "Epoch [197/200], Step[8/17] Loss:1859525.0000\n",
      "Epoch [197/200], Step[9/17] Loss:2093758.1250\n",
      "Epoch [197/200], Step[10/17] Loss:2604614.7500\n",
      "Epoch [197/200], Step[11/17] Loss:2194867.2500\n",
      "Epoch [197/200], Step[12/17] Loss:1367864.5000\n",
      "Epoch [197/200], Step[13/17] Loss:1145018.3750\n",
      "Epoch [197/200], Step[14/17] Loss:3428217.5000\n",
      "Epoch [197/200], Step[15/17] Loss:1463406.7500\n",
      "Epoch [197/200], Step[16/17] Loss:3100324.7500\n",
      "Epoch [197/200], Step[17/17] Loss:2693450.0000\n",
      "Epoch [198/200], Step[1/17] Loss:1827458.1250\n",
      "Epoch [198/200], Step[2/17] Loss:903438.0000\n",
      "Epoch [198/200], Step[3/17] Loss:1039337.8750\n",
      "Epoch [198/200], Step[4/17] Loss:1562383.1250\n",
      "Epoch [198/200], Step[5/17] Loss:7299737.5000\n",
      "Epoch [198/200], Step[6/17] Loss:1126600.0000\n",
      "Epoch [198/200], Step[7/17] Loss:1279195.5000\n",
      "Epoch [198/200], Step[8/17] Loss:1708327.7500\n",
      "Epoch [198/200], Step[9/17] Loss:1341747.6250\n",
      "Epoch [198/200], Step[10/17] Loss:2672043.2500\n",
      "Epoch [198/200], Step[11/17] Loss:3601635.5000\n",
      "Epoch [198/200], Step[12/17] Loss:1210081.5000\n",
      "Epoch [198/200], Step[13/17] Loss:2668492.7500\n",
      "Epoch [198/200], Step[14/17] Loss:3063573.5000\n",
      "Epoch [198/200], Step[15/17] Loss:3356622.2500\n",
      "Epoch [198/200], Step[16/17] Loss:4268446.5000\n",
      "Epoch [198/200], Step[17/17] Loss:926964.4375\n",
      "Epoch [199/200], Step[1/17] Loss:1310766.5000\n",
      "Epoch [199/200], Step[2/17] Loss:1522126.2500\n",
      "Epoch [199/200], Step[3/17] Loss:2043060.8750\n",
      "Epoch [199/200], Step[4/17] Loss:2914179.7500\n",
      "Epoch [199/200], Step[5/17] Loss:1651712.0000\n",
      "Epoch [199/200], Step[6/17] Loss:889523.9375\n",
      "Epoch [199/200], Step[7/17] Loss:2495369.0000\n",
      "Epoch [199/200], Step[8/17] Loss:2841243.0000\n",
      "Epoch [199/200], Step[9/17] Loss:3603860.7500\n",
      "Epoch [199/200], Step[10/17] Loss:775680.6875\n",
      "Epoch [199/200], Step[11/17] Loss:1492197.1250\n",
      "Epoch [199/200], Step[12/17] Loss:1452846.5000\n",
      "Epoch [199/200], Step[13/17] Loss:7257838.5000\n",
      "Epoch [199/200], Step[14/17] Loss:651246.8750\n",
      "Epoch [199/200], Step[15/17] Loss:5115778.0000\n",
      "Epoch [199/200], Step[16/17] Loss:1628072.6250\n",
      "Epoch [199/200], Step[17/17] Loss:2506804.2500\n",
      "Epoch [200/200], Step[1/17] Loss:8981383.0000\n",
      "Epoch [200/200], Step[2/17] Loss:2288229.2500\n",
      "Epoch [200/200], Step[3/17] Loss:1569294.5000\n",
      "Epoch [200/200], Step[4/17] Loss:1312770.2500\n",
      "Epoch [200/200], Step[5/17] Loss:4170637.5000\n",
      "Epoch [200/200], Step[6/17] Loss:2455296.7500\n",
      "Epoch [200/200], Step[7/17] Loss:1163365.0000\n",
      "Epoch [200/200], Step[8/17] Loss:1629564.7500\n",
      "Epoch [200/200], Step[9/17] Loss:1396648.3750\n",
      "Epoch [200/200], Step[10/17] Loss:1003866.3125\n",
      "Epoch [200/200], Step[11/17] Loss:3362262.7500\n",
      "Epoch [200/200], Step[12/17] Loss:1732267.5000\n",
      "Epoch [200/200], Step[13/17] Loss:1307909.0000\n",
      "Epoch [200/200], Step[14/17] Loss:1178246.8750\n",
      "Epoch [200/200], Step[15/17] Loss:2163310.0000\n",
      "Epoch [200/200], Step[16/17] Loss:3052467.0000\n",
      "Epoch [200/200], Step[17/17] Loss:1125859.8750\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "hist_2 = np.zeros(num_epochs)\n",
    "accuracy_train = np.zeros(num_epochs)\n",
    "accuracy_test = np.zeros(num_epochs)\n",
    "accuracy_val = np.zeros(num_epochs)\n",
    "total_step = len(train_loader)\n",
    "\n",
    "flag = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X_batch, labels_batch )in enumerate(train_loader):\n",
    "        #X_batch = X_batch.permute(1,0,2,3)\n",
    "        #print(X_batch.shape)\n",
    "#         \n",
    "        X_batch = X_batch.to(device, dtype=torch.float)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        #forward\n",
    "        y_pred, embed = model.forward(X_batch)\n",
    "#         print(y_pred.shape)\n",
    "#         print(embed.shape)\n",
    "\n",
    "#         print(y_pred.shape)\n",
    "#         print(X_batch.shape)\n",
    "#         print(labels_batch.shape)\n",
    "        #loss function\n",
    "        loss = loss_fn(y_pred, X_batch)\n",
    "#         print(y_pred)\n",
    "        #loss2 = loss_fn2(y_pred, labels_batch)\n",
    "        hist[epoch] = loss.item()\n",
    "        #why 0.6931\n",
    "        optimizer.zero_grad()\n",
    "        #backward \n",
    "        loss.backward()\n",
    "\n",
    "        #optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1)%1 == 0:\n",
    "            print(\"Epoch [{}/{}], Step[{}/{}] Loss:{:.4f}\" \n",
    "                .format(epoch+1, num_epochs, i+1, total_step, loss.item() ))\n",
    "#         _,__ , accuracy_train[epoch]= predict(model, train_loader)\n",
    "#         _,__ , accuracy_test[epoch]= predict(model, test_loader)\n",
    "#         _,__ , accuracy_val[epoch]= predict(model, val_loader)\n",
    "\n",
    "#         if loss.item()<1.30:\n",
    "#             flag = True\n",
    "#             break\n",
    "#     if flag:\n",
    "#         break\n",
    "        \n",
    "#     if (epoch+1) > 200:   \n",
    "#         if (epoch+1)%100 == 0:\n",
    "#             update_lr(optimizer, 1.1)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 200, 11])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape\n",
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0970e-05,  7.6155e-01,  7.6152e-01,  ...,  7.6154e-01,\n",
       "         -7.2040e-01,  7.5235e-01],\n",
       "        [-3.0970e-05,  7.6155e-01,  7.6152e-01,  ...,  7.6154e-01,\n",
       "         -7.2040e-01,  7.5235e-01],\n",
       "        [-3.0970e-05,  7.6155e-01,  7.6152e-01,  ...,  7.6154e-01,\n",
       "         -7.2040e-01,  7.5235e-01],\n",
       "        ...,\n",
       "        [-3.0970e-05,  7.6155e-01,  7.6152e-01,  ...,  7.6154e-01,\n",
       "         -7.2040e-01,  7.5235e-01],\n",
       "        [-3.0968e-05,  7.6155e-01,  7.6152e-01,  ...,  7.6154e-01,\n",
       "         -7.2040e-01,  7.5235e-01],\n",
       "        [-2.8516e-05,  7.6156e-01,  7.6152e-01,  ...,  7.6154e-01,\n",
       "         -7.2124e-01,  7.5264e-01]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.8048e+01, 9.4743e+02, 3.4876e+02, 5.6907e+02, 3.3600e+02, 8.0000e+01,\n",
       "        1.9360e+03, 5.5200e+04, 4.7575e+02, 5.1700e-01, 8.1500e-01],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed.view((100,4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_batch.cpu().detach().numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #used for Linear_LSTM to plot fc1 output\n",
    "# emb = embed\n",
    "# fig=plt.figure(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# plt.plot(emb.cpu().detach().numpy()[0:3600], label='t')\n",
    "# # plt.ylim(0.58, 0.68)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #used for Linear_LSTM to plot fc1 output\n",
    "# emb = embed[:,:,1]\n",
    "# print(emb.shape)\n",
    "# fig=plt.figure(figsize=(18, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# for i in range(4):\n",
    "#     plt.subplot(4,1,i+1)\n",
    "#     plt.plot(emb.cpu().detach().numpy()[:,i].T, label='t')\n",
    "# #     plt.ylim(0.58, 0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_batch.cpu().detach().numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####################\n",
    "# Plot preds and performance\n",
    "#####################\n",
    "plt.plot(hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(y_pred.cpu().detach().numpy(), label=\"Preds\")\n",
    "# plt.plot(y_train.cpu().detach().numpy(), label=\"labels\")\n",
    "# plt.legend()\n",
    "# plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEGCAYAAABLgMOSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAd/klEQVR4nO3deZRU5b3u8e8TQEHxIJNGRWw8Eg0oky0SOXGhRNREAoJezHIlNA7EOATj8ghORxOHqNdcsxRDTmtEoybCwYl4EggQh3tzHWgER0QI4KUVFQQHDBhpfveP2t0p2+ruYtNV1Q3PZ61aXfutt/b+1e7qenrvXfvdigjMzMy211dKXYCZmbVODhAzM0vFAWJmZqk4QMzMLBUHiJmZpdK21AUUU7du3aKsrKzUZZiZtSqLFi1aHxHd67fvUgFSVlZGVVVVqcswM2tVJL2Vq927sMzMLBUHiJmZpeIAMTOzVHapYyBmVjyff/451dXVbNmypdSlWJ7at29Pjx49aNeuXV79HSBmVhDV1dXstddelJWVIanU5VgTIoIPPviA6upqevXqlddzvAvLzApiy5YtdO3a1eHRSkiia9eu27XF6AAxs4JxeLQu2/v7coCYmVkqDhAzM0vFAWJm1sLdeOONqZ53zjnn8PrrrzdzNf/kADEz20Fbt24t6PwbCpCIYNu2bQ0+7+6776ZPnz6FKstf4zWzwvvpH17j9Xc+btZ59tn/X7hmZN8m+40ePZo1a9awZcsWJk2axMSJE5kzZw5XXHEFNTU1dOvWjQULFrBp0yYuuugiqqqqkMQ111zD2LFj6dixI5s2bQJg1qxZPPHEE9x7771UVFTQpUsXFi9ezKBBgxg3bhwXX3wxmzdvpkOHDkyfPp1DDz2UmpoaJk+ezNy5c5HEueeeS58+fZg6dSqPPvooAPPmzWPatGk88sgjX6p/ypQpbN68mQEDBtC3b19uuOEGTj75ZI477jieffZZHnvsMW666SYWLlzI5s2bOe200/jpT38KwLBhw7j11lspLy+nY8eOTJo0iSeeeIIOHTrw+OOPs+++++7Q78ABYmY7tXvuuYcuXbqwefNmjjrqKEaNGsW5557LM888Q69evdiwYQMA1113HZ06deKVV14BYOPGjU3O+80332T+/Pm0adOGjz/+mGeeeYa2bdsyf/58rrjiCh5++GEqKytZtWoVixcvpm3btmzYsIHOnTtzwQUXsG7dOrp378706dOZMGFCzmXcdNNNTJ06lSVLlgCwevVqli1bxvTp0/nVr34FwA033ECXLl2oqalh+PDhvPzyy/Tr1+8L8/n0008ZMmQIN9xwA5dddhl33XUXV111Ver1Cg4QMyuCfLYUCuX222+v+09/zZo1VFZWcuyxx9adLNelSxcA5s+fz0MPPVT3vM6dOzc579NPP502bdoA8NFHHzF+/HiWL1+OJD7//PO6+Z533nm0bdv2C8v7/ve/zwMPPMCECRN49tln+e1vf5v3azrooIMYMmRI3fTMmTOprKxk69atrF27ltdff/1LAbLbbrtxyimnAHDkkUcyb968vJfXEAeIme20nnrqKebPn8+zzz7LHnvswbBhw+jfvz/Lli37Ut+IyHkeRHZb/ZPs9txzz7r7V199NccddxyPPvooq1evZtiwYY3Od8KECYwcOZL27dtz+umn1wVMPrKXu2rVKm699VYWLlxI586dqaioyHkyYLt27erqaNOmTbMct/FBdDPbaX300Ud07tyZPfbYgzfeeIPnnnuOzz77jKeffppVq1YB1O3CGjFiBFOnTq17bu0urH333ZelS5eybdu2ui2ZhpZ1wAEHAHDvvffWtY8YMYJf//rXdR/Ytcvbf//92X///bn++uupqKho9HW0a9euboumvo8//pg999yTTp068d577/GnP/2p0Xk1JweIme20TjrpJLZu3Uq/fv24+uqrGTJkCN27d6eyspIxY8bQv39/xo0bB8BVV13Fxo0bOfzww+nfvz9PPvkkkDkGccopp3D88cez3377Nbisyy67jMsvv5yhQ4dSU1NT137OOefQs2dP+vXrR//+/fnd735X99iZZ57JgQce2OQ3pSZOnEi/fv0488wzv/RY//79GThwIH379uWss85i6NCh27WOdoQiomgLK7Xy8vLwFQnNimPp0qV8/etfL3UZLdqFF17IwIEDOfvss0tdSp1cvzdJiyKivH5fHwMxMyuBI488kj333JNf/OIXpS4lNQeImVkJLFq06EttRx99NJ999tkX2u6//36OOOKIYpW1XRwgZmYtxPPPP1/qEraLD6KbmVkqDhAzM0vFAWJmZqk4QMzMWri0w7lD5qTGd955pxmr+aeSBoikkyQtk7RC0pQcj+8uaUby+POSyuo93lPSJkmXFqtmM7P6SjWcez52ygCR1Aa4EzgZ6AN8T1L90zHPBjZGxCHAbcDN9R6/DSjeeftm1uqMHj2aI488kr59+1JZWQnAnDlzGDRoEP3792f48OEAbNq0iQkTJnDEEUfQr18/Hn74YQA6duxYN69Zs2bVDTtSUVHBJZdcwnHHHcfkyZN54YUXOOaYYxg4cCDHHHNM3XhbNTU1XHrppXXzveOOO1iwYAGnnnpq3XznzZvHmDFjctafPZx77ZnoDzzwAIMHD2bAgAH88Ic/pKamhpqaGioqKjj88MM54ogjuO2225g1axZVVVWceeaZDBgwgM2bNzfrui3l13gHAysiYiWApIeAUUD25bNGAdcm92cBUyUpIkLSaGAl8GnxSjazVP40Bd59pXnn+dUj4OSbmuy2sw3nvnTpUmbMmMFf//pX2rVrx/nnn8+DDz5I3759efvtt3n11VcB+PDDD9l7772ZOnVq3TVBmlspA+QAYE3WdDVwdEN9ImKrpI+ArpI2A5OBE4BGd19JmghMBOjZs2fzVG5mrcbONpz7ggULWLRoEUcddRQAmzdvZp999mHkyJGsXLmSiy66iO985zuMGDEir/ntiFIGyJfHN4b6A3M11OenwG0RsSnXMMlf6BxRCVRCZiysFHWa2Y7KY0uhEHbG4dwjgvHjx/Pzn//8S4+99NJLzJ07lzvvvJOZM2dyzz335DXPtEp5EL0aODBrugdQ/0hPXR9JbYFOwAYyWyq3SFoNXAxcIenCQhdsZq3Lzjic+/Dhw5k1axbvv/9+3fzeeust1q9fz7Zt2xg7dizXXXcdL774IgB77bUXn3zySV7ra3uVMkAWAr0l9ZK0G3AGMLten9nA+OT+acBfIuObEVEWEWXAL4EbI2IqZmZZdsbh3Pv06cP111/PiBEj6NevHyeccAJr167l7bffZtiwYQwYMICKioq6LZSKigrOO++8ghxEL+lw7pK+TSYA2gD3RMQNkn4GVEXEbEntgfuBgWS2PM6oPeieNY9rgU0RcWtTy/Nw7mbF4+Hcm+bh3HdARPwR+GO9tv/Iur8FOL2JeVxbkOLMzArIw7mbmVkqHs7dzMyajYdzNzOzXYIDxMzMUnGAmJlZKg4QMzNLxQFiZmapOEDMzHYyZWVlrF+/vuDLcYCYme2gQl9QqqXyeSBmVnA3v3Azb2x4o1nneViXw5g8eHKT/UaPHs2aNWvYsmULkyZNYuLEicyZM4crrriCmpoaunXrxoIFC9i0aRMXXXQRVVVVSOKaa65h7NixdOzYkU2bNgGZC0o98cQT3HvvvVRUVNClSxcWL17MoEGDGDduHBdffDGbN2+mQ4cOTJ8+nUMPPZSamhomT57M3LlzkcS5555Lnz59mDp1at3gjPPmzWPatGk88sgjX6p/2rRprFq1iltuuQXIDNS4aNEi7rjjjpyvrZgcIGa2U2vtF5Q67bTT+MY3vlEXIDNmzODKK6/M+drGjh1L165dm2O15cUBYmYFl8+WQqG09gtKde/enYMPPpjnnnuO3r17s2zZMoYOHZrztS1fvtwBYmbWHHaWC0qNGzeOmTNncthhh3HqqaciKedrq19fofkgupnttHaWC0qNGTOGxx57jN///vd11y/J9dqKzQFiZjutneWCUp07d6ZPnz689dZbDB48uMHXVmwlvaBUsfmCUmbF4wtKNc0XlDIzs+3mC0qZmVkqvqCUmVkjGvoGkuVW6gtKbe8hDR9EN7OCaN++PR988MF2fyhZaUQEH3zwAe3bt8/7Od4CMbOC6NGjB9XV1axbt67UpVie2rdvT48ePfLu7wAxs4Jo165d3dnetnPyLiwzM0vFAWJmZqk4QMzMLBUHiJmZpeIAMTOzVBwgZmaWigPEzMxScYCYmVkqJQ0QSSdJWiZphaQpOR7fXdKM5PHnJZUl7SdIWiTpleTn8cWu3cxsV1eyAJHUBrgTOBnoA3xPUv2rqpwNbIyIQ4DbgJuT9vXAyIg4AhgP3F+cqs3MrFYpt0AGAysiYmVE/AN4CBhVr88o4L7k/ixguCRFxOKIeCdpfw1oL2n3olRtZmZAaQPkAGBN1nR10pazT0RsBT4CutbrMxZYHBGfYWZmRVPKwRRzXSSg/rjPjfaR1JfMbq0RDS5EmghMBOjZs+f2V2lmZjmVcgukGjgwa7oH8E5DfSS1BToBG5LpHsCjwA8i4m8NLSQiKiOiPCLKu3fv3ozlm5nt2koZIAuB3pJ6SdoNOAOYXa/PbDIHyQFOA/4SESFpb+C/gcsj4q9Fq9jMzOqULECSYxoXAnOBpcDMiHhN0s8kfTfp9hugq6QVwCVA7Vd9LwQOAa6WtCS57VPkl2BmtkvTrnS5yfLy8qiqqip1GWZmrYqkRRFRXr/dZ6KbmVkqDhAzM0vFAWJmZqk4QMzMLBUHiJmZpeIAMTOzVPIOEEkjkyHVl0g6v5BFmZlZy9dggEjqX6/p+8AQYBDwo0IWZWZmLV9jgymeL0nAf0TEu2RGxb0B2MaXx6wyM7NdTIMBEhE/TLZC/lNSFXA1cAywB3BdkeozM7MWqtFjIBHxUkSMApaQGdhwv4iY7WtvmJlZY8dAzpO0WNKLwJ7ASUBnSXMlfbNoFZqZWYvU2BbI+RExkMyB83+PiK0RcTuZYddPLUp1ZmbWYjV2EP1tSdcBHYA3ahsjYiOZodXNzGwX1liAjAJOBD4H5hWnHDMzay0a+xbWP4A/FLEWMzNrRTyUiZmZpeIAMTOzVBwgZmaWigPEzMxScYCYmVkqTQaIpAsldS5GMWZm1nrkswXyVWChpJmSTkpG6DUzs11ckwESEVcBvYHfABXAckk3SvrXAtdmZmYtWF7HQCIigHeT21agMzBL0i0FrM3MzFqwxoYyAUDSj4HxwHrgbjIDK34u6SvAcuCywpZoZmYtUZMBAnQDxkTEW9mNEbFN0imFKcvMzFq6fHZh/RHYUDshaS9JRwNExNJCFWZmZi1bPgEyDdiUNf1p0mZmZruwfAJEyUF0ILPrivx2fZmZ2U4snwBZKenHktolt0nAykIXZmZmLVs+AXIecAzwNlANHA1MbI6FJycmLpO0QtKUHI/vLmlG8vjzksqyHrs8aV8m6cTmqMfMzPLX5K6oiHifzHXQm5WkNsCdwAlkgmmhpNkR8XpWt7OBjRFxiKQzgJuBcZL6JDX1BfYH5kv6WkTUNHedZmaWWz7ngbQn80HeF2hf2x4RZ+3gsgcDKyJiZbKch8hcRjc7QEYB1yb3ZwFTk6FURgEPRcRnwCpJK5L5PbuDNeV08d0n8M62dYWYtZlZwe3/le788pzmvzJ5Pruw7iczHtaJwNNAD+CTZlj2AcCarOnqpC1nn4jYCnwEdM3zuQBImiipSlLVunUOATOz5pLPt6kOiYjTJY2KiPsk/Q6Y2wzLzjUoY+TZJ5/nZhojKoFKgPLy8px9mlKI5DYza+3y2QL5PPn5oaTDgU5AWTMsuxo4MGu6B/BOQ30ktU2WvSHP55qZWQHlEyCVyfVArgJmkzlGcXMzLHsh0FtSL0m7kTkoPrten9lkxuECOA34S3JOymzgjORbWr3IjBb8QjPUZGZmeWp0F1YyYOLHEbEReAY4uLkWHBFbJV1IZndYG+CeiHhN0s+AqoiYTWYI+fuTg+QbSL4NlvSbSSbMtgIX+BtYZmbFpayTzHN3kJ6JiGOLVE9BlZeXR1VVVanLMDNrVSQtiojy+u357MKaJ+lSSQdK6lJ7K0CNZmbWiuTzLaza8z0uyGoLmnF3lpmZtT75nIneqxiFmJlZ65LPmeg/yNUeEb9t/nLMzKy1yGcX1lFZ99sDw4EXAQeImdkuLJ9dWBdlT0vqRGZ4EzMz24Xl8y2s+v5O5sQ9MzPbheVzDOQP/HOcqa8AfYCZhSzKzMxavnyOgdyadX8r8FZEVBeoHjMzayXyCZD/B6yNiC0AkjpIKouI1QWtzMzMWrR8joH8F7Ata7omaTMzs11YPgHSNiL+UTuR3N+tcCWZmVlrkE+ArJP03doJSaOA9YUryczMWoN8joGcBzwoaWoyXQ3kPDvdzMx2HfmcSPg3YIikjmSGf2+O66GbmVkr1+QuLEk3Sto7IjZFxCeSOku6vhjFmZlZy5XPMZCTI+LD2onk6oTfLlxJZmbWGuQTIG0k7V47IakDsHsj/c3MbBeQz0H0B4AFkqaTGdLkLDwSr5nZLi+fg+i3SHoZ+BYg4LqImFvwyszMrEXLZwuEiJgDzAGQNFTSnRFxQRNPMzOznVheASJpAPA9YBywCnikkEWZmVnL12CASPoacAaZ4PgAmEHmPJDjilSbmZm1YI1tgbwB/G9gZESsAJD0k6JUZWZmLV5jX+MdC7wLPCnpLknDyRxENzMzazhAIuLRiBgHHAY8BfwE2FfSNEkjilSfmZm1UE2eSBgRn0bEgxFxCtADWAJMKXhlZmbWouVzJnqdiNgQEf8ZEccXqiAzM2sdtitAzMzMajlAzMwslZIEiKQukuZJWp787NxAv/FJn+WSxidte0j6b0lvSHpN0k3Frd7MzKB0WyBTgAUR0RtYQI6D8pK6ANcARwODgWuygubWiDgMGAgMlXRycco2M7NapQqQUcB9yf37gNE5+pwIzEsO3G8E5gEnRcTfI+JJgIj4B/AimW+HmZlZEZUqQPaNiLUAyc99cvQ5AFiTNV2dtNWRtDcwksxWjJmZFVFegymmIWk+8NUcD12Z7yxytEXW/NsCvwduj4iVjdQxEZgI0LNnzzwXbWZmTSlYgETEtxp6TNJ7kvaLiLWS9gPez9GtGhiWNd2DzBnxtSqB5RHxyybqqEz6Ul5eHo31NTOz/JVqF9ZsYHxyfzzweI4+c4ERkjonB89HJG1Iuh7oBFxchFrNzCyHUgXITcAJkpYDJyTTSCqXdDdkznoHrgMWJrefRcQGST3I7AbrA7woaYmkc0rxIszMdmWK2HX26pSXl0dVVVWpyzAza1UkLYqI8vrtPhPdzMxScYCYmVkqDhAzM0vFAWJmZqk4QMzMLBUHiJmZpeIAMTOzVBwgZmaWigPEzMxScYCYmVkqDhAzM0vFAWJmZqk4QMzMLBUHiJmZpeIAMTOzVBwgZmaWigPEzMxScYCYmVkqDhAzM0vFAWJmZqk4QMzMLBUHiJmZpeIAMTOzVBwgZmaWigPEzMxScYCYmVkqDhAzM0vFAWJmZqk4QMzMLBUHiJmZpeIAMTOzVEoSIJK6SJonaXnys3MD/cYnfZZLGp/j8dmSXi18xWZmVl+ptkCmAAsiojewIJn+AkldgGuAo4HBwDXZQSNpDLCpOOWamVl9pQqQUcB9yf37gNE5+pwIzIuIDRGxEZgHnAQgqSNwCXB9EWo1M7McShUg+0bEWoDk5z45+hwArMmark7aAK4DfgH8vakFSZooqUpS1bp163asajMzq9O2UDOWNB/4ao6Hrsx3FjnaQtIA4JCI+ImksqZmEhGVQCVAeXl55LlsMzNrQsECJCK+1dBjkt6TtF9ErJW0H/B+jm7VwLCs6R7AU8A3gCMlrSZT/z6SnoqIYZiZWdGUahfWbKD2W1Xjgcdz9JkLjJDUOTl4PgKYGxHTImL/iCgD/g140+FhZlZ8pQqQm4ATJC0HTkimkVQu6W6AiNhA5ljHwuT2s6TNzMxaAEXsOocFysvLo6qqqtRlmJm1KpIWRUR5/XafiW5mZqk4QMzMLBUHiJmZpeIAMTOzVBwgZmaWigPEzMxScYCYmVkqDhAzM0vFAWJmZqk4QMzMLBUHiJmZpeIAMTOzVBwgZmaWigPEzMxScYCYmVkqDhAzM0vFAWJmZqk4QMzMLBUHiJmZpeIAMTOzVBwgZmaWigPEzMxScYCYmVkqDhAzM0tFEVHqGopG0jrgrZRP7wasb8Zymovr2n4ttTbXtX1aal3QcmtLW9dBEdG9fuMuFSA7QlJVRJSXuo76XNf2a6m1ua7t01LrgpZbW3PX5V1YZmaWigPEzMxScYDkr7LUBTTAdW2/llqb69o+LbUuaLm1NWtdPgZiZmapeAvEzMxScYCYmVkqDpAmSDpJ0jJJKyRNKXEtB0p6UtJSSa9JmpS0XyvpbUlLktu3S1DbakmvJMuvStq6SJonaXnys3ORazo0a50skfSxpItLtb4k3SPpfUmvZrXlXEfKuD15370saVCR6/qfkt5Ilv2opL2T9jJJm7PW3a+LXFeDvztJlyfra5mkE4tc14ysmlZLWpK0F3N9NfT5ULj3WET41sANaAP8DTgY2A14CehTwnr2AwYl9/cC3gT6ANcCl5Z4Xa0GutVruwWYktyfAtxc4t/lu8BBpVpfwLHAIODVptYR8G3gT4CAIcDzRa5rBNA2uX9zVl1l2f1KsL5y/u6Sv4OXgN2BXsnfbZti1VXv8V8A/1GC9dXQ50PB3mPeAmncYGBFRKyMiH8ADwGjSlVMRKyNiBeT+58AS4EDSlVPHkYB9yX37wNGl7CW4cDfIiLtSAQ7LCKeATbUa25oHY0CfhsZzwF7S9qvWHVFxJ8jYmsy+RzQoxDL3t66GjEKeCgiPouIVcAKMn+/Ra1LkoD/Afy+EMtuTCOfDwV7jzlAGncAsCZrupoW8oEtqQwYCDyfNF2YbIbeU+xdRYkA/ixpkaSJSdu+EbEWMm9uYJ8S1FXrDL74R13q9VWroXXUkt57Z5H5T7VWL0mLJT0t6ZslqCfX766lrK9vAu9FxPKstqKvr3qfDwV7jzlAGqccbSX/3rOkjsDDwMUR8TEwDfhXYACwlswmdLENjYhBwMnABZKOLUENOUnaDfgu8F9JU0tYX01pEe89SVcCW4EHk6a1QM+IGAhcAvxO0r8UsaSGfnctYn0B3+OL/6gUfX3l+HxosGuOtu1aZw6QxlUDB2ZN9wDeKVEtAEhqR+bN8WBEPAIQEe9FRE1EbAPuokCb7o2JiHeSn+8DjyY1vFe7SZz8fL/YdSVOBl6MiPeSGku+vrI0tI5K/t6TNB44BTgzkp3myS6iD5L7i8gca/hasWpq5HfXEtZXW2AMMKO2rdjrK9fnAwV8jzlAGrcQ6C2pV/Jf7BnA7FIVk+xf/Q2wNCL+V1Z79n7LU4FX6z+3wHXtKWmv2vtkDsC+SmZdjU+6jQceL2ZdWb7wX2Gp11c9Da2j2cAPkm/KDAE+qt0NUQySTgImA9+NiL9ntXeX1Ca5fzDQG1hZxLoa+t3NBs6QtLukXkldLxSrrsS3gDciorq2oZjrq6HPBwr5HivGtwNa843MNxXeJPOfw5UlruXfyGxivgwsSW7fBu4HXknaZwP7Fbmug8l8A+Yl4LXa9QR0BRYAy5OfXUqwzvYAPgA6ZbWVZH2RCbG1wOdk/vs7u6F1RGb3wp3J++4VoLzIda0gs3+89n3266Tv2OR3/BLwIjCyyHU1+LsDrkzW1zLg5GLWlbTfC5xXr28x11dDnw8Fe495KBMzM0vFu7DMzCwVB4iZmaXiADEzs1QcIGZmlooDxMzMUnGAmDUjSTX64gjAzTaCczKyaynPWTH7gralLsBsJ7M5IgaUugizYvAWiFkRJNeIuFnSC8ntkKT9IEkLksEBF0jqmbTvq8x1OF5Kbscks2oj6a7keg9/ltShZC/KdnkOELPm1aHeLqxxWY99HBGDganAL5O2qWSG1O5HZsDC25P224GnI6I/mWtPvJa09wbujIi+wIdkznQ2KwmfiW7WjCRtioiOOdpXA8dHxMpkwLt3I6KrpPVkhuP4PGlfGxHdJK0DekTEZ1nzKAPmRUTvZHoy0C4iri/8KzP7Mm+BmBVPNHC/oT65fJZ1vwYfx7QScoCYFc+4rJ/PJvf/L5lRngHOBP5Pcn8B8CMASW2KfM0Ns7z4vxez5tVB0pKs6TkRUftV3t0lPU/mH7fvJW0/Bu6R9O/AOmBC0j4JqJR0NpktjR+RGQHWrMXwMRCzIkiOgZRHxPpS12LWXLwLy8zMUvEWiJmZpeItEDMzS8UBYmZmqThAzMwsFQeImZml4gAxM7NU/j+DwsY0SHkJdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################\n",
    "# Plot preds and performance\n",
    "####################\n",
    "plt.plot(accuracy_train, label=\"accuracy_train\")\n",
    "plt.plot(accuracy_test, label=\"accuracy_test\")\n",
    "plt.plot(accuracy_val, label=\"accuracy_val\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy  %')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_predicts, train_labels, accuracy = predict(model, train_loader)\n",
    "cm = metrics.confusion_matrix(train_labels, train_predicts)\n",
    "\n",
    "fig,ax = plot_confusion_matrix(cm, colorbar = True, show_normed = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicts, test_labels, accuracy = predict(model, test_loader)\n",
    "cm = metrics.confusion_matrix(test_labels, test_predicts)\n",
    "\n",
    "fig,ax = plot_confusion_matrix(cm, colorbar = True, show_normed = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predicts, val_labels, accuracy = predict(model, val_loader)\n",
    "cm = metrics.confusion_matrix(val_labels, val_predicts)\n",
    "\n",
    "fig,ax = plot_confusion_matrix(cm, colorbar = True, show_normed = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
